{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9089c2",
   "metadata": {},
   "source": [
    "# Heirarchial chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3acab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import LongformerModel, LongformerTokenizer, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset, load_from_disk\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "import hashlib\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import evaluate\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download NLTK punkt tokenizer\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "def nltk_sent_tokenize(text):\n",
    "    \"\"\"Splits text into sentences using NLTK.\"\"\"\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "def get_token_f1_score(candidate_tokens, reference_tokens):\n",
    "    \"\"\"Computes a token-based F1 score between a candidate and reference sentence.\"\"\"\n",
    "    candidate_tokens_set = set(candidate_tokens)\n",
    "    reference_tokens_set = set(reference_tokens)\n",
    "    if not candidate_tokens_set or not reference_tokens_set:\n",
    "        return 0.0\n",
    "    intersection = len(candidate_tokens_set.intersection(reference_tokens_set))\n",
    "    precision = intersection / len(candidate_tokens_set)\n",
    "    recall = intersection / len(reference_tokens_set)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Custom Model Definition for Longformer\n",
    "# ==============================================================================\n",
    "class LongformerExtractiveSummarizationModel(nn.Module):\n",
    "    \"\"\"A custom PyTorch model for extractive summarization using a Longformer backbone.\"\"\"\n",
    "    def __init__(self, pos_weight=None):\n",
    "        super(LongformerExtractiveSummarizationModel, self).__init__()\n",
    "        self.longformer = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.classifier = nn.Linear(self.longformer.config.hidden_size, 1)\n",
    "        self.pos_weight = pos_weight if pos_weight is not None else torch.tensor(1.0)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, global_attention_mask=None, labels=None):\n",
    "        outputs = self.longformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            global_attention_mask=global_attention_mask\n",
    "        )\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        logits = logits.squeeze(-1)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight.to(logits.device))\n",
    "            loss = loss_fct(logits, labels.float())\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Configuration and Data Loading\n",
    "# ==============================================================================\n",
    "DATA_DIR = \"govreport_tfidf_vscode2\"\n",
    "NUM_TRAIN_SAMPLES = 5000\n",
    "NUM_VAL_SAMPLES = 300\n",
    "NUM_TEST_SAMPLES = 300\n",
    "F1_THRESHOLD = 0.3\n",
    "CACHE_DIR = \"dataset_cache_longformer_v4\"\n",
    "CHUNK_SIZE = 4096\n",
    "train_file = f\"{DATA_DIR}/train.json\"\n",
    "test_file = f\"{DATA_DIR}/test.json\"\n",
    "validation_file = f\"{DATA_DIR}/validation.json\"\n",
    "\n",
    "def load_jsonl_data(file_path, max_samples=None):\n",
    "    \"\"\"Loads a JSON Lines file and returns a list of dictionaries.\"\"\"\n",
    "    data = []\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        for i, obj in enumerate(reader):\n",
    "            if max_samples and i >= max_samples:\n",
    "                break\n",
    "            data.append(obj)\n",
    "    return data\n",
    "\n",
    "print(f\"Loading {NUM_TRAIN_SAMPLES} samples from the training file...\")\n",
    "train_data_raw = load_jsonl_data(train_file, max_samples=NUM_TRAIN_SAMPLES)\n",
    "print(f\"Loading {NUM_VAL_SAMPLES} samples from the validation file...\")\n",
    "validation_data_raw = load_jsonl_data(validation_file, max_samples=NUM_VAL_SAMPLES)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Data Preprocessing and Dataset Creation\n",
    "# ==============================================================================\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Processes raw data, validates inputs, and precomputes sentence embeddings.\"\"\"\n",
    "    sent_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    processed_samples = []\n",
    "    for i, item in enumerate(data):\n",
    "        original_text = item.get('original_text', '')\n",
    "        extractive_summary = item.get('extractive_summary', '')\n",
    "        if not isinstance(original_text, str) or not original_text.strip():\n",
    "            continue\n",
    "        if not isinstance(extractive_summary, str) or not extractive_summary.strip():\n",
    "            continue\n",
    "        document_sentences = nltk_sent_tokenize(original_text)\n",
    "        sentence_embeddings = sent_model.encode(document_sentences, convert_to_numpy=True).tolist()\n",
    "        processed_samples.append({\n",
    "            'document': original_text,\n",
    "            'extractive_summary': extractive_summary,\n",
    "            'id': i,\n",
    "            'document_sentences': document_sentences,\n",
    "            'sentence_embeddings': sentence_embeddings\n",
    "        })\n",
    "    print(f\"Processed {len(processed_samples)} valid samples out of {len(data)}\")\n",
    "    return processed_samples\n",
    "\n",
    "train_data = preprocess_data(train_data_raw)\n",
    "validation_data = preprocess_data(validation_data_raw)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Tokenizer, Label Alignment, and Caching\n",
    "# ==============================================================================\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "def tokenize_and_align_labels(examples, f1_threshold=F1_THRESHOLD):\n",
    "    \"\"\"Tokenizes documents without truncation, creates hierarchical chunks, and aligns labels.\"\"\"\n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_global_attention_mask = []\n",
    "    all_labels = []\n",
    "    all_document_id = []\n",
    "  \n",
    "    for idx, (doc, summary, doc_id, doc_sentences) in enumerate(zip(\n",
    "        examples['document'], examples['extractive_summary'], examples['id'], examples['document_sentences']\n",
    "    )):\n",
    "        print(f\"Processing document {idx} (ID: {doc_id})...\")\n",
    "        if not doc or not summary:\n",
    "            continue\n",
    "      \n",
    "        summary_sentences = nltk_sent_tokenize(summary)\n",
    "        if not doc_sentences or not summary_sentences:\n",
    "            continue\n",
    "      \n",
    "        tokenized_summary_sentences = [tokenizer.tokenize(s) for s in summary_sentences]\n",
    "        sentence_labels = []\n",
    "        tokenized_doc_sentences = []\n",
    "        for doc_sentence in doc_sentences:\n",
    "            doc_sent_tokens = tokenizer.tokenize(doc_sentence)\n",
    "            tokenized_doc_sentences.append(tokenizer.encode(doc_sentence, add_special_tokens=False))\n",
    "            max_f1 = max(get_token_f1_score(doc_sent_tokens, sum_sent_tokens) for sum_sent_tokens in tokenized_summary_sentences) if tokenized_summary_sentences else 0.0\n",
    "            sentence_labels.append(1.0 if max_f1 >= f1_threshold else 0.0)\n",
    "      \n",
    "        current_input_ids = [tokenizer.cls_token_id]\n",
    "        current_attention_mask = [1]\n",
    "        current_labels = [0.0]\n",
    "        current_sent_idx = 0\n",
    "      \n",
    "        while current_sent_idx < len(doc_sentences):\n",
    "            while current_sent_idx < len(doc_sentences):\n",
    "                sent_tokens = tokenized_doc_sentences[current_sent_idx]\n",
    "                if len(current_input_ids) + len(sent_tokens) + 1 > CHUNK_SIZE:\n",
    "                    break\n",
    "                current_input_ids += sent_tokens\n",
    "                current_attention_mask += [1] * len(sent_tokens)\n",
    "                current_labels += [sentence_labels[current_sent_idx]] * len(sent_tokens)\n",
    "                current_sent_idx += 1\n",
    "            \n",
    "            if len(current_input_ids) > 1:\n",
    "                current_input_ids.append(tokenizer.sep_token_id)\n",
    "                current_attention_mask.append(1)\n",
    "                current_labels.append(0.0)\n",
    "                \n",
    "                padding_length = CHUNK_SIZE - len(current_input_ids)\n",
    "                if padding_length > 0:\n",
    "                    current_input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "                    current_attention_mask += [0] * padding_length\n",
    "                    current_labels += [0.0] * padding_length\n",
    "                \n",
    "                global_attention_mask = [0] * len(current_input_ids)\n",
    "                global_attention_mask[0] = 1\n",
    "                \n",
    "                all_input_ids.append(current_input_ids)\n",
    "                all_attention_mask.append(current_attention_mask)\n",
    "                all_global_attention_mask.append(global_attention_mask)\n",
    "                all_labels.append(current_labels)\n",
    "                all_document_id.append(doc_id)\n",
    "            \n",
    "            if current_sent_idx < len(doc_sentences):\n",
    "                current_input_ids = [tokenizer.cls_token_id]\n",
    "                current_attention_mask = [1]\n",
    "                current_labels = [0.0]\n",
    "      \n",
    "        if not all_input_ids or len(all_input_ids[-1]) < 10:\n",
    "            continue\n",
    "  \n",
    "    return {\n",
    "        'input_ids': all_input_ids,\n",
    "        'attention_mask': all_attention_mask,\n",
    "        'global_attention_mask': all_global_attention_mask,\n",
    "        'labels': all_labels,\n",
    "        'document_id': all_document_id\n",
    "    }\n",
    "\n",
    "def load_or_generate_dataset(data, split_name, max_samples, f1_threshold=F1_THRESHOLD):\n",
    "    \"\"\"Loads a cached dataset or generates and caches a new one.\"\"\"\n",
    "    if not data:\n",
    "        print(f\"No valid data for {split_name} after preprocessing\")\n",
    "        return Dataset.from_dict({'input_ids': [], 'attention_mask': [], 'global_attention_mask': [], 'labels': [], 'document_id': []})\n",
    "  \n",
    "    cache_key = f\"{split_name}-{max_samples}-{f1_threshold}\"\n",
    "    cache_path = os.path.join(CACHE_DIR, hashlib.md5(cache_key.encode()).hexdigest())\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading cached dataset from {cache_path}...\")\n",
    "        return load_from_disk(cache_path)\n",
    "  \n",
    "    print(f\"Generating and caching dataset for {split_name} at {cache_path}...\")\n",
    "    dataset = Dataset.from_dict({\n",
    "        'document': [item['document'] for item in data],\n",
    "        'extractive_summary': [item['extractive_summary'] for item in data],\n",
    "        'id': [item['id'] for item in data],\n",
    "        'document_sentences': [item['document_sentences'] for item in data],\n",
    "        'sentence_embeddings': [item['sentence_embeddings'] for item in data]\n",
    "    })\n",
    "    dataset_tokenized = dataset.map(\n",
    "        lambda examples: tokenize_and_align_labels(examples, f1_threshold),\n",
    "        batched=True,\n",
    "        batch_size=4,\n",
    "        remove_columns=[\"document\", \"extractive_summary\", \"id\", \"document_sentences\", \"sentence_embeddings\"],\n",
    "        desc=f\"Tokenizing {split_name} data\",\n",
    "        num_proc=4\n",
    "    )\n",
    "  \n",
    "    def is_valid_example(example):\n",
    "        if not isinstance(example['input_ids'], list) or not example['input_ids']:\n",
    "            return False\n",
    "        if len(example['input_ids']) != len(example['attention_mask']) or \\\n",
    "           len(example['input_ids']) != len(example['global_attention_mask']) or \\\n",
    "           len(example['input_ids']) != len(example['labels']):\n",
    "            return False\n",
    "        if 'document_id' not in example:\n",
    "            return False\n",
    "        return True\n",
    "  \n",
    "    dataset_tokenized = dataset_tokenized.filter(is_valid_example, desc=f\"Filtering invalid examples for {split_name}\")\n",
    "  \n",
    "    if not os.path.exists(CACHE_DIR):\n",
    "        os.makedirs(CACHE_DIR)\n",
    "  \n",
    "    dataset_tokenized.save_to_disk(cache_path)\n",
    "    print(\"Caching complete.\")\n",
    "    return dataset_tokenized\n",
    "\n",
    "# Clear cache to ensure new tokenization\n",
    "\"\"\"if os.path.exists(CACHE_DIR):\n",
    "    import shutil\n",
    "    shutil.rmtree(CACHE_DIR)\"\"\"\n",
    "\n",
    "train_dataset_tokenized = load_or_generate_dataset(train_data, 'train', NUM_TRAIN_SAMPLES)\n",
    "validation_dataset_tokenized = load_or_generate_dataset(validation_data, 'validation', NUM_VAL_SAMPLES)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset_tokenized)}\")\n",
    "print(f\"Validation dataset size: {len(validation_dataset_tokenized)}\")\n",
    "\n",
    "if len(train_dataset_tokenized) == 0 or len(validation_dataset_tokenized) == 0:\n",
    "    raise ValueError(\"One or both datasets are empty after processing.\")\n",
    "\n",
    "# Calculate pos_weight based on class imbalance\n",
    "def calculate_pos_weight(dataset):\n",
    "    \"\"\"Calculates pos_weight based on the ratio of negative to positive labels.\"\"\"\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    for example in dataset:\n",
    "        labels = example['labels']\n",
    "        pos_count += sum(1 for label in labels if label == 1.0)\n",
    "        neg_count += sum(1 for label in labels if label == 0.0)\n",
    "    pos_weight = neg_count / pos_count if pos_count > 0 else 1.0\n",
    "    return torch.tensor(pos_weight)\n",
    "\n",
    "pos_weight = calculate_pos_weight(train_dataset_tokenized)\n",
    "print(f\"Calculated pos_weight: {pos_weight.item():.2f}\")\n",
    "\n",
    "# Calculate average reference summary length for dynamic top_k\n",
    "def calculate_avg_summary_sentences(data):\n",
    "    \"\"\"Calculates the average number of sentences in reference summaries.\"\"\"\n",
    "    total_sentences = sum(len(nltk_sent_tokenize(item['extractive_summary'])) for item in data)\n",
    "    return max(1, round(total_sentences / len(data)))\n",
    "\n",
    "avg_summary_sentences = calculate_avg_summary_sentences(validation_data)\n",
    "print(f\"Average reference summary sentences: {avg_summary_sentences}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. Custom metrics for evaluation with ROUGE, Top-k, and MMR\n",
    "# ==============================================================================\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred, raw_data, tokenizer_obj, eval_dataset, top_k=avg_summary_sentences, lambda_mmr=1):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.sigmoid(torch.from_numpy(logits))\n",
    "    predictions_binary = (predictions > 0.5).numpy().astype(int)\n",
    "    \n",
    "    labels_flat = labels.flatten()\n",
    "    predictions_flat = predictions_binary.flatten()\n",
    "    \n",
    "    metrics = {}\n",
    "    if len(labels_flat) > 0:\n",
    "        metrics[\"f1\"] = f1_score(labels_flat, predictions_flat, average='binary', zero_division=0)\n",
    "        metrics[\"precision\"] = precision_score(labels_flat, predictions_flat, average='binary', zero_division=0)\n",
    "        metrics[\"recall\"] = recall_score(labels_flat, predictions_flat, average='binary', zero_division=0)\n",
    "        metrics[\"accuracy\"] = accuracy_score(labels_flat, predictions_flat)\n",
    "    else:\n",
    "        metrics[\"f1\"] = 0.0\n",
    "        metrics[\"precision\"] = 0.0\n",
    "        metrics[\"recall\"] = 0.0\n",
    "        metrics[\"accuracy\"] = 0.0\n",
    "    \n",
    "    predicted_summaries = []\n",
    "    reference_summaries = []\n",
    "    \n",
    "    doc_logits = defaultdict(list)\n",
    "    doc_attention = defaultdict(list)\n",
    "    for i, doc_id in enumerate(eval_dataset['document_id']):\n",
    "        doc_logits[doc_id].append(predictions[i].numpy())\n",
    "        doc_attention[doc_id].append(eval_dataset['attention_mask'][i])\n",
    "    \n",
    "    for doc_id in sorted(doc_logits.keys()):\n",
    "        chunks = doc_logits[doc_id]\n",
    "        att_masks = doc_attention[doc_id]\n",
    "        reference_summaries.append(raw_data[doc_id]['extractive_summary'])\n",
    "        document = raw_data[doc_id]['document']\n",
    "        sentence_embeddings = np.array(raw_data[doc_id]['sentence_embeddings'])\n",
    "        \n",
    "        aggregated_scores = []\n",
    "        for chunk, att_mask in zip(chunks, att_masks):\n",
    "            effective_len = sum(att_mask)\n",
    "            if effective_len < 3:\n",
    "                continue\n",
    "            content_scores = chunk[1:effective_len - 1]\n",
    "            aggregated_scores.extend(content_scores)\n",
    "        \n",
    "        document_sentences = nltk_sent_tokenize(document)\n",
    "        sentence_scores = []\n",
    "        tokenized_doc_sentences = [tokenizer_obj.encode(s, add_special_tokens=False) for s in document_sentences]\n",
    "        \n",
    "        start_index = 0\n",
    "        for sent_tokens in tokenized_doc_sentences:\n",
    "            end_index = start_index + len(sent_tokens)\n",
    "            if end_index > len(aggregated_scores):\n",
    "                break\n",
    "            sentence_logits = aggregated_scores[start_index:end_index]\n",
    "            sentence_scores.append(np.max(sentence_logits) if len(sentence_logits) > 0 else 0.0)\n",
    "            start_index = end_index\n",
    "        \n",
    "        sentence_embeddings = torch.tensor(sentence_embeddings, dtype=torch.float32)\n",
    "        selected_indices = []\n",
    "        for _ in range(min(top_k, len(document_sentences))):\n",
    "            if not sentence_scores:\n",
    "                break\n",
    "            if not selected_indices:\n",
    "                best_idx = np.argmax(sentence_scores)\n",
    "                selected_indices.append(best_idx)\n",
    "            else:\n",
    "                mmr_scores = []\n",
    "                for i, score in enumerate(sentence_scores):\n",
    "                    if i in selected_indices:\n",
    "                        mmr_scores.append(-float('inf'))\n",
    "                        continue\n",
    "                    relevance = score\n",
    "                    max_similarity = max([util.pytorch_cos_sim(sentence_embeddings[i], sentence_embeddings[j]).item() for j in selected_indices])\n",
    "                    mmr_score = lambda_mmr * relevance - (1 - lambda_mmr) * max_similarity\n",
    "                    mmr_scores.append(mmr_score)\n",
    "                best_idx = np.argmax(mmr_scores)\n",
    "                if mmr_scores[best_idx] > 0:\n",
    "                    selected_indices.append(best_idx)\n",
    "        \n",
    "        predicted_sentences = [document_sentences[i] for i in selected_indices]\n",
    "        if not predicted_sentences and document_sentences:\n",
    "            predicted_sentences.append(document_sentences[0])\n",
    "        \n",
    "        predicted_summaries.append(\" \".join(predicted_sentences))\n",
    "    \n",
    "    rouge_results = rouge_metric.compute(predictions=predicted_summaries, references=reference_summaries, use_stemmer=True)\n",
    "    \n",
    "    rouge1_val = rouge_results[\"rouge1\"]\n",
    "    rouge2_val = rouge_results[\"rouge2\"]\n",
    "    rougeL_val = rouge_results[\"rougeL\"]\n",
    "    metrics.update({\n",
    "        \"rouge1\": rouge1_val.mid.fmeasure if hasattr(rouge1_val, 'mid') else rouge1_val,\n",
    "        \"rouge2\": rouge2_val.mid.fmeasure if hasattr(rouge2_val, 'mid') else rouge2_val,\n",
    "        \"rougeL\": rougeL_val.mid.fmeasure if hasattr(rougeL_val, 'mid') else rougeL_val,\n",
    "    })\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. Training Arguments and Trainer\n",
    "# ==============================================================================\n",
    "total_train_batch_size = 4 * 2\n",
    "approx_num_chunks = sum(1 for _ in train_dataset_tokenized) if train_dataset_tokenized else 5000\n",
    "max_steps = (approx_num_chunks // total_train_batch_size) * 5\n",
    "save_steps = max(500, max_steps // 10)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./extractive_summarization_results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=save_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=save_steps,\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    max_steps=max_steps,\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "# Find the latest checkpoint\n",
    "latest_checkpoint = None\n",
    "if os.path.exists(training_args.output_dir):\n",
    "    checkpoints = [d for d in os.listdir(training_args.output_dir) if d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "        latest_checkpoint = os.path.join(training_args.output_dir, latest_checkpoint)\n",
    "\n",
    "model = LongformerExtractiveSummarizationModel(pos_weight=pos_weight)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_tokenized,\n",
    "    eval_dataset=validation_dataset_tokenized,\n",
    "    compute_metrics=lambda p: compute_metrics(p, validation_data, tokenizer, validation_dataset_tokenized, top_k=avg_summary_sentences, lambda_mmr=1),\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. Start the fine-tuning\n",
    "# ==============================================================================\n",
    "print(\"Starting training...\")\n",
    "trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbff1a4",
   "metadata": {},
   "source": [
    "# Overlapping chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import LongformerModel, LongformerTokenizer, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset, load_from_disk\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "import hashlib\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import evaluate\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download NLTK punkt tokenizer\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "def nltk_sent_tokenize(text):\n",
    "    \"\"\"Splits text into sentences using NLTK.\"\"\"\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "def get_token_f1_score(candidate_tokens, reference_tokens):\n",
    "    \"\"\"Computes a token-based F1 score between a candidate and reference sentence.\"\"\"\n",
    "    candidate_tokens_set = set(candidate_tokens)\n",
    "    reference_tokens_set = set(reference_tokens)\n",
    "    if not candidate_tokens_set or not reference_tokens_set:\n",
    "        return 0.0\n",
    "    intersection = len(candidate_tokens_set.intersection(reference_tokens_set))\n",
    "    precision = intersection / len(candidate_tokens_set)\n",
    "    recall = intersection / len(reference_tokens_set)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Custom Model Definition for Longformer\n",
    "# ==============================================================================\n",
    "class LongformerExtractiveSummarizationModel(nn.Module):\n",
    "    \"\"\"A custom PyTorch model for extractive summarization using a Longformer backbone.\"\"\"\n",
    "    def __init__(self, pos_weight=None):\n",
    "        super(LongformerExtractiveSummarizationModel, self).__init__()\n",
    "        self.longformer = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.classifier = nn.Linear(self.longformer.config.hidden_size, 1)\n",
    "        self.pos_weight = pos_weight if pos_weight is not None else torch.tensor(1.0)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, global_attention_mask=None, labels=None):\n",
    "        outputs = self.longformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            global_attention_mask=global_attention_mask\n",
    "        )\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        logits = logits.squeeze(-1)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight.to(logits.device))\n",
    "            loss = loss_fct(logits, labels.float())\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Configuration and Data Loading\n",
    "# ==============================================================================\n",
    "DATA_DIR = \"govreport_tfidf_vscode2\"\n",
    "NUM_TRAIN_SAMPLES = 5000\n",
    "NUM_VAL_SAMPLES = 300\n",
    "NUM_TEST_SAMPLES = 300\n",
    "F1_THRESHOLD = 0.3\n",
    "CACHE_DIR = \"dataset_cache_longformer_v4\"\n",
    "CHUNK_SIZE = 4096\n",
    "train_file = f\"{DATA_DIR}/train.json\"\n",
    "test_file = f\"{DATA_DIR}/test.json\"\n",
    "validation_file = f\"{DATA_DIR}/validation.json\"\n",
    "\n",
    "def load_jsonl_data(file_path, max_samples=None):\n",
    "    \"\"\"Loads a JSON Lines file and returns a list of dictionaries.\"\"\"\n",
    "    data = []\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        for i, obj in enumerate(reader):\n",
    "            if max_samples and i >= max_samples:\n",
    "                break\n",
    "            data.append(obj)\n",
    "    return data\n",
    "\n",
    "print(f\"Loading {NUM_TRAIN_SAMPLES} samples from the training file...\")\n",
    "train_data_raw = load_jsonl_data(train_file, max_samples=NUM_TRAIN_SAMPLES)\n",
    "print(f\"Loading {NUM_VAL_SAMPLES} samples from the validation file...\")\n",
    "validation_data_raw = load_jsonl_data(validation_file, max_samples=NUM_VAL_SAMPLES)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Data Preprocessing and Dataset Creation\n",
    "# ==============================================================================\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Processes raw data, validates inputs, and precomputes sentence embeddings.\"\"\"\n",
    "    sent_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    processed_samples = []\n",
    "    for i, item in enumerate(data):\n",
    "        original_text = item.get('original_text', '')\n",
    "        extractive_summary = item.get('extractive_summary', '')\n",
    "        if not isinstance(original_text, str) or not original_text.strip():\n",
    "            continue\n",
    "        if not isinstance(extractive_summary, str) or not extractive_summary.strip():\n",
    "            continue\n",
    "        document_sentences = nltk_sent_tokenize(original_text)\n",
    "        sentence_embeddings = sent_model.encode(document_sentences, convert_to_numpy=True).tolist()\n",
    "        processed_samples.append({\n",
    "            'document': original_text,\n",
    "            'extractive_summary': extractive_summary,\n",
    "            'id': i,\n",
    "            'document_sentences': document_sentences,\n",
    "            'sentence_embeddings': sentence_embeddings\n",
    "        })\n",
    "    print(f\"Processed {len(processed_samples)} valid samples out of {len(data)}\")\n",
    "    return processed_samples\n",
    "\n",
    "train_data = preprocess_data(train_data_raw)\n",
    "validation_data = preprocess_data(validation_data_raw)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Tokenizer, Label Alignment, and Caching\n",
    "# ==============================================================================\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "def tokenize_and_align_labels(examples, f1_threshold=F1_THRESHOLD):\n",
    "    \"\"\"Tokenizes documents without truncation, creates sequential chunks, and aligns labels.\"\"\"\n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_global_attention_mask = []\n",
    "    all_labels = []\n",
    "    all_document_id = []\n",
    "\n",
    "    for idx, (doc, summary, doc_id, doc_sentences) in enumerate(zip(\n",
    "        examples['document'], examples['extractive_summary'], examples['id'], examples['document_sentences']\n",
    "    )):\n",
    "        print(f\"Processing document {idx} (ID: {doc_id})...\")\n",
    "        if not doc or not summary:\n",
    "            continue\n",
    "\n",
    "        summary_sentences = nltk_sent_tokenize(summary)\n",
    "        if not doc_sentences or not summary_sentences:\n",
    "            continue\n",
    "\n",
    "        tokenized_summary_sentences = [tokenizer.tokenize(s) for s in summary_sentences]\n",
    "        tokenized_doc = tokenizer.encode(doc, add_special_tokens=False)\n",
    "        sentence_labels = []\n",
    "        tokenized_doc_sentences = [tokenizer.encode(s, add_special_tokens=False) for s in doc_sentences]\n",
    "\n",
    "        # Assign labels to each sentence based on F1 score\n",
    "        for doc_sentence in doc_sentences:\n",
    "            doc_sent_tokens = tokenizer.tokenize(doc_sentence)\n",
    "            max_f1 = max(get_token_f1_score(doc_sent_tokens, sum_sent_tokens) for sum_sent_tokens in tokenized_summary_sentences) if tokenized_summary_sentences else 0.0\n",
    "            sentence_labels.append(1.0 if max_f1 >= f1_threshold else 0.0)\n",
    "\n",
    "        # Sequential chunking\n",
    "        current_input_ids = [tokenizer.cls_token_id]\n",
    "        current_attention_mask = [1]\n",
    "        current_labels = [0.0]\n",
    "        token_idx = 0\n",
    "        sent_idx = 0\n",
    "        sent_token_idx = 0\n",
    "\n",
    "        while token_idx < len(tokenized_doc):\n",
    "            # Add tokens until chunk is full or document ends\n",
    "            while token_idx < len(tokenized_doc):\n",
    "                if len(current_input_ids) >= CHUNK_SIZE - 1:  # Reserve space for SEP token\n",
    "                    break\n",
    "                current_input_ids.append(tokenized_doc[token_idx])\n",
    "                current_attention_mask.append(1)\n",
    "                # Assign label based on current sentence\n",
    "                if sent_idx < len(doc_sentences):\n",
    "                    current_labels.append(sentence_labels[sent_idx])\n",
    "                else:\n",
    "                    current_labels.append(0.0)\n",
    "                token_idx += 1\n",
    "\n",
    "                # Update sentence index if we've reached the end of current sentence tokens\n",
    "                if sent_idx < len(doc_sentences) and sent_token_idx < len(tokenized_doc_sentences[sent_idx]):\n",
    "                    sent_token_idx += 1\n",
    "                if sent_idx < len(doc_sentences) and sent_token_idx >= len(tokenized_doc_sentences[sent_idx]):\n",
    "                    sent_idx += 1\n",
    "                    sent_token_idx = 0\n",
    "\n",
    "            # Finalize chunk\n",
    "            current_input_ids.append(tokenizer.sep_token_id)\n",
    "            current_attention_mask.append(1)\n",
    "            current_labels.append(0.0)\n",
    "\n",
    "            # Pad to CHUNK_SIZE if necessary\n",
    "            padding_length = CHUNK_SIZE - len(current_input_ids)\n",
    "            if padding_length > 0:\n",
    "                current_input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "                current_attention_mask += [0] * padding_length\n",
    "                current_labels += [0.0] * padding_length\n",
    "\n",
    "            # Create global attention mask\n",
    "            global_attention_mask = [0] * len(current_input_ids)\n",
    "            global_attention_mask[0] = 1  # CLS token gets global attention\n",
    "\n",
    "            # Append to lists\n",
    "            all_input_ids.append(current_input_ids)\n",
    "            all_attention_mask.append(current_attention_mask)\n",
    "            all_global_attention_mask.append(global_attention_mask)\n",
    "            all_labels.append(current_labels)\n",
    "            all_document_id.append(doc_id)\n",
    "\n",
    "            # Reset for next chunk\n",
    "            current_input_ids = [tokenizer.cls_token_id]\n",
    "            current_attention_mask = [1]\n",
    "            current_labels = [0.0]\n",
    "\n",
    "        # Skip if no valid chunks were created\n",
    "        if not all_input_ids or len(all_input_ids[-1]) < 10:\n",
    "            continue\n",
    "\n",
    "    return {\n",
    "        'input_ids': all_input_ids,\n",
    "        'attention_mask': all_attention_mask,\n",
    "        'global_attention_mask': all_global_attention_mask,\n",
    "        'labels': all_labels,\n",
    "        'document_id': all_document_id\n",
    "    }\n",
    "\n",
    "def load_or_generate_dataset(data, split_name, max_samples, f1_threshold=F1_THRESHOLD):\n",
    "    \"\"\"Loads a cached dataset or generates and caches a new one.\"\"\"\n",
    "    if not data:\n",
    "        print(f\"No valid data for {split_name} after preprocessing\")\n",
    "        return Dataset.from_dict({'input_ids': [], 'attention_mask': [], 'global_attention_mask': [], 'labels': [], 'document_id': []})\n",
    "\n",
    "    cache_key = f\"{split_name}-{max_samples}-{f1_threshold}\"\n",
    "    cache_path = os.path.join(CACHE_DIR, hashlib.md5(cache_key.encode()).hexdigest())\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading cached dataset from {cache_path}...\")\n",
    "        return load_from_disk(cache_path)\n",
    "\n",
    "    print(f\"Generating and caching dataset for {split_name} at {cache_path}...\")\n",
    "    dataset = Dataset.from_dict({\n",
    "        'document': [item['document'] for item in data],\n",
    "        'extractive_summary': [item['extractive_summary'] for item in data],\n",
    "        'id': [item['id'] for item in data],\n",
    "        'document_sentences': [item['document_sentences'] for item in data],\n",
    "        'sentence_embeddings': [item['sentence_embeddings'] for item in data]\n",
    "    })\n",
    "    dataset_tokenized = dataset.map(\n",
    "        lambda examples: tokenize_and_align_labels(examples, f1_threshold),\n",
    "        batched=True,\n",
    "        batch_size=4,\n",
    "        remove_columns=[\"document\", \"extractive_summary\", \"id\", \"document_sentences\", \"sentence_embeddings\"],\n",
    "        desc=f\"Tokenizing {split_name} data\",\n",
    "        num_proc=4\n",
    "    )\n",
    "\n",
    "    def is_valid_example(example):\n",
    "        if not isinstance(example['input_ids'], list) or not example['input_ids']:\n",
    "            return False\n",
    "        if len(example['input_ids']) != len(example['attention_mask']) or \\\n",
    "           len(example['input_ids']) != len(example['global_attention_mask']) or \\\n",
    "           len(example['input_ids']) != len(example['labels']):\n",
    "            return False\n",
    "        if 'document_id' not in example:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    dataset_tokenized = dataset_tokenized.filter(is_valid_example, desc=f\"Filtering invalid examples for {split_name}\")\n",
    "\n",
    "    if not os.path.exists(CACHE_DIR):\n",
    "        os.makedirs(CACHE_DIR)\n",
    "\n",
    "    dataset_tokenized.save_to_disk(cache_path)\n",
    "    print(\"Caching complete.\")\n",
    "    return dataset_tokenized\n",
    "\n",
    "# Clear cache to ensure new tokenization\n",
    "\"\"\"if os.path.exists(CACHE_DIR):\n",
    "    import shutil\n",
    "    shutil.rmtree(CACHE_DIR)\"\"\"\n",
    "train_dataset_tokenized = load_or_generate_dataset(train_data, 'train', NUM_TRAIN_SAMPLES)\n",
    "validation_dataset_tokenized = load_or_generate_dataset(validation_data, 'validation', NUM_VAL_SAMPLES)\n",
    "print(f\"Train dataset size: {len(train_dataset_tokenized)}\")\n",
    "print(f\"Validation dataset size: {len(validation_dataset_tokenized)}\")\n",
    "if len(train_dataset_tokenized) == 0 or len(validation_dataset_tokenized) == 0:\n",
    "    raise ValueError(\"One or both datasets are empty after processing.\")\n",
    "\n",
    "# Calculate pos_weight based on class imbalance\n",
    "def calculate_pos_weight(dataset):\n",
    "    \"\"\"Calculates pos_weight based on the ratio of negative to positive labels.\"\"\"\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    for example in dataset:\n",
    "        labels = example['labels']\n",
    "        pos_count += sum(1 for label in labels if label == 1.0)\n",
    "        neg_count += sum(1 for label in labels if label == 0.0)\n",
    "    pos_weight = neg_count / pos_count if pos_count > 0 else 1.0\n",
    "    return torch.tensor(pos_weight)\n",
    "\n",
    "pos_weight = calculate_pos_weight(train_dataset_tokenized)\n",
    "print(f\"Calculated pos_weight: {pos_weight.item():.2f}\")\n",
    "\n",
    "# Calculate average reference summary length for dynamic top_k\n",
    "def calculate_avg_summary_sentences(data):\n",
    "    \"\"\"Calculates the average number of sentences in reference summaries.\"\"\"\n",
    "    total_sentences = sum(len(nltk_sent_tokenize(item['extractive_summary'])) for item in data)\n",
    "    return max(1, round(total_sentences / len(data)))\n",
    "\n",
    "avg_summary_sentences = calculate_avg_summary_sentences(validation_data)\n",
    "print(f\"Average reference summary sentences: {avg_summary_sentences}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. Custom metrics for evaluation with ROUGE, Top-k, and MMR\n",
    "# ==============================================================================\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred, raw_data, tokenizer_obj, eval_dataset, top_k=avg_summary_sentences, lambda_mmr=1):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.sigmoid(torch.from_numpy(logits))\n",
    "    predictions_binary = (predictions > 0.5).numpy().astype(int)\n",
    "\n",
    "    labels_flat = labels.flatten()\n",
    "    predictions_flat = predictions_binary.flatten()\n",
    "\n",
    "    metrics = {}\n",
    "    if len(labels_flat) > 0:\n",
    "        metrics[\"f1\"] = f1_score(labels_flat, predictions_flat, average='binary', zero_division=0)\n",
    "        metrics[\"precision\"] = precision_score(labels_flat, predictions_flat, average='binary', zero_division=0)\n",
    "        metrics[\"recall\"] = recall_score(labels_flat, predictions_flat, average='binary', zero_division=0)\n",
    "        metrics[\"accuracy\"] = accuracy_score(labels_flat, predictions_flat)\n",
    "    else:\n",
    "        metrics[\"f1\"] = 0.0\n",
    "        metrics[\"precision\"] = 0.0\n",
    "        metrics[\"recall\"] = 0.0\n",
    "        metrics[\"accuracy\"] = 0.0\n",
    "\n",
    "    predicted_summaries = []\n",
    "    reference_summaries = []\n",
    "\n",
    "    doc_logits = defaultdict(list)\n",
    "    doc_attention = defaultdict(list)\n",
    "    for i, doc_id in enumerate(eval_dataset['document_id']):\n",
    "        doc_logits[doc_id].append(predictions[i].numpy())\n",
    "        doc_attention[doc_id].append(eval_dataset['attention_mask'][i])\n",
    "\n",
    "    for doc_id in sorted(doc_logits.keys()):\n",
    "        chunks = doc_logits[doc_id]\n",
    "        att_masks = doc_attention[doc_id]\n",
    "        reference_summaries.append(raw_data[doc_id]['extractive_summary'])\n",
    "        document = raw_data[doc_id]['document']\n",
    "        sentence_embeddings = np.array(raw_data[doc_id]['sentence_embeddings'])\n",
    "\n",
    "        aggregated_scores = []\n",
    "        for chunk, att_mask in zip(chunks, att_masks):\n",
    "            effective_len = sum(att_mask)\n",
    "            if effective_len < 3:\n",
    "                continue\n",
    "            content_scores = chunk[1:effective_len - 1]\n",
    "            aggregated_scores.extend(content_scores)\n",
    "\n",
    "        document_sentences = nltk_sent_tokenize(document)\n",
    "        sentence_scores = []\n",
    "        tokenized_doc_sentences = [tokenizer_obj.encode(s, add_special_tokens=False) for s in document_sentences]\n",
    "\n",
    "        start_index = 0\n",
    "        for sent_tokens in tokenized_doc_sentences:\n",
    "            end_index = start_index + len(sent_tokens)\n",
    "            if end_index > len(aggregated_scores):\n",
    "                break\n",
    "            sentence_logits = aggregated_scores[start_index:end_index]\n",
    "            sentence_scores.append(np.max(sentence_logits) if len(sentence_logits) > 0 else 0.0)\n",
    "            start_index = end_index\n",
    "\n",
    "        sentence_embeddings = torch.tensor(sentence_embeddings, dtype=torch.float32)\n",
    "        selected_indices = []\n",
    "        for _ in range(min(top_k, len(document_sentences))):\n",
    "            if not sentence_scores:\n",
    "                break\n",
    "            if not selected_indices:\n",
    "                best_idx = np.argmax(sentence_scores)\n",
    "                selected_indices.append(best_idx)\n",
    "            else:\n",
    "                mmr_scores = []\n",
    "                for i, score in enumerate(sentence_scores):\n",
    "                    if i in selected_indices:\n",
    "                        mmr_scores.append(-float('inf'))\n",
    "                        continue\n",
    "                    relevance = score\n",
    "                    max_similarity = max([util.pytorch_cos_sim(sentence_embeddings[i], sentence_embeddings[j]).item() for j in selected_indices])\n",
    "                    mmr_score = lambda_mmr * relevance - (1 - lambda_mmr) * max_similarity\n",
    "                    mmr_scores.append(mmr_score)\n",
    "                best_idx = np.argmax(mmr_scores)\n",
    "                if mmr_scores[best_idx] > 0:\n",
    "                    selected_indices.append(best_idx)\n",
    "\n",
    "        predicted_sentences = [document_sentences[i] for i in selected_indices]\n",
    "        if not predicted_sentences and document_sentences:\n",
    "            predicted_sentences.append(document_sentences[0])\n",
    "\n",
    "        predicted_summaries.append(\" \".join(predicted_sentences))\n",
    "\n",
    "    rouge_results = rouge_metric.compute(predictions=predicted_summaries, references=reference_summaries, use_stemmer=True)\n",
    "\n",
    "    rouge1_val = rouge_results[\"rouge1\"]\n",
    "    rouge2_val = rouge_results[\"rouge2\"]\n",
    "    rougeL_val = rouge_results[\"rougeL\"]\n",
    "    metrics.update({\n",
    "        \"rouge1\": rouge1_val.mid.fmeasure if hasattr(rouge1_val, 'mid') else rouge1_val,\n",
    "        \"rouge2\": rouge2_val.mid.fmeasure if hasattr(rouge2_val, 'mid') else rouge2_val,\n",
    "        \"rougeL\": rougeL_val.mid.fmeasure if hasattr(rougeL_val, 'mid') else rougeL_val,\n",
    "    })\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. Training Arguments and Trainer\n",
    "# ==============================================================================\n",
    "total_train_batch_size = 4 * 2\n",
    "approx_num_chunks = sum(1 for _ in train_dataset_tokenized) if train_dataset_tokenized else 5000\n",
    "max_steps = (approx_num_chunks // total_train_batch_size) * 5\n",
    "save_steps = max(500, max_steps // 10)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./extractive_summarization_results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=save_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=save_steps,\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    max_steps=max_steps,\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "# Find the latest checkpoint\n",
    "latest_checkpoint = None\n",
    "if os.path.exists(training_args.output_dir):\n",
    "    checkpoints = [d for d in os.listdir(training_args.output_dir) if d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "        latest_checkpoint = os.path.join(training_args.output_dir, latest_checkpoint)\n",
    "\n",
    "model = LongformerExtractiveSummarizationModel(pos_weight=pos_weight)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_tokenized,\n",
    "    eval_dataset=validation_dataset_tokenized,\n",
    "    compute_metrics=lambda p: compute_metrics(p, validation_data, tokenizer, validation_dataset_tokenized, top_k=avg_summary_sentences, lambda_mmr=1),\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. Start the fine-tuning\n",
    "# ==============================================================================\n",
    "print(\"Starting training...\")\n",
    "trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a2c20",
   "metadata": {},
   "source": [
    "# Overlapping chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d519e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import LongformerModel, LongformerTokenizer, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset, load_from_disk\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "import hashlib\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import evaluate\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download NLTK punkt tokenizer\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "def nltk_sent_tokenize(text):\n",
    "    \"\"\"Splits text into sentences using NLTK.\"\"\"\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "def get_token_f1_score(candidate_tokens, reference_tokens):\n",
    "    \"\"\"Computes a token-based F1 score between a candidate and reference sentence.\"\"\"\n",
    "    candidate_tokens_set = set(candidate_tokens)\n",
    "    reference_tokens_set = set(reference_tokens)\n",
    "    if not candidate_tokens_set or not reference_tokens_set:\n",
    "        return 0.0\n",
    "    intersection = len(candidate_tokens_set.intersection(reference_tokens_set))\n",
    "    precision = intersection / len(candidate_tokens_set)\n",
    "    recall = intersection / len(reference_tokens_set)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Custom Model Definition for Longformer\n",
    "# ==============================================================================\n",
    "class LongformerExtractiveSummarizationModel(nn.Module):\n",
    "    \"\"\"A custom PyTorch model for extractive summarization using a Longformer backbone.\"\"\"\n",
    "    def __init__(self, pos_weight=None):\n",
    "        super(LongformerExtractiveSummarizationModel, self).__init__()\n",
    "        self.longformer = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.classifier = nn.Linear(self.longformer.config.hidden_size, 1)\n",
    "        self.pos_weight = pos_weight if pos_weight is not None else torch.tensor(1.0)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, global_attention_mask=None, labels=None):\n",
    "        outputs = self.longformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            global_attention_mask=global_attention_mask\n",
    "        )\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        logits = logits.squeeze(-1)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight.to(logits.device))\n",
    "            loss = loss_fct(logits, labels.float())\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Configuration and Data Loading\n",
    "# ==============================================================================\n",
    "DATA_DIR = \"govreport_tfidf_vscode2\"\n",
    "NUM_TRAIN_SAMPLES = 5000\n",
    "NUM_VAL_SAMPLES = 300\n",
    "NUM_TEST_SAMPLES = 300\n",
    "F1_THRESHOLD = 0.3\n",
    "CACHE_DIR = \"dataset_cache_longformer_v4\"\n",
    "CHUNK_SIZE = 4096\n",
    "OVERLAP_SIZE = 512  # Define overlap size for overlapping chunking\n",
    "train_file = f\"{DATA_DIR}/train.json\"\n",
    "test_file = f\"{DATA_DIR}/test.json\"\n",
    "validation_file = f\"{DATA_DIR}/validation.json\"\n",
    "\n",
    "def load_jsonl_data(file_path, max_samples=None):\n",
    "    \"\"\"Loads a JSON Lines file and returns a list of dictionaries.\"\"\"\n",
    "    data = []\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        for i, obj in enumerate(reader):\n",
    "            if max_samples and i >= max_samples:\n",
    "                break\n",
    "            data.append(obj)\n",
    "    return data\n",
    "\n",
    "print(f\"Loading {NUM_TRAIN_SAMPLES} samples from the training file...\")\n",
    "train_data_raw = load_jsonl_data(train_file, max_samples=NUM_TRAIN_SAMPLES)\n",
    "print(f\"Loading {NUM_VAL_SAMPLES} samples from the validation file...\")\n",
    "validation_data_raw = load_jsonl_data(validation_file, max_samples=NUM_VAL_SAMPLES)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Data Preprocessing and Dataset Creation\n",
    "# ==============================================================================\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Processes raw data, validates inputs, and precomputes sentence embeddings.\"\"\"\n",
    "    sent_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    processed_samples = []\n",
    "    for i, item in enumerate(data):\n",
    "        original_text = item.get('original_text', '')\n",
    "        extractive_summary = item.get('extractive_summary', '')\n",
    "        if not isinstance(original_text, str) or not original_text.strip():\n",
    "            continue\n",
    "        if not isinstance(extractive_summary, str) or not extractive_summary.strip():\n",
    "            continue\n",
    "        document_sentences = nltk_sent_tokenize(original_text)\n",
    "        sentence_embeddings = sent_model.encode(document_sentences, convert_to_numpy=True).tolist()\n",
    "        processed_samples.append({\n",
    "            'document': original_text,\n",
    "            'extractive_summary': extractive_summary,\n",
    "            'id': i,\n",
    "            'document_sentences': document_sentences,\n",
    "            'sentence_embeddings': sentence_embeddings\n",
    "        })\n",
    "    print(f\"Processed {len(processed_samples)} valid samples out of {len(data)}\")\n",
    "    return processed_samples\n",
    "\n",
    "train_data = preprocess_data(train_data_raw)\n",
    "validation_data = preprocess_data(validation_data_raw)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Tokenizer, Label Alignment, and Caching\n",
    "# ==============================================================================\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "def tokenize_and_align_labels(examples, f1_threshold=F1_THRESHOLD):\n",
    "    \"\"\"Tokenizes documents without truncation, creates overlapping chunks, and aligns labels.\"\"\"\n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_global_attention_mask = []\n",
    "    all_labels = []\n",
    "    all_document_id = []\n",
    "\n",
    "    for idx, (doc, summary, doc_id, doc_sentences) in enumerate(zip(\n",
    "        examples['document'], examples['extractive_summary'], examples['id'], examples['document_sentences']\n",
    "    )):\n",
    "        print(f\"Processing document {idx} (ID: {doc_id})...\")\n",
    "        if not doc or not summary:\n",
    "            continue\n",
    "\n",
    "        summary_sentences = nltk_sent_tokenize(summary)\n",
    "        if not doc_sentences or not summary_sentences:\n",
    "            continue\n",
    "\n",
    "        tokenized_summary_sentences = [tokenizer.tokenize(s) for s in summary_sentences]\n",
    "        tokenized_doc = tokenizer.encode(doc, add_special_tokens=False)\n",
    "        sentence_labels = []\n",
    "        tokenized_doc_sentences = [tokenizer.encode(s, add_special_tokens=False) for s in doc_sentences]\n",
    "\n",
    "        # Assign labels to each sentence based on F1 score\n",
    "        for doc_sentence in doc_sentences:\n",
    "            doc_sent_tokens = tokenizer.tokenize(doc_sentence)\n",
    "            max_f1 = max(get_token_f1_score(doc_sent_tokens, sum_sent_tokens) for sum_sent_tokens in tokenized_summary_sentences) if tokenized_summary_sentences else 0.0\n",
    "            sentence_labels.append(1.0 if max_f1 >= f1_threshold else 0.0)\n",
    "\n",
    "        # Overlapping chunking\n",
    "        stride = CHUNK_SIZE - OVERLAP_SIZE - 2  # Reserve space for CLS and SEP tokens\n",
    "        token_idx = 0\n",
    "        sent_idx = 0\n",
    "        sent_token_idx = 0\n",
    "\n",
    "        while token_idx < len(tokenized_doc):\n",
    "            current_input_ids = [tokenizer.cls_token_id]\n",
    "            current_attention_mask = [1]\n",
    "            current_labels = [0.0]\n",
    "            tokens_added = 0\n",
    "\n",
    "            # Add tokens until chunk is full or document ends\n",
    "            while token_idx < len(tokenized_doc) and tokens_added < stride:\n",
    "                current_input_ids.append(tokenized_doc[token_idx])\n",
    "                current_attention_mask.append(1)\n",
    "                # Assign label based on current sentence\n",
    "                if sent_idx < len(doc_sentences):\n",
    "                    current_labels.append(sentence_labels[sent_idx])\n",
    "                else:\n",
    "                    current_labels.append(0.0)\n",
    "                token_idx += 1\n",
    "                tokens_added += 1\n",
    "\n",
    "                # Update sentence index if we've reached the end of current sentence tokens\n",
    "                if sent_idx < len(doc_sentences) and sent_token_idx < len(tokenized_doc_sentences[sent_idx]):\n",
    "                    sent_token_idx += 1\n",
    "                if sent_idx < len(doc_sentences) and sent_token_idx >= len(tokenized_doc_sentences[sent_idx]):\n",
    "                    sent_idx += 1\n",
    "                    sent_token_idx = 0\n",
    "\n",
    "            # Finalize chunk\n",
    "            current_input_ids.append(tokenizer.sep_token_id)\n",
    "            current_attention_mask.append(1)\n",
    "            current_labels.append(0.0)\n",
    "\n",
    "            # Pad to CHUNK_SIZE if necessary\n",
    "            padding_length = CHUNK_SIZE - len(current_input_ids)\n",
    "            if padding_length > 0:\n",
    "                current_input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "                current_attention_mask += [0] * padding_length\n",
    "                current_labels += [0.0] * padding_length\n",
    "\n",
    "            # Create global attention mask\n",
    "            global_attention_mask = [0] * len(current_input_ids)\n",
    "            global_attention_mask[0] = 1  # CLS token gets global attention\n",
    "\n",
    "            # Append to lists\n",
    "            all_input_ids.append(current_input_ids)\n",
    "            all_attention_mask.append(current_attention_mask)\n",
    "            all_global_attention_mask.append(global_attention_mask)\n",
    "            all_labels.append(current_labels)\n",
    "            all_document_id.append(doc_id)\n",
    "\n",
    "            # Move token_idx back for overlap\n",
    "            token_idx = max(0, token_idx - OVERLAP_SIZE)\n",
    "\n",
    "        # Skip if no valid chunks were created\n",
    "        if not all_input_ids or len(all_input_ids[-1]) < 10:\n",
    "            continue\n",
    "\n",
    "    return {\n",
    "        'input_ids': all_input_ids,\n",
    "        'attention_mask': all_attention_mask,\n",
    "        'global_attention_mask': all_global_attention_mask,\n",
    "        'labels': all_labels,\n",
    "        'document_id': all_document_id\n",
    "    }\n",
    "\n",
    "def load_or_generate_dataset(data, split_name, max_samples, f1_threshold=F1_THRESHOLD):\n",
    "    \"\"\"Loads a cached dataset or generates and caches a new one.\"\"\"\n",
    "    if not data:\n",
    "        print(f\"No valid data for {split_name} after preprocessing\")\n",
    "        return Dataset.from_dict({'input_ids': [], 'attention_mask': [], 'global_attention_mask': [], 'labels': [], 'document_id': []})\n",
    "\n",
    "    cache_key = f\"{split_name}-{max_samples}-{f1_threshold}\"\n",
    "    cache_path = os.path.join(CACHE_DIR, hashlib.md5(cache_key.encode()).hexdigest())\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading cached dataset from {cache_path}...\")\n",
    "        return load_from_disk(cache_path)\n",
    "\n",
    "    print(f\"Generating and caching dataset for {split_name} at {cache_path}...\")\n",
    "    dataset = Dataset.from_dict({\n",
    "        'document': [item['document'] for item in data],\n",
    "        'extractive_summary': [item['extractive_summary'] for item in data],\n",
    "        'id': [item['id'] for item in data],\n",
    "        'document_sentences': [item['document_sentences'] for item in data],\n",
    "        'sentence_embeddings': [item['sentence_embeddings'] for item in data]\n",
    "    })\n",
    "    dataset_tokenized = dataset.map(\n",
    "        lambda examples: tokenize_and_align_labels(examples, f1_threshold),\n",
    "        batched=True,\n",
    "        batch_size=4,\n",
    "        remove_columns=[\"document\", \"extractive_summary\", \"id\", \"document_sentences\", \"sentence_embeddings\"],\n",
    "        desc=f\"Tokenizing {split_name} data\",\n",
    "        num_proc=4\n",
    "    )\n",
    "\n",
    "    def is_valid_example(example):\n",
    "        if not isinstance(example['input_ids'], list) or not example['input_ids']:\n",
    "            return False\n",
    "        if len(example['input_ids']) != len(example['attention_mask']) or \\\n",
    "           len(example['input_ids']) != len(example['global_attention_mask']) or \\\n",
    "           len(example['input_ids']) != len(example['labels']):\n",
    "            return False\n",
    "        if 'document_id' not in example:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    dataset_tokenized = dataset_tokenized.filter(is_valid_example, desc=f\"Filtering invalid examples for {split_name}\")\n",
    "\n",
    "    if not os.path.exists(CACHE_DIR):\n",
    "        os.makedirs(CACHE_DIR)\n",
    "\n",
    "    dataset_tokenized.save_to_disk(cache_path)\n",
    "    print(\"Caching complete.\")\n",
    "    return dataset_tokenized\n",
    "\n",
    "# Clear cache to ensure new tokenization\n",
    "\"\"\"if os.path.exists(CACHE_DIR):\n",
    "    import shutil\n",
    "    shutil.rmtree(CACHE_DIR)\"\"\"\n",
    "train_dataset_tokenized = load_or_generate_dataset(train_data, 'train', NUM_TRAIN_SAMPLES)\n",
    "validation_dataset_tokenized = load_or_generate_dataset(validation_data, 'validation', NUM_VAL_SAMPLES)\n",
    "print(f\"Train dataset size: {len(train_dataset_tokenized)}\")\n",
    "print(f\"Validation dataset size: {len(validation_dataset_tokenized)}\")\n",
    "if len(train_dataset_tokenized) == 0 or len(validation_dataset_tokenized) == 0:\n",
    "    raise ValueError(\"One or both datasets are empty after processing.\")\n",
    "\n",
    "# Calculate pos_weight based on class imbalance\n",
    "def calculate_pos_weight(dataset):\n",
    "    \"\"\"Calculates pos_weight based on the ratio of negative to positive labels.\"\"\"\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    for example in dataset:\n",
    "        labels = example['labels']\n",
    "        pos_count += sum(1 for label in labels if label == 1.0)\n",
    "        neg_count += sum(1 for label in labels if label == 0.0)\n",
    "    pos_weight = neg_count / pos_count if pos_count > 0 else 1.0\n",
    "    return torch.tensor(pos_weight)\n",
    "\n",
    "pos_weight = calculate_pos_weight(train_dataset_tokenized)\n",
    "print(f\"Calculated pos_weight: {pos_weight.item():.2f}\")\n",
    "\n",
    "# Calculate average reference summary length for dynamic top_k\n",
    "def calculate_avg_summary_sentences(data):\n",
    "    \"\"\"Calculates the average number of sentences in reference summaries.\"\"\"\n",
    "    total_sentences = sum(len(nltk_sent_tokenize(item['extractive_summary'])) for item in data)\n",
    "    return max(1, round(total_sentences / len(data)))\n",
    "\n",
    "avg_summary_sentences = calculate_avg_summary_sentences(validation_data)\n",
    "print(f\"Average reference summary sentences: {avg_summary_sentences}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. Custom metrics for evaluation with ROUGE, Top-k, and MMR\n",
    "# ==============================================================================\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred, raw_data, tokenizer_obj, eval_dataset, top_k=avg_summary_sentences, lambda_mmr=1):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.sigmoid(torch.from_numpy(logits))\n",
    "    predictions_binary = (predictions > 0.5).numpy().astype(int)\n",
    "\n",
    "    labels_flat = labels.flatten()\n",
    "    predictions_flat = predictions_binary.flatten()\n",
    "\n",
    "    metrics = {}\n",
    "    if len(labels_flat) > 0:\n",
    "        metrics[\"f1\"] = f1_score(labels_flat, predictions_flat, average='binary', zero_division=0)\n",
    "        metrics[\"precision\"] = precision_score(labels_flat, predictions_flat, average='binary', zero_division=0)\n",
    "        metrics[\"recall\"] = recall_score(labels_flat, predictions_flat, average='binary', zero_division=0)\n",
    "        metrics[\"accuracy\"] = accuracy_score(labels_flat, predictions_flat)\n",
    "    else:\n",
    "        metrics[\"f1\"] = 0.0\n",
    "        metrics[\"precision\"] = 0.0\n",
    "        metrics[\"recall\"] = 0.0\n",
    "        metrics[\"accuracy\"] = 0.0\n",
    "\n",
    "    predicted_summaries = []\n",
    "    reference_summaries = []\n",
    "\n",
    "    doc_logits = defaultdict(list)\n",
    "    doc_attention = defaultdict(list)\n",
    "    for i, doc_id in enumerate(eval_dataset['document_id']):\n",
    "        doc_logits[doc_id].append(predictions[i].numpy())\n",
    "        doc_attention[doc_id].append(eval_dataset['attention_mask'][i])\n",
    "\n",
    "    for doc_id in sorted(doc_logits.keys()):\n",
    "        chunks = doc_logits[doc_id]\n",
    "        att_masks = doc_attention[doc_id]\n",
    "        reference_summaries.append(raw_data[doc_id]['extractive_summary'])\n",
    "        document = raw_data[doc_id]['document']\n",
    "        sentence_embeddings = np.array(raw_data[doc_id]['sentence_embeddings'])\n",
    "\n",
    "        aggregated_scores = []\n",
    "        for chunk, att_mask in zip(chunks, att_masks):\n",
    "            effective_len = sum(att_mask)\n",
    "            if effective_len < 3:\n",
    "                continue\n",
    "            content_scores = chunk[1:effective_len - 1]\n",
    "            aggregated_scores.extend(content_scores)\n",
    "\n",
    "        document_sentences = nltk_sent_tokenize(document)\n",
    "        sentence_scores = []\n",
    "        tokenized_doc_sentences = [tokenizer_obj.encode(s, add_special_tokens=False) for s in document_sentences]\n",
    "\n",
    "        start_index = 0\n",
    "        for sent_tokens in tokenized_doc_sentences:\n",
    "            end_index = start_index + len(sent_tokens)\n",
    "            if end_index > len(aggregated_scores):\n",
    "                break\n",
    "            sentence_logits = aggregated_scores[start_index:end_index]\n",
    "            sentence_scores.append(np.max(sentence_logits) if len(sentence_logits) > 0 else 0.0)\n",
    "            start_index = end_index\n",
    "\n",
    "        sentence_embeddings = torch.tensor(sentence_embeddings, dtype=torch.float32)\n",
    "        selected_indices = []\n",
    "        for _ in range(min(top_k, len(document_sentences))):\n",
    "            if not sentence_scores:\n",
    "                break\n",
    "            if not selected_indices:\n",
    "                best_idx = np.argmax(sentence_scores)\n",
    "                selected_indices.append(best_idx)\n",
    "            else:\n",
    "                mmr_scores = []\n",
    "                for i, score in enumerate(sentence_scores):\n",
    "                    if i in selected_indices:\n",
    "                        mmr_scores.append(-float('inf'))\n",
    "                        continue\n",
    "                    relevance = score\n",
    "                    max_similarity = max([util.pytorch_cos_sim(sentence_embeddings[i], sentence_embeddings[j]).item() for j in selected_indices])\n",
    "                    mmr_score = lambda_mmr * relevance - (1 - lambda_mmr) * max_similarity\n",
    "                    mmr_scores.append(mmr_score)\n",
    "                best_idx = np.argmax(mmr_scores)\n",
    "                if mmr_scores[best_idx] > 0:\n",
    "                    selected_indices.append(best_idx)\n",
    "\n",
    "        predicted_sentences = [document_sentences[i] for i in selected_indices]\n",
    "        if not predicted_sentences and document_sentences:\n",
    "            predicted_sentences.append(document_sentences[0])\n",
    "\n",
    "        predicted_summaries.append(\" \".join(predicted_sentences))\n",
    "\n",
    "    rouge_results = rouge_metric.compute(predictions=predicted_summaries, references=reference_summaries, use_stemmer=True)\n",
    "\n",
    "    rouge1_val = rouge_results[\"rouge1\"]\n",
    "    rouge2_val = rouge_results[\"rouge2\"]\n",
    "    rougeL_val = rouge_results[\"rougeL\"]\n",
    "    metrics.update({\n",
    "        \"rouge1\": rouge1_val.mid.fmeasure if hasattr(rouge1_val, 'mid') else rouge1_val,\n",
    "        \"rouge2\": rouge2_val.mid.fmeasure if hasattr(rouge2_val, 'mid') else rouge2_val,\n",
    "        \"rougeL\": rougeL_val.mid.fmeasure if hasattr(rougeL_val, 'mid') else rougeL_val,\n",
    "    })\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. Training Arguments and Trainer\n",
    "# ==============================================================================\n",
    "total_train_batch_size = 4 * 2\n",
    "approx_num_chunks = sum(1 for _ in train_dataset_tokenized) if train_dataset_tokenized else 5000\n",
    "max_steps = (approx_num_chunks // total_train_batch_size) * 5\n",
    "save_steps = max(500, max_steps // 10)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./extractive_summarization_results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=save_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=save_steps,\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    max_steps=max_steps,\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "# Find the latest checkpoint\n",
    "latest_checkpoint = None\n",
    "if os.path.exists(training_args.output_dir):\n",
    "    checkpoints = [d for d in os.listdir(training_args.output_dir) if d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "        latest_checkpoint = os.path.join(training_args.output_dir, latest_checkpoint)\n",
    "\n",
    "model = LongformerExtractiveSummarizationModel(pos_weight=pos_weight)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_tokenized,\n",
    "    eval_dataset=validation_dataset_tokenized,\n",
    "    compute_metrics=lambda p: compute_metrics(p, validation_data, tokenizer, validation_dataset_tokenized, top_k=avg_summary_sentences, lambda_mmr=1),\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. Start the fine-tuning\n",
    "# ==============================================================================\n",
    "print(\"Starting training...\")\n",
    "trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "print(\"Training finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
