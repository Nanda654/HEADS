{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87453510",
   "metadata": {},
   "source": [
    "# user defined length for extractive summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a97836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import LongformerModel, LongformerTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import hashlib\n",
    "\n",
    "# Download NLTK punkt tokenizer if not present\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Re-define the Custom Model Architecture\n",
    "# ==============================================================================\n",
    "class LongformerExtractiveSummarizationModel(nn.Module):\n",
    "    def __init__(self, pos_weight=None):\n",
    "        super(LongformerExtractiveSummarizationModel, self).__init__()\n",
    "        self.longformer = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.classifier = nn.Linear(self.longformer.config.hidden_size, 1)\n",
    "        self.pos_weight = pos_weight if pos_weight is not None else torch.tensor(1.0)\n",
    "    def forward(self, input_ids=None, attention_mask=None, global_attention_mask=None, labels=None):\n",
    "        outputs = self.longformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            global_attention_mask=global_attention_mask\n",
    "        )\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        logits = logits.squeeze(-1)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight.to(logits.device))\n",
    "            loss = loss_fct(logits, labels.float())\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Setup\n",
    "# ==============================================================================\n",
    "# Path to the best checkpoint\n",
    "CHECKPOINT_PATH = \"./extractive_summarization_results/checkpoint-3148\"\n",
    "CHUNK_SIZE = 4096\n",
    "# Lambda for MMR: 0.7 is a good starting point for balancing relevance and diversity\n",
    "LAMBDA_MMR = 0.7\n",
    "# Initialize tokenizer and sentence embedding model\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "sent_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Load the Fine-tuned Model\n",
    "# ==============================================================================\n",
    "def load_model(checkpoint_path):\n",
    "    \"\"\"Loads the fine-tuned model from a checkpoint.\"\"\"\n",
    "    print(f\"Loading model from {checkpoint_path}...\")\n",
    "    model = LongformerExtractiveSummarizationModel()\n",
    "   \n",
    "    # Check for available model files\n",
    "    model_file = None\n",
    "    if os.path.exists(os.path.join(checkpoint_path, \"pytorch_model.bin\")):\n",
    "        model_file = \"pytorch_model.bin\"\n",
    "    elif os.path.exists(os.path.join(checkpoint_path, \"model.safetensors\")):\n",
    "        model_file = \"model.safetensors\"\n",
    "   \n",
    "    if not model_file:\n",
    "        raise FileNotFoundError(f\"No valid model file found in {checkpoint_path}. Expected 'pytorch_model.bin' or 'model.safetensors'.\")\n",
    "    # Load state dictionary\n",
    "    if model_file.endswith(\".safetensors\"):\n",
    "        from safetensors.torch import load_file\n",
    "        state_dict = load_file(os.path.join(checkpoint_path, model_file))\n",
    "    else:\n",
    "        state_dict = torch.load(os.path.join(checkpoint_path, model_file), map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Main Summary Generation Function with Hierarchical Chunking (No MMR)\n",
    "# ==============================================================================\n",
    "def generate_extractive_summary(document: str, top_k: int) -> str:\n",
    "    \"\"\"Generates an extractive summary for a given document with a specified number of sentences using hierarchical chunking.\"\"\"\n",
    "    if not document or not document.strip():\n",
    "        return \"Input document is empty.\"\n",
    "    \n",
    "    # Split document into paragraphs\n",
    "    paragraphs = [p.strip() for p in document.split('\\n\\n') if p.strip()]\n",
    "    if not paragraphs:\n",
    "        document_sentences = sent_tokenize(document)\n",
    "        if not document_sentences:\n",
    "            return \"No sentences found in the document.\"\n",
    "    else:\n",
    "        document_sentences = [sent for para in paragraphs for sent in sent_tokenize(para)]\n",
    "    \n",
    "    if not document_sentences:\n",
    "        return \"No sentences found in the document.\"\n",
    "   \n",
    "    # Ensure top_k does not exceed the number of available sentences\n",
    "    top_k = min(top_k, len(document_sentences))\n",
    "   \n",
    "    # Get sentence embeddings (still needed for potential future use or debugging)\n",
    "    sentence_embeddings = sent_model.encode(document_sentences, convert_to_numpy=True)\n",
    "   \n",
    "    # Tokenize and hierarchically chunk the document\n",
    "    all_chunks_tokens = []\n",
    "    all_chunks_attention = []\n",
    "    all_chunks_global_attention = []\n",
    "    sentence_to_chunk_map = []  # Track which chunk each sentence belongs to\n",
    "    \n",
    "    for para_idx, paragraph in enumerate(paragraphs):\n",
    "        current_input_ids = [tokenizer.cls_token_id]\n",
    "        current_attention_mask = [1]\n",
    "        current_sent_start_idx = len([s for p in paragraphs[:para_idx] for s in sent_tokenize(p)])\n",
    "        \n",
    "        for sent_idx, sentence in enumerate(sent_tokenize(paragraph)):\n",
    "            global_sent_idx = current_sent_start_idx + sent_idx\n",
    "            sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "            if len(current_input_ids) + len(sentence_tokens) + 1 > CHUNK_SIZE:\n",
    "                # Chunk is full, finalize and start a new one\n",
    "                padding_length = CHUNK_SIZE - len(current_input_ids)\n",
    "                current_input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "                current_attention_mask += [0] * padding_length\n",
    "                global_attention_mask = [0] * CHUNK_SIZE\n",
    "                global_attention_mask[0] = 1\n",
    "                \n",
    "                all_chunks_tokens.append(current_input_ids)\n",
    "                all_chunks_attention.append(current_attention_mask)\n",
    "                all_chunks_global_attention.append(global_attention_mask)\n",
    "                sentence_to_chunk_map.append((global_sent_idx, len(all_chunks_tokens) - 1))  # Map last sentence to chunk\n",
    "                \n",
    "                current_input_ids = [tokenizer.cls_token_id]\n",
    "                current_attention_mask = [1]\n",
    "            \n",
    "            current_input_ids += sentence_tokens\n",
    "            current_attention_mask += [1] * len(sentence_tokens)\n",
    "            sentence_to_chunk_map.append((global_sent_idx, len(all_chunks_tokens)))\n",
    "        \n",
    "        # Finalize the last chunk for this paragraph\n",
    "        if len(current_input_ids) > 1:\n",
    "            current_input_ids.append(tokenizer.sep_token_id)\n",
    "            current_attention_mask.append(1)\n",
    "            padding_length = CHUNK_SIZE - len(current_input_ids)\n",
    "            current_input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "            current_attention_mask += [0] * padding_length\n",
    "            global_attention_mask = [0] * CHUNK_SIZE\n",
    "            global_attention_mask[0] = 1\n",
    "            \n",
    "            all_chunks_tokens.append(current_input_ids)\n",
    "            all_chunks_attention.append(current_attention_mask)\n",
    "            all_chunks_global_attention.append(global_attention_mask)\n",
    "            sentence_to_chunk_map.append((current_sent_start_idx + len(sent_tokenize(paragraph)) - 1, len(all_chunks_tokens) - 1))\n",
    "\n",
    "    # Convert to tensors\n",
    "    input_ids_tensor = torch.tensor(all_chunks_tokens)\n",
    "    attention_mask_tensor = torch.tensor(all_chunks_attention)\n",
    "    global_attention_mask_tensor = torch.tensor(all_chunks_global_attention)\n",
    "    \n",
    "    # Get logits from the model\n",
    "    with torch.no_grad():\n",
    "        logits = model(\n",
    "            input_ids=input_ids_tensor,\n",
    "            attention_mask=attention_mask_tensor,\n",
    "            global_attention_mask=global_attention_mask_tensor\n",
    "        )\n",
    "    \n",
    "    # Post-process logits to get sentence scores with hierarchical mapping\n",
    "    predictions = torch.sigmoid(logits)\n",
    "    aggregated_scores = []\n",
    "    for chunk_idx, (chunk, att_mask) in enumerate(zip(predictions, all_chunks_attention)):\n",
    "        effective_len = sum(att_mask)\n",
    "        if effective_len > 2:\n",
    "            content_scores = chunk[1:effective_len - 1].tolist()\n",
    "            aggregated_scores.extend(content_scores)\n",
    "    \n",
    "    sentence_scores = [0.0] * len(document_sentences)\n",
    "    for global_sent_idx, chunk_idx in sentence_to_chunk_map:\n",
    "        if global_sent_idx < len(document_sentences):\n",
    "            start_token = sum(len(tokenizer.encode(document_sentences[s], add_special_tokens=False)) for s in range(global_sent_idx))\n",
    "            end_token = start_token + len(tokenizer.encode(document_sentences[global_sent_idx], add_special_tokens=False))\n",
    "            if end_token <= len(aggregated_scores):\n",
    "                sentence_logits = aggregated_scores[start_token:end_token]\n",
    "                sentence_scores[global_sent_idx] = max(sentence_logits) if len(sentence_logits) > 0 else 0.0\n",
    "    \n",
    "    # Select top-k sentences based on scores (no MMR)\n",
    "    selected_indices = np.argsort(sentence_scores)[-top_k:][::-1]\n",
    "    \n",
    "    # Reconstruct summary from selected sentences\n",
    "    predicted_sentences = [document_sentences[i] for i in selected_indices]\n",
    "    return \" \".join(predicted_sentences)\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. Example Usage\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the fine-tuned model\n",
    "    try:\n",
    "        model = load_model(CHECKPOINT_PATH)\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        exit()\n",
    "    # Get user input\n",
    "    user_input = input(\"Enter the document text to summarize: \\n\\n\")\n",
    "    try:\n",
    "        top_k_input = int(input(\"\\nEnter the desired number of sentences for the summary (e.g., 3, 5): \"))\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Using default of 3 sentences.\")\n",
    "        top_k_input = 3\n",
    "    # Generate and print the summary\n",
    "    summary = generate_extractive_summary(user_input, top_k_input)\n",
    "    print(\"\\n--- Generated Summary ---\")\n",
    "    print(summary)\n",
    "    print(\"-------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
