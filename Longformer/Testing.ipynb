{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c80c32",
   "metadata": {},
   "source": [
    "# Testing Longformer on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c991d316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import LongformerModel, LongformerTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import evaluate\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download NLTK punkt tokenizer\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "def nltk_sent_tokenize(text):\n",
    "    \"\"\"Splits text into sentences using NLTK.\"\"\"\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "def get_token_f1_score(candidate_tokens, reference_tokens):\n",
    "    \"\"\"Computes a token-based F1 score between a candidate and reference sentence.\"\"\"\n",
    "    candidate_tokens_set = set(candidate_tokens)\n",
    "    reference_tokens_set = set(reference_tokens)\n",
    "    if not candidate_tokens_set or not reference_tokens_set:\n",
    "        return 0.0\n",
    "    intersection = len(candidate_tokens_set.intersection(reference_tokens_set))\n",
    "    precision = intersection / len(candidate_tokens_set)\n",
    "    recall = intersection / len(reference_tokens_set)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Custom Model Definition for Longformer\n",
    "# ==============================================================================\n",
    "class LongformerExtractiveSummarizationModel(nn.Module):\n",
    "    \"\"\"A custom PyTorch model for extractive summarization using a Longformer backbone.\"\"\"\n",
    "    def __init__(self, pos_weight=None):\n",
    "        super(LongformerExtractiveSummarizationModel, self).__init__()\n",
    "        self.longformer = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.classifier = nn.Linear(self.longformer.config.hidden_size, 1)\n",
    "        self.pos_weight = pos_weight if pos_weight is not None else torch.tensor(1.0)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, global_attention_mask=None, labels=None):\n",
    "        outputs = self.longformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            global_attention_mask=global_attention_mask\n",
    "        )\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        logits = logits.squeeze(-1)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight.to(logits.device))\n",
    "            loss = loss_fct(logits, labels.float())\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Configuration\n",
    "# ==============================================================================\n",
    "DATA_DIR = \"govreport_tfidf_vscode2\"\n",
    "NUM_TEST_SAMPLES = 200\n",
    "F1_THRESHOLD = 0.3\n",
    "CHUNK_SIZE = 4096\n",
    "test_file = f\"{DATA_DIR}/test.json\"\n",
    "CHECKPOINT_DIR = \"./extractive_summarization_results/\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Data Loading and Preprocessing\n",
    "# ==============================================================================\n",
    "def load_jsonl_data(file_path, max_samples=None):\n",
    "    \"\"\"Loads a JSON Lines file and returns a list of dictionaries.\"\"\"\n",
    "    data = []\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        for i, obj in enumerate(reader):\n",
    "            if max_samples and i >= max_samples:\n",
    "                break\n",
    "            data.append(obj)\n",
    "    return data\n",
    "\n",
    "print(f\"Loading {NUM_TEST_SAMPLES} samples from the test file...\")\n",
    "test_data_raw = load_jsonl_data(test_file, max_samples=NUM_TEST_SAMPLES)\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Processes raw data, validates inputs, and precomputes sentence embeddings.\"\"\"\n",
    "    sent_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    processed_samples = []\n",
    "    for i, item in enumerate(data):\n",
    "        original_text = item.get('original_text', '')\n",
    "        extractive_summary = item.get('extractive_summary', '')\n",
    "        if not isinstance(original_text, str) or not original_text.strip():\n",
    "            continue\n",
    "        if not isinstance(extractive_summary, str) or not extractive_summary.strip():\n",
    "            continue\n",
    "        document_sentences = nltk_sent_tokenize(original_text)\n",
    "        sentence_embeddings = sent_model.encode(document_sentences, convert_to_numpy=True).tolist()\n",
    "        processed_samples.append({\n",
    "            'document': original_text,\n",
    "            'extractive_summary': extractive_summary,\n",
    "            'id': i,\n",
    "            'document_sentences': document_sentences,\n",
    "            'sentence_embeddings': sentence_embeddings\n",
    "        })\n",
    "    print(f\"Processed {len(processed_samples)} valid samples out of {len(data)}\")\n",
    "    return processed_samples\n",
    "\n",
    "test_data = preprocess_data(test_data_raw)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Tokenization and Dataset Creation\n",
    "# ==============================================================================\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "def tokenize_and_align_labels(examples, f1_threshold=F1_THRESHOLD):\n",
    "    \"\"\"Tokenizes documents without truncation, creates hierarchical chunks, and aligns labels.\"\"\"\n",
    "    print(\"f1_threshold\", f1_threshold)\n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_global_attention_mask = []\n",
    "    all_labels = []\n",
    "    all_document_id = []\n",
    "\n",
    "    for idx, (doc, summary, doc_id, doc_sentences) in enumerate(zip(\n",
    "        examples['document'], examples['extractive_summary'], examples['id'], examples['document_sentences']\n",
    "    )):\n",
    "        print(f\"Processing test document {idx} (ID: {doc_id})...\")\n",
    "        if not doc or not summary:\n",
    "            continue\n",
    "\n",
    "        summary_sentences = nltk_sent_tokenize(summary)\n",
    "        if not doc_sentences or not summary_sentences:\n",
    "            continue\n",
    "\n",
    "        tokenized_summary_sentences = [tokenizer.tokenize(s) for s in summary_sentences]\n",
    "        sentence_labels = []\n",
    "        tokenized_doc_sentences = []\n",
    "        for doc_sentence in doc_sentences:\n",
    "            doc_sent_tokens = tokenizer.tokenize(doc_sentence)\n",
    "            tokenized_doc_sentences.append(tokenizer.encode(doc_sentence, add_special_tokens=False))\n",
    "            max_f1 = max(get_token_f1_score(doc_sent_tokens, sum_sent_tokens) for sum_sent_tokens in tokenized_summary_sentences) if tokenized_summary_sentences else 0.0\n",
    "            sentence_labels.append(1.0 if max_f1 >= f1_threshold else 0.0)\n",
    "\n",
    "        current_input_ids = [tokenizer.cls_token_id]\n",
    "        current_attention_mask = [1]\n",
    "        current_labels = [0.0]\n",
    "        current_sent_idx = 0\n",
    "\n",
    "        while current_sent_idx < len(doc_sentences):\n",
    "            while current_sent_idx < len(doc_sentences):\n",
    "                sent_tokens = tokenized_doc_sentences[current_sent_idx]\n",
    "                if len(current_input_ids) + len(sent_tokens) + 1 > CHUNK_SIZE:\n",
    "                    break\n",
    "                current_input_ids += sent_tokens\n",
    "                current_attention_mask += [1] * len(sent_tokens)\n",
    "                current_labels += [sentence_labels[current_sent_idx]] * len(sent_tokens)\n",
    "                current_sent_idx += 1\n",
    "\n",
    "            if len(current_input_ids) > 1:\n",
    "                current_input_ids.append(tokenizer.sep_token_id)\n",
    "                current_attention_mask.append(1)\n",
    "                current_labels.append(0.0)\n",
    "\n",
    "                padding_length = CHUNK_SIZE - len(current_input_ids)\n",
    "                if padding_length > 0:\n",
    "                    current_input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "                    current_attention_mask += [0] * padding_length\n",
    "                    current_labels += [0.0] * padding_length\n",
    "\n",
    "                global_attention_mask = [0] * len(current_input_ids)\n",
    "                global_attention_mask[0] = 1\n",
    "\n",
    "                all_input_ids.append(current_input_ids)\n",
    "                all_attention_mask.append(current_attention_mask)\n",
    "                all_global_attention_mask.append(global_attention_mask)\n",
    "                all_labels.append(current_labels)\n",
    "                all_document_id.append(doc_id)\n",
    "\n",
    "            if current_sent_idx < len(doc_sentences):\n",
    "                current_input_ids = [tokenizer.cls_token_id]\n",
    "                current_attention_mask = [1]\n",
    "                current_labels = [0.0]\n",
    "\n",
    "        if not all_input_ids or len(all_input_ids[-1]) < 10:\n",
    "            continue\n",
    "\n",
    "    return {\n",
    "        'input_ids': all_input_ids,\n",
    "        'attention_mask': all_attention_mask,\n",
    "        'global_attention_mask': all_global_attention_mask,\n",
    "        'labels': all_labels,\n",
    "        'document_id': all_document_id\n",
    "    }\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'document': [item['document'] for item in test_data],\n",
    "    'extractive_summary': [item['extractive_summary'] for item in test_data],\n",
    "    'id': [item['id'] for item in test_data],\n",
    "    'document_sentences': [item['document_sentences'] for item in test_data],\n",
    "    'sentence_embeddings': [item['sentence_embeddings'] for item in test_data]\n",
    "})\n",
    "test_dataset_tokenized = test_dataset.map(\n",
    "    lambda examples: tokenize_and_align_labels(examples, F1_THRESHOLD),\n",
    "    batched=True,\n",
    "    batch_size=4,\n",
    "    remove_columns=[\"document\", \"extractive_summary\", \"id\", \"document_sentences\", \"sentence_embeddings\"],\n",
    "    desc=\"Tokenizing test data\",\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "def is_valid_example(example):\n",
    "    if not isinstance(example['input_ids'], list) or not example['input_ids']:\n",
    "        return False\n",
    "    if len(example['input_ids']) != len(example['attention_mask']) or \\\n",
    "       len(example['input_ids']) != len(example['global_attention_mask']) or \\\n",
    "       len(example['input_ids']) != len(example['labels']):\n",
    "        return False\n",
    "    if 'document_id' not in example:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "test_dataset_tokenized = test_dataset_tokenized.filter(is_valid_example, desc=\"Filtering invalid test examples\")\n",
    "print(f\"Test dataset size: {len(test_dataset_tokenized)}\")\n",
    "if len(test_dataset_tokenized) == 0:\n",
    "    raise ValueError(\"Test dataset is empty after processing.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. Load Trained Model\n",
    "# ==============================================================================\n",
    "# Find the latest checkpoint with a valid model file\n",
    "latest_checkpoint = None\n",
    "model_file = None\n",
    "if os.path.exists(CHECKPOINT_DIR):\n",
    "    checkpoints = [d for d in os.listdir(CHECKPOINT_DIR) if d.startswith(\"checkpoint-\")]\n",
    "    if checkpoints:\n",
    "        for checkpoint in sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]), reverse=True):\n",
    "            checkpoint_path = os.path.join(CHECKPOINT_DIR, checkpoint)\n",
    "            for file_name in [\"pytorch_model.bin\", \"model.bin\", \"model.safetensors\"]:\n",
    "                model_path = os.path.join(checkpoint_path, file_name)\n",
    "                if os.path.exists(model_path):\n",
    "                    latest_checkpoint = checkpoint_path\n",
    "                    model_file = file_name\n",
    "                    break\n",
    "            if latest_checkpoint:\n",
    "                break\n",
    "        if latest_checkpoint is None:\n",
    "            raise FileNotFoundError(f\"No valid model file (e.g., pytorch_model.bin, model.bin, model.safetensors) found in any checkpoint. Available checkpoints and files: {[(c, os.listdir(os.path.join(CHECKPOINT_DIR, c))) for c in checkpoints]}\")\n",
    "    else:\n",
    "        raise ValueError(f\"No checkpoints found in {CHECKPOINT_DIR}. Available directories: {os.listdir(CHECKPOINT_DIR)}\")\n",
    "else:\n",
    "    raise ValueError(f\"Checkpoint directory {CHECKPOINT_DIR} does not exist\")\n",
    "\n",
    "# Calculate pos_weight for consistency\n",
    "def calculate_pos_weight(dataset):\n",
    "    \"\"\"Calculates pos_weight based on the ratio of negative to positive labels.\"\"\"\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    for example in dataset:\n",
    "        labels = example['labels']\n",
    "        pos_count += sum(1 for label in labels if label == 1.0)\n",
    "        neg_count += sum(1 for label in labels if label == 0.0)\n",
    "    pos_weight = neg_count / pos_count if pos_count > 0 else 1.0\n",
    "    return torch.tensor(pos_weight)\n",
    "\n",
    "pos_weight = calculate_pos_weight(test_dataset_tokenized)\n",
    "print(f\"Calculated pos_weight: {pos_weight.item():.2f}\")\n",
    "latest_checkpoint = \"./extractive_summarization_results/checkpoint-3148\"\n",
    "# Load the model from the checkpoint\n",
    "print(f\"Loading model from {latest_checkpoint} using {model_file}...\")\n",
    "model = LongformerExtractiveSummarizationModel(pos_weight=pos_weight)\n",
    "if model_file == \"model.safetensors\":\n",
    "    from safetensors.torch import load_file\n",
    "    model.load_state_dict(load_file(os.path.join(latest_checkpoint, model_file)))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(os.path.join(latest_checkpoint, model_file)))\n",
    "model.eval()\n",
    "\n",
    "# Calculate average reference summary length for dynamic top_k\n",
    "def calculate_avg_summary_sentences(data):\n",
    "    \"\"\"Calculates the average number of sentences in reference summaries.\"\"\"\n",
    "    total_sentences = sum(len(nltk_sent_tokenize(item['extractive_summary'])) for item in data)\n",
    "    return max(1, round(total_sentences / len(data)))\n",
    "\n",
    "avg_summary_sentences = calculate_avg_summary_sentences(test_data)\n",
    "print(f\"Average reference summary sentences: {avg_summary_sentences}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. Evaluation Metrics\n",
    "# ==============================================================================\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "def compute_metrics(eval_pred, raw_data, tokenizer_obj, eval_dataset, top_k=avg_summary_sentences, lambda_mmr=1):\n",
    "    print(\"lambda_mmr\", lambda_mmr)\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.sigmoid(torch.from_numpy(logits))\n",
    "    predictions_binary = (predictions > 0.5).numpy().astype(int)\n",
    "\n",
    "    labels_flat = labels.flatten()\n",
    "    predictions_flat = predictions_binary.flatten()\n",
    "\n",
    "    metrics = {}\n",
    "    if len(labels_flat) > 0:\n",
    "        metrics[\"f1\"] = f1_score(labels_flat, predictions_flat, average='binary', zero_division=0)\n",
    "        metrics[\"precision\"] = precision_score(labels_flat, predictions_flat, average='binary', zero_division=0)\n",
    "        metrics[\"recall\"] = recall_score(labels_flat, predictions_flat, average='binary', zero_division=0)\n",
    "        metrics[\"accuracy\"] = accuracy_score(labels_flat, predictions_flat)\n",
    "    else:\n",
    "        metrics[\"f1\"] = 0.0\n",
    "        metrics[\"precision\"] = 0.0\n",
    "        metrics[\"recall\"] = 0.0\n",
    "        metrics[\"accuracy\"] = 0.0\n",
    "\n",
    "    predicted_summaries = []\n",
    "    reference_summaries = []\n",
    "\n",
    "    doc_logits = defaultdict(list)\n",
    "    doc_attention = defaultdict(list)\n",
    "    for i, doc_id in enumerate(eval_dataset['document_id']):\n",
    "        doc_logits[doc_id].append(predictions[i].numpy())\n",
    "        doc_attention[doc_id].append(eval_dataset['attention_mask'][i])\n",
    "\n",
    "    for doc_id in sorted(doc_logits.keys()):\n",
    "        chunks = doc_logits[doc_id]\n",
    "        att_masks = doc_attention[doc_id]\n",
    "        reference_summaries.append(raw_data[doc_id]['extractive_summary'])\n",
    "        document = raw_data[doc_id]['document']\n",
    "        sentence_embeddings = np.array(raw_data[doc_id]['sentence_embeddings'])\n",
    "\n",
    "        aggregated_scores = []\n",
    "        for chunk, att_mask in zip(chunks, att_masks):\n",
    "            effective_len = sum(att_mask)\n",
    "            if effective_len < 3:\n",
    "                continue\n",
    "            content_scores = chunk[1:effective_len - 1]\n",
    "            aggregated_scores.extend(content_scores)\n",
    "\n",
    "        document_sentences = nltk_sent_tokenize(document)\n",
    "        sentence_scores = []\n",
    "        tokenized_doc_sentences = [tokenizer_obj.encode(s, add_special_tokens=False) for s in document_sentences]\n",
    "\n",
    "        start_index = 0\n",
    "        for sent_tokens in tokenized_doc_sentences:\n",
    "            end_index = start_index + len(sent_tokens)\n",
    "            if end_index > len(aggregated_scores):\n",
    "                break\n",
    "            sentence_logits = aggregated_scores[start_index:end_index]\n",
    "            sentence_scores.append(np.max(sentence_logits) if len(sentence_logits) > 0 else 0.0)\n",
    "            start_index = end_index\n",
    "\n",
    "        # Remove MMR and select top-k sentences based on scores\n",
    "        selected_indices = np.argsort(sentence_scores)[-top_k:][::-1]\n",
    "\n",
    "        predicted_sentences = [document_sentences[i] for i in selected_indices]\n",
    "        if not predicted_sentences and document_sentences:\n",
    "            predicted_sentences.append(document_sentences[0])\n",
    "\n",
    "        predicted_summaries.append(\" \".join(predicted_sentences))\n",
    "\n",
    "    # ROUGE\n",
    "    rouge_results = rouge_metric.compute(predictions=predicted_summaries, references=reference_summaries, use_stemmer=True)\n",
    "    rouge1_val = rouge_results[\"rouge1\"]\n",
    "    rouge2_val = rouge_results[\"rouge2\"]\n",
    "    rougeL_val = rouge_results[\"rougeL\"]\n",
    "    metrics.update({\n",
    "        \"rouge1\": rouge1_val.mid.fmeasure if hasattr(rouge1_val, 'mid') else rouge1_val,\n",
    "        \"rouge2\": rouge2_val.mid.fmeasure if hasattr(rouge2_val, 'mid') else rouge2_val,\n",
    "        \"rougeL\": rougeL_val.mid.fmeasure if hasattr(rougeL_val, 'mid') else rougeL_val,\n",
    "    })\n",
    "\n",
    "    # METEOR\n",
    "    meteor_result = meteor_metric.compute(predictions=predicted_summaries, references=reference_summaries)\n",
    "    metrics[\"meteor\"] = meteor_result[\"meteor\"]\n",
    "\n",
    "    # BERTScore\n",
    "    P, R, F1 = bert_score(predicted_summaries, reference_summaries, lang=\"en\", model_type=\"bert-base-uncased\")\n",
    "    metrics[\"bertscore_precision\"] = P.mean().item()\n",
    "    metrics[\"bertscore_recall\"] = R.mean().item()\n",
    "    metrics[\"bertscore_f1\"] = F1.mean().item()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. Setup Trainer for Testing\n",
    "# ==============================================================================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CHECKPOINT_DIR,\n",
    "    per_device_eval_batch_size=4,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_dataset_tokenized,\n",
    "    compute_metrics=lambda p: compute_metrics(p, test_data, tokenizer, test_dataset_tokenized, top_k=avg_summary_sentences),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 8. Evaluate the Model\n",
    "# ==============================================================================\n",
    "print(\"Starting evaluation...\")\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Evaluation finished.\")\n",
    "print(f\"Test Metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5156812",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "attachments": {
    "scores_pipeline.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "334a416c",
   "metadata": {},
   "source": [
    "![scores_pipeline.png](attachment:scores_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe307e9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
