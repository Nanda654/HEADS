{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af9ec89f",
   "metadata": {},
   "source": [
    "# Testing on Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ebab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jsonlines\n",
    "import torch\n",
    "import multiprocessing\n",
    "from transformers import LongformerModel, LongformerTokenizerFast\n",
    "import spacy\n",
    "import evaluate\n",
    "from typing import List, Dict\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# ============================================================\n",
    "# Force 32 CPU cores for all libraries\n",
    "# ============================================================\n",
    "NUM_CORES = 32\n",
    "torch.set_num_threads(NUM_CORES)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(NUM_CORES)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(NUM_CORES)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "# ============================================================\n",
    "# Spacy sentence tokenizer\n",
    "# ============================================================\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"tagger\", \"lemmatizer\"])\n",
    "except OSError:\n",
    "    print(\"Downloading spacy model 'en_core_web_sm'...\")\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"tagger\", \"lemmatizer\"])\n",
    "\n",
    "def spacy_sent_tokenize(text):\n",
    "    # nlp.pipe uses parallel processes (n_process=NUM_CORES)\n",
    "    return [sent.text.strip() for sent in nlp(text).sents if sent.text.strip()]\n",
    "\n",
    "# ============================================================\n",
    "# Config\n",
    "# ============================================================\n",
    "DATA_DIR = \"govreport_tfidf_vscode2\"\n",
    "NUM_VAL_SAMPLES = 2000\n",
    "F1_THRESHOLD = 0.4\n",
    "MAX_LENGTH = 4096\n",
    "validation_file = f\"{DATA_DIR}/validation.json\"\n",
    "\n",
    "# ============================================================\n",
    "# Load JSONL\n",
    "# ============================================================\n",
    "def load_jsonl_data(file_path, max_samples=None):\n",
    "    data = []\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        for i, obj in enumerate(reader):\n",
    "            if max_samples and i >= max_samples:\n",
    "                break\n",
    "            data.append(obj)\n",
    "    return data\n",
    "\n",
    "# ============================================================\n",
    "# Token-level F1 score\n",
    "# ============================================================\n",
    "def get_token_f1_score(candidate_tokens, reference_tokens):\n",
    "    candidate_tokens_set = set(candidate_tokens)\n",
    "    reference_tokens_set = set(reference_tokens)\n",
    "    if not candidate_tokens_set or not reference_tokens_set:\n",
    "        return 0.0\n",
    "    intersection = len(candidate_tokens_set & reference_tokens_set)\n",
    "    precision = intersection / len(candidate_tokens_set)\n",
    "    recall = intersection / len(reference_tokens_set)\n",
    "    return 0.0 if precision + recall == 0 else 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# ============================================================\n",
    "# Prepare single example\n",
    "# ============================================================\n",
    "def prepare_single_example(document, summary, tokenizer):\n",
    "    all_doc_sentences = spacy_sent_tokenize(document)\n",
    "    summary_sentences = spacy_sent_tokenize(summary)\n",
    "    tokenized_summary = [tokenizer.tokenize(s) for s in summary_sentences]\n",
    "    \n",
    "    chunk_text = \" \".join(all_doc_sentences)\n",
    "    tokenized_chunk = tokenizer(chunk_text, truncation=True, max_length=MAX_LENGTH, return_offsets_mapping=True)\n",
    "\n",
    "    sentence_spans = []\n",
    "    sentence_labels = []\n",
    "    current_offset = 0\n",
    "\n",
    "    for chunk_sent in all_doc_sentences:\n",
    "        sent_tokens = tokenizer.tokenize(chunk_sent)\n",
    "        start_token_idx, end_token_idx = -1, -1\n",
    "        \n",
    "        start_offset = chunk_text.find(chunk_sent, current_offset)\n",
    "        if start_offset == -1:\n",
    "            current_offset += len(chunk_sent) + 1\n",
    "            continue\n",
    "        end_offset = start_offset + len(chunk_sent)\n",
    "        current_offset = end_offset + 1\n",
    "\n",
    "        for j, (start_char, end_char) in enumerate(tokenized_chunk['offset_mapping']):\n",
    "            if start_char == start_offset and start_token_idx == -1:\n",
    "                start_token_idx = j\n",
    "            if end_char == end_offset-1 and end_token_idx == -1:\n",
    "                end_token_idx = j + 1\n",
    "            if start_token_idx != -1 and end_token_idx != -1:\n",
    "                break\n",
    "        \n",
    "        if start_token_idx != -1 and end_token_idx != -1:\n",
    "            sentence_spans.append([start_token_idx, end_token_idx])\n",
    "            max_f1 = max((get_token_f1_score(sent_tokens, sum_tokens) for sum_tokens in tokenized_summary), default=0.0)\n",
    "            label = 1.0 if max_f1 >= F1_THRESHOLD else 0.0\n",
    "            sentence_labels.append(label)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized_chunk['input_ids'],\n",
    "        \"attention_mask\": tokenized_chunk['attention_mask'],\n",
    "        \"sentence_spans\": sentence_spans,\n",
    "        \"labels\": sentence_labels,\n",
    "        \"document_text\": document,\n",
    "        \"summary_text\": summary\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# Evaluator\n",
    "# ============================================================\n",
    "class BaseModelRougeEvaluator:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', add_prefix_space=True)\n",
    "        self.model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "        self.classifier = torch.nn.Linear(self.model.config.hidden_size, 1)\n",
    "        self.rouge_metric = evaluate.load(\"rouge\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.classifier.to(self.device)\n",
    "\n",
    "    def run_evaluation(self, file_path, num_samples):\n",
    "        validation_data_raw = load_jsonl_data(file_path, max_samples=num_samples)\n",
    "\n",
    "        predictions, references = [], []\n",
    "        all_precisions, all_recalls, all_f1s = [], [], []\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for item in tqdm(validation_data_raw, desc=\"Evaluating ROUGE & Metrics\"):\n",
    "                document = item['original_text']\n",
    "                summary = item['extractive_summary']\n",
    "\n",
    "                processed_item = prepare_single_example(document, summary, self.tokenizer)\n",
    "                input_ids = torch.as_tensor([processed_item['input_ids']], dtype=torch.long).to(self.device)\n",
    "                attention_mask = torch.as_tensor([processed_item['attention_mask']], dtype=torch.long).to(self.device)\n",
    "                sentence_spans = torch.as_tensor([processed_item['sentence_spans']], dtype=torch.long)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                sequence_output = outputs.last_hidden_state\n",
    "\n",
    "                doc_logits = []\n",
    "                for span in sentence_spans[0]:\n",
    "                    start, end = span[0].item(), span[1].item()\n",
    "                    if start >= 0 and end >= 0:\n",
    "                        sentence_hidden_states = sequence_output[0, start:end, :]\n",
    "                        mean_pooled_sentence_rep = torch.mean(sentence_hidden_states, dim=0, keepdim=True)\n",
    "                        doc_logits.append(self.classifier(mean_pooled_sentence_rep).squeeze(-1))\n",
    "                \n",
    "                if doc_logits:\n",
    "                    dummy_logits = torch.cat(doc_logits) if len(doc_logits) > 1 else doc_logits[0]\n",
    "                    num_sentences_to_select = min(3, len(doc_logits))\n",
    "                    _, topk_indices = torch.topk(dummy_logits, k=num_sentences_to_select)\n",
    "                    predictions_for_doc = [1 if i in topk_indices.tolist() else 0 for i in range(len(doc_logits))]\n",
    "                else:\n",
    "                    predictions_for_doc = []\n",
    "\n",
    "                doc_sentences = spacy_sent_tokenize(document)\n",
    "                extracted_sentences = []\n",
    "                if not isinstance(predictions_for_doc, list):\n",
    "                    predictions_for_doc = [predictions_for_doc]\n",
    "\n",
    "                for k, pred in enumerate(predictions_for_doc):\n",
    "                    if pred == 1 and k < len(doc_sentences):\n",
    "                        extracted_sentences.append(doc_sentences[k])\n",
    "                \n",
    "                generated_summary_text = \" \".join(extracted_sentences)\n",
    "                predictions.append(generated_summary_text)\n",
    "                references.append(summary)\n",
    "\n",
    "                if processed_item[\"labels\"]:\n",
    "                    all_precisions.append(precision_score(processed_item[\"labels\"], predictions_for_doc, zero_division=0))\n",
    "                    all_recalls.append(recall_score(processed_item[\"labels\"], predictions_for_doc, zero_division=0))\n",
    "                    all_f1s.append(f1_score(processed_item[\"labels\"], predictions_for_doc, zero_division=0))\n",
    "\n",
    "        rouge_results = self.rouge_metric.compute(predictions=predictions, references=references)\n",
    "        \n",
    "        print(\"\\n--- Base Model Scores ---\")\n",
    "        for key, value in rouge_results.items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        print(f\"  Precision: {sum(all_precisions)/len(all_precisions):.4f}\")\n",
    "        print(f\"  Recall:    {sum(all_recalls)/len(all_recalls):.4f}\")\n",
    "        print(f\"  F1:        {sum(all_f1s)/len(all_f1s):.4f}\")\n",
    "        print(\"-----------------------------\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluator = BaseModelRougeEvaluator()\n",
    "    evaluator.run_evaluation(validation_file, num_samples=NUM_VAL_SAMPLES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c4fcf",
   "metadata": {},
   "source": [
    "--- Base Model Scores ---\n",
    "  - rouge1: 0.2057\n",
    "  - rouge2: 0.0677\n",
    "  - rougeL: 0.1304\n",
    "  - rougeLsum: 0.1303\n",
    "  - Precision: 0.1877\n",
    "  - Recall:    0.0292\n",
    "  - F1 score:  0.0471"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08be7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
