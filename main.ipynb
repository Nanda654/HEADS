{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nanda654/HEADS/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "from transformers import LongformerModel, LongformerTokenizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "import numpy as np\n",
        "\n",
        "# --- Load Longformer Model and Tokenizer ---\n",
        "print(\"Loading Longformer model and tokenizer...\")\n",
        "model_name = 'allenai/longformer-base-4096'\n",
        "tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
        "model = LongformerModel.from_pretrained(model_name)\n",
        "\n",
        "# --- Set Device (GPU if available, else CPU) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval() # Set model to evaluation mode\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "print(\"Longformer model loaded.\")\n",
        "\n",
        "# --- Helper Function for Sentence Embeddings ---\n",
        "def get_sentence_embeddings(text, batch_size=4):\n",
        "    \"\"\"\n",
        "    Splits text into sentences, tokenizes them, and gets Longformer embeddings.\n",
        "    Handles long documents by processing sentences in batches.\n",
        "    Returns:\n",
        "        sentences (list): List of original sentence strings.\n",
        "        sentence_embeddings (np.array): NumPy array of sentence embeddings.\n",
        "    \"\"\"\n",
        "    doc = nlp(text) # nlp is globally defined at the start of the cell\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "    if not sentences:\n",
        "        print(\"Warning: No valid sentences found in the input text.\")\n",
        "        return [], np.array([])\n",
        "\n",
        "    all_sentence_embeddings = []\n",
        "    print(f\"Total sentences to process: {len(sentences)}\")\n",
        "\n",
        "    for i in range(0, len(sentences), batch_size):\n",
        "        batch_sentences = sentences[i:i + batch_size] # CORRECTED: using batch_size\n",
        "        try:\n",
        "            inputs = tokenizer(\n",
        "                batch_sentences,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=tokenizer.model_max_length\n",
        "            ).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "            all_sentence_embeddings.extend(cls_embeddings)\n",
        "            # Removed detailed batch print to reduce output clutter unless needed for debugging speed\n",
        "            # print(f\"  Processed batch {i // batch_size + 1}/{(len(sentences) + batch_size - 1) // batch_size}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch of sentences (index {i}-{i+len(batch_sentences)-1}): {e}\")\n",
        "            all_sentence_embeddings.extend([np.zeros(model.config.hidden_size)] * len(batch_sentences))\n",
        "            continue\n",
        "\n",
        "    return sentences, np.array(all_sentence_embeddings)\n",
        "\n",
        "# --- Centroid-Based Summarization Function (Optimized to accept pre-calculated embeddings) ---\n",
        "def centroid_summarization_optimized(sentences, embeddings, num_sentences=3):\n",
        "    \"\"\"\n",
        "    Generates an extractive summary using a centroid-based approach.\n",
        "    Accepts pre-calculated sentences and embeddings.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Centroid-Based Summarization ---\")\n",
        "    if not sentences or embeddings.shape[0] == 0:\n",
        "        print(\"  No sentences or embeddings provided. Cannot summarize.\")\n",
        "        return [], []\n",
        "\n",
        "    if num_sentences <= 0:\n",
        "        print(\"  Number of sentences for summary must be positive.\")\n",
        "        return [], []\n",
        "\n",
        "    num_sentences_to_extract = min(num_sentences, len(sentences))\n",
        "\n",
        "    document_centroid = np.mean(embeddings, axis=0)\n",
        "    similarities = cosine_similarity(embeddings, document_centroid.reshape(1, -1)).flatten()\n",
        "\n",
        "    summary_sentences_mmr = []\n",
        "    selected_indices = set()\n",
        "    ranked_initial_indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "    for _ in range(num_sentences_to_extract):\n",
        "        best_sentence_idx = -1\n",
        "        max_mmr_score = -1\n",
        "\n",
        "        for i in ranked_initial_indices:\n",
        "            if i not in selected_indices:\n",
        "                relevance = similarities[i]\n",
        "\n",
        "                if not selected_indices:\n",
        "                    mmr_score = relevance\n",
        "                else:\n",
        "                    diversity_scores = cosine_similarity(embeddings[i].reshape(1, -1),\n",
        "                                                         embeddings[list(selected_indices)])\n",
        "                    redundancy = np.max(diversity_scores)\n",
        "                    lambda_param = 0.7\n",
        "                    mmr_score = lambda_param * relevance - (1 - lambda_param) * redundancy\n",
        "\n",
        "                if mmr_score > max_mmr_score:\n",
        "                    max_mmr_score = mmr_score\n",
        "                    best_sentence_idx = i\n",
        "\n",
        "        if best_sentence_idx != -1:\n",
        "            summary_sentences_mmr.append((sentences[best_sentence_idx], best_sentence_idx))\n",
        "            selected_indices.add(best_sentence_idx)\n",
        "            ranked_initial_indices = ranked_initial_indices[ranked_initial_indices != best_sentence_idx]\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    summary_sentences_mmr.sort(key=lambda x: x[1])\n",
        "    final_summary_sents = [s[0] for s in summary_sentences_mmr]\n",
        "    final_summary_indices = [s[1] for s in summary_sentences_mmr]\n",
        "\n",
        "    print(\"--- Centroid-Based Summarization Complete ---\")\n",
        "    return final_summary_sents, final_summary_indices\n",
        "\n",
        "# --- K-Means Based Summarization Function (Optimized to accept pre-calculated embeddings) ---\n",
        "def kmeans_summarization_optimized(sentences, embeddings, num_clusters=5, num_sentences_per_cluster=1):\n",
        "    \"\"\"\n",
        "    Generates an extractive summary using K-Means clustering.\n",
        "    Accepts pre-calculated sentences and embeddings.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting K-Means Based Summarization ---\")\n",
        "    if not sentences or embeddings.shape[0] == 0:\n",
        "        print(\"  No sentences or embeddings provided. Cannot summarize.\")\n",
        "        return [], []\n",
        "\n",
        "    if num_clusters <= 0 or num_sentences_per_cluster <= 0:\n",
        "        print(\"  Number of clusters and sentences per cluster must be positive.\")\n",
        "        return [], []\n",
        "\n",
        "    effective_num_clusters = min(num_clusters, len(sentences))\n",
        "\n",
        "    if effective_num_clusters == 0:\n",
        "        print(\"  Not enough sentences to form clusters.\")\n",
        "        return [], []\n",
        "\n",
        "    kmeans = KMeans(n_clusters=effective_num_clusters, random_state=42, n_init='auto')\n",
        "    kmeans.fit(embeddings)\n",
        "    clusters = kmeans.labels_\n",
        "    centroids = kmeans.cluster_centers_\n",
        "\n",
        "    summary_sentences_with_idx = []\n",
        "    selected_indices = set()\n",
        "\n",
        "    for i in range(effective_num_clusters):\n",
        "        cluster_sentence_indices = np.where(clusters == i)[0]\n",
        "\n",
        "        if len(cluster_sentence_indices) == 0:\n",
        "            continue\n",
        "\n",
        "        distances = cdist(embeddings[cluster_sentence_indices], centroids[i].reshape(1, -1), 'cosine').flatten()\n",
        "        sorted_cluster_indices = cluster_sentence_indices[np.argsort(distances)]\n",
        "\n",
        "        count_selected_from_cluster = 0\n",
        "        for original_idx in sorted_cluster_indices:\n",
        "            if original_idx not in selected_indices:\n",
        "                summary_sentences_with_idx.append((sentences[original_idx], original_idx))\n",
        "                selected_indices.add(original_idx)\n",
        "                count_selected_from_cluster += 1\n",
        "                if count_selected_from_cluster >= num_sentences_per_cluster:\n",
        "                    break\n",
        "\n",
        "    summary_sentences_with_idx.sort(key=lambda x: x[1])\n",
        "    final_summary_sents = [s[0] for s in summary_sentences_with_idx]\n",
        "    final_summary_indices = [s[1] for s in summary_sentences_with_idx]\n",
        "\n",
        "    print(\"--- K-Means Based Summarization Complete ---\")\n",
        "    return final_summary_sents, final_summary_indices\n",
        "\n",
        "# --- Combined Extractive Summarization Function (Optimized to accept pre-calculated embeddings) ---\n",
        "def combined_extractive_summary_optimized(sentences, embeddings, total_summary_sentences=7,\n",
        "                                centroid_sentences_to_propose=5,\n",
        "                                kmeans_clusters_to_propose=4,\n",
        "                                kmeans_sentences_per_cluster_to_propose=1,\n",
        "                                lambda_param_mmr=0.7):\n",
        "    \"\"\"\n",
        "    Generates a single extractive summary by combining candidates from\n",
        "    both centroid-based and K-Means approaches, then using MMR for final selection.\n",
        "    Accepts pre-calculated sentences and embeddings.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Combined Extractive Summarization ---\")\n",
        "    if not sentences or embeddings.shape[0] == 0:\n",
        "        print(\"  No sentences or embeddings provided. Cannot summarize combined.\")\n",
        "        return []\n",
        "\n",
        "    centroid_candidates_sents, centroid_candidates_indices = centroid_summarization_optimized(\n",
        "        sentences, embeddings, num_sentences=centroid_sentences_to_propose\n",
        "    )\n",
        "    print(f\"  Centroid proposed {len(centroid_candidates_sents)} candidates.\")\n",
        "\n",
        "    kmeans_candidates_sents, kmeans_candidates_indices = kmeans_summarization_optimized(\n",
        "        sentences, embeddings, num_clusters=kmeans_clusters_to_propose, num_sentences_per_cluster=kmeans_sentences_per_cluster_to_propose\n",
        "    )\n",
        "    print(f\"  K-Means proposed {len(kmeans_candidates_sents)} candidates.\")\n",
        "\n",
        "    # Combine candidates and their original indices, removing duplicates\n",
        "    combined_candidates_map = {}\n",
        "    for idx, sent in zip(centroid_candidates_indices, centroid_candidates_sents):\n",
        "        combined_candidates_map[idx] = sent\n",
        "    for idx, sent in zip(kmeans_candidates_indices, kmeans_candidates_sents):\n",
        "        combined_candidates_map[idx] = sent\n",
        "\n",
        "    all_candidate_indices_sorted = sorted(combined_candidates_map.keys())\n",
        "    all_candidate_sentences = [combined_candidates_map[idx] for idx in all_candidate_indices_sorted]\n",
        "    all_candidate_embeddings = np.array([embeddings[idx] for idx in all_candidate_indices_sorted])\n",
        "\n",
        "    if not all_candidate_sentences or all_candidate_embeddings.shape[0] == 0:\n",
        "        print(\"  No unique candidates found after combining. Cannot generate combined summary.\")\n",
        "        return []\n",
        "\n",
        "    num_sentences_to_extract = min(total_summary_sentences, len(all_candidate_sentences))\n",
        "    print(f\"  Total unique candidates: {len(all_candidate_sentences)}. Extracting {num_sentences_to_extract} for combined summary.\")\n",
        "\n",
        "    document_centroid = np.mean(embeddings, axis=0)\n",
        "    candidate_similarities = cosine_similarity(all_candidate_embeddings, document_centroid.reshape(1, -1)).flatten()\n",
        "\n",
        "    final_summary_sentences = []\n",
        "    selected_candidate_indices = set()\n",
        "\n",
        "    ranked_initial_candidate_indices = np.argsort(candidate_similarities)[::-1]\n",
        "\n",
        "    for _ in range(num_sentences_to_extract):\n",
        "        best_idx_in_candidates = -1\n",
        "        max_mmr_score = -1\n",
        "\n",
        "        for i_candidate in ranked_initial_candidate_indices:\n",
        "            if i_candidate not in selected_candidate_indices:\n",
        "                relevance = candidate_similarities[i_candidate]\n",
        "\n",
        "                if not selected_candidate_indices:\n",
        "                    mmr_score = relevance\n",
        "                else:\n",
        "                    diversity_scores = cosine_similarity(all_candidate_embeddings[i_candidate].reshape(1, -1),\n",
        "                                                         all_candidate_embeddings[list(selected_candidate_indices)])\n",
        "                    redundancy = np.max(diversity_scores)\n",
        "\n",
        "                    mmr_score = lambda_param_mmr * relevance - (1 - lambda_param_mmr) * redundancy\n",
        "\n",
        "                if mmr_score > max_mmr_score:\n",
        "                    max_mmr_score = mmr_score\n",
        "                    best_idx_in_candidates = i_candidate\n",
        "\n",
        "        if best_idx_in_candidates != -1:\n",
        "            final_summary_sentences.append((all_candidate_sentences[best_idx_in_candidates],\n",
        "                                             all_candidate_indices_sorted[best_idx_in_candidates]))\n",
        "            selected_candidate_indices.add(best_idx_in_candidates)\n",
        "            ranked_initial_candidate_indices = ranked_initial_candidate_indices[ranked_initial_candidate_indices != best_idx_in_candidates]\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    final_summary_sentences.sort(key=lambda x: x[1])\n",
        "    final_summary = [s[0] for s in final_summary_sentences]\n",
        "\n",
        "    print(\"--- Combined Extractive Summarization Complete ---\")\n",
        "    return final_summary\n",
        "\n",
        "# --- Example Usage and Testing ---\n",
        "long_document = \"\"\"\n",
        "Artificial intelligence (AI) has rapidly transformed various sectors, revolutionizing industries from healthcare to finance. In healthcare, AI assists in diagnosing diseases earlier and more accurately, personalizing treatment plans, and accelerating drug discovery. Machine learning algorithms, a subset of AI, analyze vast amounts of patient data to identify patterns that human doctors might miss, leading to more effective interventions. For instance, AI-powered tools can detect subtle signs of retinopathy from eye scans, potentially preventing blindness. The integration of AI into electronic health records is also streamlining administrative tasks, freeing up medical professionals to focus more on patient care. This technological leap promises to enhance diagnostic capabilities and optimize treatment protocols significantly.\n",
        "\n",
        "The financial industry also heavily leverages AI for fraud detection, algorithmic trading, and personalized financial advice. AI systems can monitor transactions in real-time, identifying unusual patterns indicative of fraudulent activity with high precision. Furthermore, robo-advisors powered by AI provide automated, data-driven investment advice tailored to individual risk tolerance and financial goals, making financial planning more accessible to a wider demographic. The use of AI in predicting market trends and managing portfolios is becoming increasingly sophisticated, offering new avenues for investors.\n",
        "\n",
        "Beyond these, AI is deeply embedded in everyday life through virtual assistants like Siri and Alexa, recommendation engines on streaming platforms, and autonomous vehicles. AI's role in natural language processing (NLP) has led to advancements in language translation and sentiment analysis, impacting global communication and customer service. The ethical implications of AI, however, are a growing concern among researchers and policymakers. Issues such as algorithmic bias, job displacement due to automation, and privacy breaches require careful consideration and robust regulation. Ensuring transparency, fairness, and accountability in AI development is paramount to harnessing its benefits responsibly.\n",
        "\n",
        "Research in AI continues to advance at an astonishing pace, focusing on areas like explainable AI (XAI) to make AI decisions more understandable, and robust AI to improve performance in real-world, unpredictable environments. Novel architectures like generative adversarial networks (GANs) and reinforcement learning are pushing the boundaries of what AI can achieve, from creating realistic imagery to mastering complex games. The future of AI promises even more integration into society, with potential breakthroughs in areas like general artificial intelligence (AGI) and enhanced human-computer interaction, leading to smarter cities and more efficient resource management. However, achieving these advancements responsibly will necessitate ongoing collaboration between technologists, policymakers, and ethicists to address the complex challenges that arise. The rapid pace of development means that continuous public discourse and legislative adaptation are critical to navigate the challenges and maximize the societal benefits of AI, ensuring it serves humanity's best interests.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Original Document Length (sentences):\", sum(1 for _ in nlp(long_document).sents))\n",
        "\n",
        "# --- OPTIMIZATION: Calculate document embeddings only ONCE ---\n",
        "print(\"\\nCalculating document embeddings (this might take a while for long texts)...\")\n",
        "sentences_list, embeddings_array = get_sentence_embeddings(long_document, batch_size=8)\n",
        "print(\"Embeddings calculation complete.\")\n",
        "\n",
        "\n",
        "# --- Individual Centroid-Based Summarization ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Individual Centroid-Based Summary:\")\n",
        "centroid_summary, _ = centroid_summarization_optimized(sentences_list, embeddings_array, num_sentences=5)\n",
        "for i, sent in enumerate(centroid_summary):\n",
        "    print(f\"{i+1}. {sent}\")\n",
        "\n",
        "\n",
        "# --- Individual K-Means Based Summarization ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Individual K-Means Based Summary:\")\n",
        "kmeans_summary, _ = kmeans_summarization_optimized(sentences_list, embeddings_array, num_clusters=4, num_sentences_per_cluster=1)\n",
        "for i, sent in enumerate(kmeans_summary):\n",
        "    print(f\"{i+1}. {sent}\")\n",
        "\n",
        "\n",
        "# --- Combined Extractive Summarization ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Combined Extractive Summary:\")\n",
        "combined_summary = combined_extractive_summary_optimized(\n",
        "    sentences_list,\n",
        "    embeddings_array,\n",
        "    total_summary_sentences=6,\n",
        "    centroid_sentences_to_propose=7,\n",
        "    kmeans_clusters_to_propose=5,\n",
        "    kmeans_sentences_per_cluster_to_propose=1\n",
        ")\n",
        "for i, sent in enumerate(combined_summary):\n",
        "    print(f\"{i+1}. {sent}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nAll summarization processes complete.\")"
      ],
      "metadata": {
        "id": "CIGmB4v42ICD",
        "outputId": "b10955b9-bd6a-4a6f-eda4-497c3d19ce5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Longformer model and tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Input ids are automatically padded to be a multiple of `config.attention_window`: 512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Longformer model loaded.\n",
            "Original Document Length (sentences): 20\n",
            "\n",
            "Calculating document embeddings (this might take a while for long texts)...\n",
            "Total sentences to process: 20\n",
            "Embeddings calculation complete.\n",
            "\n",
            "================================================================================\n",
            "Individual Centroid-Based Summary:\n",
            "\n",
            "--- Starting Centroid-Based Summarization ---\n",
            "--- Centroid-Based Summarization Complete ---\n",
            "1. Artificial intelligence (AI) has rapidly transformed various sectors, revolutionizing industries from healthcare to finance.\n",
            "2. The integration of AI into electronic health records is also streamlining administrative tasks, freeing up medical professionals to focus more on patient care.\n",
            "3. Furthermore, robo-advisors powered by AI provide automated, data-driven investment advice tailored to individual risk tolerance and financial goals, making financial planning more accessible to a wider demographic.\n",
            "4. The future of AI promises even more integration into society, with potential breakthroughs in areas like general artificial intelligence (AGI) and enhanced human-computer interaction, leading to smarter cities and more efficient resource management.\n",
            "5. The rapid pace of development means that continuous public discourse and legislative adaptation are critical to navigate the challenges and maximize the societal benefits of AI, ensuring it serves humanity's best interests.\n",
            "\n",
            "================================================================================\n",
            "Individual K-Means Based Summary:\n",
            "\n",
            "--- Starting K-Means Based Summarization ---\n",
            "--- K-Means Based Summarization Complete ---\n",
            "1. In healthcare, AI assists in diagnosing diseases earlier and more accurately, personalizing treatment plans, and accelerating drug discovery.\n",
            "2. The financial industry also heavily leverages AI for fraud detection, algorithmic trading, and personalized financial advice.\n",
            "3. The future of AI promises even more integration into society, with potential breakthroughs in areas like general artificial intelligence (AGI) and enhanced human-computer interaction, leading to smarter cities and more efficient resource management.\n",
            "4. The rapid pace of development means that continuous public discourse and legislative adaptation are critical to navigate the challenges and maximize the societal benefits of AI, ensuring it serves humanity's best interests.\n",
            "\n",
            "================================================================================\n",
            "Combined Extractive Summary:\n",
            "\n",
            "--- Starting Combined Extractive Summarization ---\n",
            "\n",
            "--- Starting Centroid-Based Summarization ---\n",
            "--- Centroid-Based Summarization Complete ---\n",
            "  Centroid proposed 7 candidates.\n",
            "\n",
            "--- Starting K-Means Based Summarization ---\n",
            "--- K-Means Based Summarization Complete ---\n",
            "  K-Means proposed 5 candidates.\n",
            "  Total unique candidates: 11. Extracting 6 for combined summary.\n",
            "--- Combined Extractive Summarization Complete ---\n",
            "1. Artificial intelligence (AI) has rapidly transformed various sectors, revolutionizing industries from healthcare to finance.\n",
            "2. The integration of AI into electronic health records is also streamlining administrative tasks, freeing up medical professionals to focus more on patient care.\n",
            "3. Furthermore, robo-advisors powered by AI provide automated, data-driven investment advice tailored to individual risk tolerance and financial goals, making financial planning more accessible to a wider demographic.\n",
            "4. Issues such as algorithmic bias, job displacement due to automation, and privacy breaches require careful consideration and robust regulation.\n",
            "5. The future of AI promises even more integration into society, with potential breakthroughs in areas like general artificial intelligence (AGI) and enhanced human-computer interaction, leading to smarter cities and more efficient resource management.\n",
            "6. The rapid pace of development means that continuous public discourse and legislative adaptation are critical to navigate the challenges and maximize the societal benefits of AI, ensuring it serves humanity's best interests.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "All summarization processes complete.\n"
          ]
        }
      ]
    }
  ]
}