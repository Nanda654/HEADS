{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nanda654/HEADS/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Longforner"
      ],
      "metadata": {
        "id": "xEnHz7uX4Wuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "from transformers import LongformerModel, LongformerTokenizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "import numpy as np\n",
        "\n",
        "# --- Load Longformer Model and Tokenizer ---\n",
        "print(\"Loading Longformer model and tokenizer...\")\n",
        "model_name = 'allenai/longformer-base-4096'\n",
        "tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
        "model = LongformerModel.from_pretrained(model_name)\n",
        "\n",
        "# --- Set Device (GPU if available, else CPU) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval() # Set model to evaluation mode\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "print(\"Longformer model loaded.\")\n",
        "\n",
        "# --- Helper Function for Sentence Embeddings ---\n",
        "def get_sentence_embeddings(text, batch_size=4):\n",
        "    \"\"\"\n",
        "    Splits text into sentences, tokenizes them, and gets Longformer embeddings.\n",
        "    Handles long documents by processing sentences in batches.\n",
        "    Returns:\n",
        "        sentences (list): List of original sentence strings.\n",
        "        sentence_embeddings (np.array): NumPy array of sentence embeddings.\n",
        "    \"\"\"\n",
        "    doc = nlp(text) # nlp is globally defined at the start of the cell\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "    if not sentences:\n",
        "        print(\"Warning: No valid sentences found in the input text.\")\n",
        "        return [], np.array([])\n",
        "\n",
        "    all_sentence_embeddings = []\n",
        "    print(f\"Total sentences to process: {len(sentences)}\")\n",
        "\n",
        "    for i in range(0, len(sentences), batch_size):\n",
        "        batch_sentences = sentences[i:i + batch_size] # CORRECTED: using batch_size\n",
        "        try:\n",
        "            inputs = tokenizer(\n",
        "                batch_sentences,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=tokenizer.model_max_length\n",
        "            ).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "            all_sentence_embeddings.extend(cls_embeddings)\n",
        "            # Removed detailed batch print to reduce output clutter unless needed for debugging speed\n",
        "            # print(f\"  Processed batch {i // batch_size + 1}/{(len(sentences) + batch_size - 1) // batch_size}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch of sentences (index {i}-{i+len(batch_sentences)-1}): {e}\")\n",
        "            all_sentence_embeddings.extend([np.zeros(model.config.hidden_size)] * len(batch_sentences))\n",
        "            continue\n",
        "\n",
        "    return sentences, np.array(all_sentence_embeddings)\n",
        "\n",
        "# --- Centroid-Based Summarization Function (Optimized to accept pre-calculated embeddings) ---\n",
        "def centroid_summarization_optimized(sentences, embeddings, num_sentences=3):\n",
        "    \"\"\"\n",
        "    Generates an extractive summary using a centroid-based approach.\n",
        "    Accepts pre-calculated sentences and embeddings.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Centroid-Based Summarization ---\")\n",
        "    if not sentences or embeddings.shape[0] == 0:\n",
        "        print(\"  No sentences or embeddings provided. Cannot summarize.\")\n",
        "        return [], []\n",
        "\n",
        "    if num_sentences <= 0:\n",
        "        print(\"  Number of sentences for summary must be positive.\")\n",
        "        return [], []\n",
        "\n",
        "    num_sentences_to_extract = min(num_sentences, len(sentences))\n",
        "\n",
        "    document_centroid = np.mean(embeddings, axis=0)\n",
        "    similarities = cosine_similarity(embeddings, document_centroid.reshape(1, -1)).flatten()\n",
        "\n",
        "    summary_sentences_mmr = []\n",
        "    selected_indices = set()\n",
        "    ranked_initial_indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "    for _ in range(num_sentences_to_extract):\n",
        "        best_sentence_idx = -1\n",
        "        max_mmr_score = -1\n",
        "\n",
        "        for i in ranked_initial_indices:\n",
        "            if i not in selected_indices:\n",
        "                relevance = similarities[i]\n",
        "\n",
        "                if not selected_indices:\n",
        "                    mmr_score = relevance\n",
        "                else:\n",
        "                    diversity_scores = cosine_similarity(embeddings[i].reshape(1, -1),\n",
        "                                                         embeddings[list(selected_indices)])\n",
        "                    redundancy = np.max(diversity_scores)\n",
        "                    lambda_param = 0.7\n",
        "                    mmr_score = lambda_param * relevance - (1 - lambda_param) * redundancy\n",
        "\n",
        "                if mmr_score > max_mmr_score:\n",
        "                    max_mmr_score = mmr_score\n",
        "                    best_sentence_idx = i\n",
        "\n",
        "        if best_sentence_idx != -1:\n",
        "            summary_sentences_mmr.append((sentences[best_sentence_idx], best_sentence_idx))\n",
        "            selected_indices.add(best_sentence_idx)\n",
        "            ranked_initial_indices = ranked_initial_indices[ranked_initial_indices != best_sentence_idx]\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    summary_sentences_mmr.sort(key=lambda x: x[1])\n",
        "    final_summary_sents = [s[0] for s in summary_sentences_mmr]\n",
        "    final_summary_indices = [s[1] for s in summary_sentences_mmr]\n",
        "\n",
        "    print(\"--- Centroid-Based Summarization Complete ---\")\n",
        "    return final_summary_sents, final_summary_indices\n",
        "\n",
        "# --- K-Means Based Summarization Function (Optimized to accept pre-calculated embeddings) ---\n",
        "def kmeans_summarization_optimized(sentences, embeddings, num_clusters=5, num_sentences_per_cluster=1):\n",
        "    \"\"\"\n",
        "    Generates an extractive summary using K-Means clustering.\n",
        "    Accepts pre-calculated sentences and embeddings.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting K-Means Based Summarization ---\")\n",
        "    if not sentences or embeddings.shape[0] == 0:\n",
        "        print(\"  No sentences or embeddings provided. Cannot summarize.\")\n",
        "        return [], []\n",
        "\n",
        "    if num_clusters <= 0 or num_sentences_per_cluster <= 0:\n",
        "        print(\"  Number of clusters and sentences per cluster must be positive.\")\n",
        "        return [], []\n",
        "\n",
        "    effective_num_clusters = min(num_clusters, len(sentences))\n",
        "\n",
        "    if effective_num_clusters == 0:\n",
        "        print(\"  Not enough sentences to form clusters.\")\n",
        "        return [], []\n",
        "\n",
        "    kmeans = KMeans(n_clusters=effective_num_clusters, random_state=42, n_init='auto')\n",
        "    kmeans.fit(embeddings)\n",
        "    clusters = kmeans.labels_\n",
        "    centroids = kmeans.cluster_centers_\n",
        "\n",
        "    summary_sentences_with_idx = []\n",
        "    selected_indices = set()\n",
        "\n",
        "    for i in range(effective_num_clusters):\n",
        "        cluster_sentence_indices = np.where(clusters == i)[0]\n",
        "\n",
        "        if len(cluster_sentence_indices) == 0:\n",
        "            continue\n",
        "\n",
        "        distances = cdist(embeddings[cluster_sentence_indices], centroids[i].reshape(1, -1), 'cosine').flatten()\n",
        "        sorted_cluster_indices = cluster_sentence_indices[np.argsort(distances)]\n",
        "\n",
        "        count_selected_from_cluster = 0\n",
        "        for original_idx in sorted_cluster_indices:\n",
        "            if original_idx not in selected_indices:\n",
        "                summary_sentences_with_idx.append((sentences[original_idx], original_idx))\n",
        "                selected_indices.add(original_idx)\n",
        "                count_selected_from_cluster += 1\n",
        "                if count_selected_from_cluster >= num_sentences_per_cluster:\n",
        "                    break\n",
        "\n",
        "    summary_sentences_with_idx.sort(key=lambda x: x[1])\n",
        "    final_summary_sents = [s[0] for s in summary_sentences_with_idx]\n",
        "    final_summary_indices = [s[1] for s in summary_sentences_with_idx]\n",
        "\n",
        "    print(\"--- K-Means Based Summarization Complete ---\")\n",
        "    return final_summary_sents, final_summary_indices\n",
        "\n",
        "# --- Combined Extractive Summarization Function (Optimized to accept pre-calculated embeddings) ---\n",
        "def combined_extractive_summary_optimized(sentences, embeddings, total_summary_sentences=7,\n",
        "                                centroid_sentences_to_propose=5,\n",
        "                                kmeans_clusters_to_propose=4,\n",
        "                                kmeans_sentences_per_cluster_to_propose=1,\n",
        "                                lambda_param_mmr=0.7):\n",
        "    \"\"\"\n",
        "    Generates a single extractive summary by combining candidates from\n",
        "    both centroid-based and K-Means approaches, then using MMR for final selection.\n",
        "    Accepts pre-calculated sentences and embeddings.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Combined Extractive Summarization ---\")\n",
        "    if not sentences or embeddings.shape[0] == 0:\n",
        "        print(\"  No sentences or embeddings provided. Cannot summarize combined.\")\n",
        "        return []\n",
        "\n",
        "    centroid_candidates_sents, centroid_candidates_indices = centroid_summarization_optimized(\n",
        "        sentences, embeddings, num_sentences=centroid_sentences_to_propose\n",
        "    )\n",
        "    print(f\"  Centroid proposed {len(centroid_candidates_sents)} candidates.\")\n",
        "\n",
        "    kmeans_candidates_sents, kmeans_candidates_indices = kmeans_summarization_optimized(\n",
        "        sentences, embeddings, num_clusters=kmeans_clusters_to_propose, num_sentences_per_cluster=kmeans_sentences_per_cluster_to_propose\n",
        "    )\n",
        "    print(f\"  K-Means proposed {len(kmeans_candidates_sents)} candidates.\")\n",
        "\n",
        "    # Combine candidates and their original indices, removing duplicates\n",
        "    combined_candidates_map = {}\n",
        "    for idx, sent in zip(centroid_candidates_indices, centroid_candidates_sents):\n",
        "        combined_candidates_map[idx] = sent\n",
        "    for idx, sent in zip(kmeans_candidates_indices, kmeans_candidates_sents):\n",
        "        combined_candidates_map[idx] = sent\n",
        "\n",
        "    all_candidate_indices_sorted = sorted(combined_candidates_map.keys())\n",
        "    all_candidate_sentences = [combined_candidates_map[idx] for idx in all_candidate_indices_sorted]\n",
        "    all_candidate_embeddings = np.array([embeddings[idx] for idx in all_candidate_indices_sorted])\n",
        "\n",
        "    if not all_candidate_sentences or all_candidate_embeddings.shape[0] == 0:\n",
        "        print(\"  No unique candidates found after combining. Cannot generate combined summary.\")\n",
        "        return []\n",
        "\n",
        "    num_sentences_to_extract = min(total_summary_sentences, len(all_candidate_sentences))\n",
        "    print(f\"  Total unique candidates: {len(all_candidate_sentences)}. Extracting {num_sentences_to_extract} for combined summary.\")\n",
        "\n",
        "    document_centroid = np.mean(embeddings, axis=0)\n",
        "    candidate_similarities = cosine_similarity(all_candidate_embeddings, document_centroid.reshape(1, -1)).flatten()\n",
        "\n",
        "    final_summary_sentences = []\n",
        "    selected_candidate_indices = set()\n",
        "\n",
        "    ranked_initial_candidate_indices = np.argsort(candidate_similarities)[::-1]\n",
        "\n",
        "    for _ in range(num_sentences_to_extract):\n",
        "        best_idx_in_candidates = -1\n",
        "        max_mmr_score = -1\n",
        "\n",
        "        for i_candidate in ranked_initial_candidate_indices:\n",
        "            if i_candidate not in selected_candidate_indices:\n",
        "                relevance = candidate_similarities[i_candidate]\n",
        "\n",
        "                if not selected_candidate_indices:\n",
        "                    mmr_score = relevance\n",
        "                else:\n",
        "                    diversity_scores = cosine_similarity(all_candidate_embeddings[i_candidate].reshape(1, -1),\n",
        "                                                         all_candidate_embeddings[list(selected_candidate_indices)])\n",
        "                    redundancy = np.max(diversity_scores)\n",
        "\n",
        "                    mmr_score = lambda_param_mmr * relevance - (1 - lambda_param_mmr) * redundancy\n",
        "\n",
        "                if mmr_score > max_mmr_score:\n",
        "                    max_mmr_score = mmr_score\n",
        "                    best_idx_in_candidates = i_candidate\n",
        "\n",
        "        if best_idx_in_candidates != -1:\n",
        "            final_summary_sentences.append((all_candidate_sentences[best_idx_in_candidates],\n",
        "                                             all_candidate_indices_sorted[best_idx_in_candidates]))\n",
        "            selected_candidate_indices.add(best_idx_in_candidates)\n",
        "            ranked_initial_candidate_indices = ranked_initial_candidate_indices[ranked_initial_candidate_indices != best_idx_in_candidates]\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    final_summary_sentences.sort(key=lambda x: x[1])\n",
        "    final_summary = [s[0] for s in final_summary_sentences]\n",
        "\n",
        "    print(\"--- Combined Extractive Summarization Complete ---\")\n",
        "    return final_summary\n",
        "\n",
        "# --- Example Usage and Testing ---\n",
        "long_document = \"\"\"\n",
        "Artificial intelligence (AI) has rapidly transformed various sectors, revolutionizing industries from healthcare to finance. In healthcare, AI assists in diagnosing diseases earlier and more accurately, personalizing treatment plans, and accelerating drug discovery. Machine learning algorithms, a subset of AI, analyze vast amounts of patient data to identify patterns that human doctors might miss, leading to more effective interventions. For instance, AI-powered tools can detect subtle signs of retinopathy from eye scans, potentially preventing blindness. The integration of AI into electronic health records is also streamlining administrative tasks, freeing up medical professionals to focus more on patient care. This technological leap promises to enhance diagnostic capabilities and optimize treatment protocols significantly.\n",
        "\n",
        "The financial industry also heavily leverages AI for fraud detection, algorithmic trading, and personalized financial advice. AI systems can monitor transactions in real-time, identifying unusual patterns indicative of fraudulent activity with high precision. Furthermore, robo-advisors powered by AI provide automated, data-driven investment advice tailored to individual risk tolerance and financial goals, making financial planning more accessible to a wider demographic. The use of AI in predicting market trends and managing portfolios is becoming increasingly sophisticated, offering new avenues for investors.\n",
        "\n",
        "Beyond these, AI is deeply embedded in everyday life through virtual assistants like Siri and Alexa, recommendation engines on streaming platforms, and autonomous vehicles. AI's role in natural language processing (NLP) has led to advancements in language translation and sentiment analysis, impacting global communication and customer service. The ethical implications of AI, however, are a growing concern among researchers and policymakers. Issues such as algorithmic bias, job displacement due to automation, and privacy breaches require careful consideration and robust regulation. Ensuring transparency, fairness, and accountability in AI development is paramount to harnessing its benefits responsibly.\n",
        "\n",
        "Research in AI continues to advance at an astonishing pace, focusing on areas like explainable AI (XAI) to make AI decisions more understandable, and robust AI to improve performance in real-world, unpredictable environments. Novel architectures like generative adversarial networks (GANs) and reinforcement learning are pushing the boundaries of what AI can achieve, from creating realistic imagery to mastering complex games. The future of AI promises even more integration into society, with potential breakthroughs in areas like general artificial intelligence (AGI) and enhanced human-computer interaction, leading to smarter cities and more efficient resource management. However, achieving these advancements responsibly will necessitate ongoing collaboration between technologists, policymakers, and ethicists to address the complex challenges that arise. The rapid pace of development means that continuous public discourse and legislative adaptation are critical to navigate the challenges and maximize the societal benefits of AI, ensuring it serves humanity's best interests.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Original Document Length (sentences):\", sum(1 for _ in nlp(long_document).sents))\n",
        "\n",
        "# --- OPTIMIZATION: Calculate document embeddings only ONCE ---\n",
        "print(\"\\nCalculating document embeddings (this might take a while for long texts)...\")\n",
        "sentences_list, embeddings_array = get_sentence_embeddings(long_document, batch_size=8)\n",
        "print(\"Embeddings calculation complete.\")\n",
        "\n",
        "\n",
        "# --- Individual Centroid-Based Summarization ---\n",
        "'''print(\"\\n\" + \"=\"*80)\n",
        "print(\"Individual Centroid-Based Summary:\")\n",
        "centroid_summary, _ = centroid_summarization_optimized(sentences_list, embeddings_array, num_sentences=5)\n",
        "for i, sent in enumerate(centroid_summary):\n",
        "    print(f\"{i+1}. {sent}\")\n",
        "\n",
        "\n",
        "# --- Individual K-Means Based Summarization ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Individual K-Means Based Summary:\")\n",
        "kmeans_summary, _ = kmeans_summarization_optimized(sentences_list, embeddings_array, num_clusters=4, num_sentences_per_cluster=1)\n",
        "for i, sent in enumerate(kmeans_summary):\n",
        "    print(f\"{i+1}. {sent}\")'''\n",
        "\n",
        "\n",
        "# --- Combined Extractive Summarization ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Combined Extractive Summary:\")\n",
        "combined_summary = combined_extractive_summary_optimized(\n",
        "    sentences_list,\n",
        "    embeddings_array,\n",
        "    total_summary_sentences=6,\n",
        "    centroid_sentences_to_propose=7,\n",
        "    kmeans_clusters_to_propose=5,\n",
        "    kmeans_sentences_per_cluster_to_propose=1\n",
        ")\n",
        "for i, sent in enumerate(combined_summary):\n",
        "    print(f\"{i+1}. {sent}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nAll summarization processes complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9WZGQZ-5Lx5",
        "outputId": "ee3d1366-03ef-4b7c-c60f-ff46bdee229c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Longformer model and tokenizer...\n",
            "Using device: cpu\n",
            "Longformer model loaded.\n",
            "Original Document Length (sentences): 20\n",
            "\n",
            "Calculating document embeddings (this might take a while for long texts)...\n",
            "Total sentences to process: 20\n",
            "Embeddings calculation complete.\n",
            "\n",
            "================================================================================\n",
            "Combined Extractive Summary:\n",
            "\n",
            "--- Starting Combined Extractive Summarization ---\n",
            "\n",
            "--- Starting Centroid-Based Summarization ---\n",
            "--- Centroid-Based Summarization Complete ---\n",
            "  Centroid proposed 7 candidates.\n",
            "\n",
            "--- Starting K-Means Based Summarization ---\n",
            "--- K-Means Based Summarization Complete ---\n",
            "  K-Means proposed 5 candidates.\n",
            "  Total unique candidates: 11. Extracting 6 for combined summary.\n",
            "--- Combined Extractive Summarization Complete ---\n",
            "1. Artificial intelligence (AI) has rapidly transformed various sectors, revolutionizing industries from healthcare to finance.\n",
            "2. The integration of AI into electronic health records is also streamlining administrative tasks, freeing up medical professionals to focus more on patient care.\n",
            "3. Furthermore, robo-advisors powered by AI provide automated, data-driven investment advice tailored to individual risk tolerance and financial goals, making financial planning more accessible to a wider demographic.\n",
            "4. Issues such as algorithmic bias, job displacement due to automation, and privacy breaches require careful consideration and robust regulation.\n",
            "5. The future of AI promises even more integration into society, with potential breakthroughs in areas like general artificial intelligence (AGI) and enhanced human-computer interaction, leading to smarter cities and more efficient resource management.\n",
            "6. The rapid pace of development means that continuous public discourse and legislative adaptation are critical to navigate the challenges and maximize the societal benefits of AI, ensuring it serves humanity's best interests.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "All summarization processes complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BART"
      ],
      "metadata": {
        "id": "_s3CRcmh5ieu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries (only runs if not already installed)\n",
        "#%pip install transformers torch\n",
        "\n",
        "import torch\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "# --- Load BART Model and Tokenizer ---\n",
        "print(\"Loading BART model and tokenizer for abstractive summarization...\")\n",
        "bart_model_name = 'facebook/bart-large-cnn' # This is a good choice for summarization\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(bart_model_name)\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(bart_model_name)\n",
        "\n",
        "# --- Set Device (GPU if available, else CPU) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "bart_model.eval() # Set to evaluation mode\n",
        "bart_model.to(device)\n",
        "print(f\"BART using device: {device}\")\n",
        "print(\"BART model loaded.\")\n",
        "\n",
        "# --- Abstractive Summarization Function (using BART) ---\n",
        "def bart_abstractive_summary(text_to_summarize, max_length=150, min_length=50, num_beams=4, early_stopping=True):\n",
        "    \"\"\"\n",
        "    Generates an abstractive summary using the pre-loaded BART model.\n",
        "    Assumes bart_tokenizer and bart_model are loaded globally.\n",
        "\n",
        "    Args:\n",
        "        text_to_summarize (str or list of str): The input text (or list of sentences) to summarize.\n",
        "                                                  If a list, it will be joined into a single string.\n",
        "        max_length (int): Maximum length of the generated summary.\n",
        "        min_length (int): Minimum length of the generated summary.\n",
        "        num_beams (int): Number of beams for beam search. Higher values lead to better quality but slower generation.\n",
        "        early_stopping (bool): Whether to stop beam search when all beams have finished their generation.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated abstractive summary.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting BART Abstractive Summarization ---\")\n",
        "\n",
        "    if isinstance(text_to_summarize, list):\n",
        "        text_to_summarize = \" \".join(text_to_summarize)\n",
        "\n",
        "    if not text_to_summarize.strip():\n",
        "        print(\"  Input text for abstractive summary is empty. Cannot summarize.\")\n",
        "        return \"\"\n",
        "\n",
        "    inputs = bart_tokenizer( # bart_tokenizer is now accessible globally\n",
        "        [text_to_summarize],\n",
        "        max_length=1024, # BART's typical max input length\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    ).to(device) # device is also globally accessible\n",
        "\n",
        "    summary_ids = bart_model.generate( # bart_model is now accessible globally\n",
        "        inputs[\"input_ids\"],\n",
        "        num_beams=num_beams,\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        early_stopping=early_stopping\n",
        "    )\n",
        "\n",
        "    summary_text = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    print(\"--- BART Abstractive Summarization Complete ---\")\n",
        "    return summary_text\n",
        "\n",
        "# --- Example Usage for BART only ---\n",
        "input_text_for_bart = \" \".join(combined_summary)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Abstractive Summary (using BART directly on the full text):\")\n",
        "bart_only_summary = bart_abstractive_summary(\n",
        "    input_text_for_bart,\n",
        "    max_length=150, # Max length of the final abstractive summary\n",
        "    min_length=50,  # Min length of the final abstractive summary\n",
        "    num_beams=4     # Beam search parameter for quality\n",
        ")\n",
        "print(bart_only_summary)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nBART only summarization complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61J6V9ac5qhw",
        "outputId": "deb4e767-0f63-40f2-9c98-8bab5e72dad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BART model and tokenizer for abstractive summarization...\n",
            "BART using device: cpu\n",
            "BART model loaded.\n",
            "\n",
            "================================================================================\n",
            "Abstractive Summary (using BART directly on the full text):\n",
            "\n",
            "--- Starting BART Abstractive Summarization ---\n",
            "--- BART Abstractive Summarization Complete ---\n",
            "Artificial intelligence (AI) has rapidly transformed various sectors. Issues such as algorithmic bias, job displacement due to automation, and privacy breaches require careful consideration and robust regulation. Future of AI promises even more integration into society, with potential breakthroughs in areas like general artificial intelligence.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "BART only summarization complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Full code for Longformer and BART and pipeline"
      ],
      "metadata": {
        "id": "RDWlIx9jB3tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Setup, Model Loading, and Function Definitions\n",
        "\n",
        "# Install necessary libraries (only runs if not already installed)\n",
        "%pip install transformers torch scikit-learn numpy scipy spacy\n",
        "\n",
        "# Download spaCy model (only downloads if not already present)\n",
        "try:\n",
        "    import spacy\n",
        "    # Try to load the model directly without 'download' first\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy 'en_core_web_sm' model already loaded.\")\n",
        "except OSError:\n",
        "    print(\"spaCy model 'en_core_web_sm' not found. Downloading...\")\n",
        "    from spacy.cli import download\n",
        "    download(\"en_core_web_sm\")\n",
        "    import spacy\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy 'en_core_web_sm' model downloaded and loaded.\")\n",
        "\n",
        "import torch\n",
        "from transformers import LongformerModel, LongformerTokenizer, BartForConditionalGeneration, BartTokenizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "import numpy as np\n",
        "\n",
        "# --- Load Longformer Model and Tokenizer (for Extractive) ---\n",
        "print(\"Loading Longformer model and tokenizer...\")\n",
        "longformer_model_name = 'allenai/longformer-base-4096'\n",
        "longformer_tokenizer = LongformerTokenizer.from_pretrained(longformer_model_name)\n",
        "longformer_model = LongformerModel.from_pretrained(longformer_model_name)\n",
        "\n",
        "# --- Load BART Model and Tokenizer (for Abstractive) ---\n",
        "print(\"Loading BART model and tokenizer for abstractive summarization...\")\n",
        "bart_model_name = 'facebook/bart-large-cnn' # This is a good choice for summarization\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(bart_model_name)\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(bart_model_name)\n",
        "\n",
        "\n",
        "# --- Set Device (GPU if available, else CPU) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "longformer_model.eval() # Set to evaluation mode\n",
        "longformer_model.to(device)\n",
        "print(f\"Longformer using device: {device}\")\n",
        "\n",
        "bart_model.eval() # Set to evaluation mode\n",
        "bart_model.to(device)\n",
        "print(f\"BART using device: {device}\")\n",
        "print(\"All models loaded and moved to device.\")\n",
        "\n",
        "# --- Helper Function for Sentence Embeddings (Longformer) ---\n",
        "def get_sentence_embeddings(text, batch_size=4):\n",
        "    \"\"\"\n",
        "    Splits text into sentences, tokenizes them, and gets Longformer embeddings.\n",
        "    Handles long documents by processing sentences in batches.\n",
        "    Returns:\n",
        "        sentences (list): List of original sentence strings.\n",
        "        sentence_embeddings (np.array): NumPy array of sentence embeddings.\n",
        "    \"\"\"\n",
        "    # nlp, longformer_tokenizer, longformer_model, and device are global here\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "    if not sentences:\n",
        "        print(\"Warning: No valid sentences found in the input text.\")\n",
        "        return [], np.array([])\n",
        "\n",
        "    all_sentence_embeddings = []\n",
        "    print(f\"Total sentences to process: {len(sentences)}\")\n",
        "\n",
        "    for i in range(0, len(sentences), batch_size):\n",
        "        batch_sentences = sentences[i:i + batch_size]\n",
        "        try:\n",
        "            inputs = longformer_tokenizer(\n",
        "                batch_sentences,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=longformer_tokenizer.model_max_length\n",
        "            ).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = longformer_model(**inputs)\n",
        "\n",
        "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "            all_sentence_embeddings.extend(cls_embeddings)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch of sentences (index {i}-{i+len(batch_sentences)-1}): {e}\")\n",
        "            all_sentence_embeddings.extend([np.zeros(longformer_model.config.hidden_size)] * len(batch_sentences))\n",
        "            continue\n",
        "\n",
        "    return sentences, np.array(all_sentence_embeddings)\n",
        "\n",
        "# --- Centroid-Based Summarization Function (Optimized to accept pre-calculated embeddings) ---\n",
        "def centroid_summarization_optimized(sentences, embeddings, num_sentences=3):\n",
        "    \"\"\"\n",
        "    Generates an extractive summary using a centroid-based approach.\n",
        "    Accepts pre-calculated sentences and embeddings.\n",
        "    \"\"\"\n",
        "    if not sentences or embeddings.shape[0] == 0:\n",
        "        print(\"  No sentences or embeddings provided. Cannot summarize.\")\n",
        "        return [], []\n",
        "\n",
        "    if num_sentences <= 0:\n",
        "        print(\"  Number of sentences for summary must be positive.\")\n",
        "        return [], []\n",
        "\n",
        "    num_sentences_to_extract = min(num_sentences, len(sentences))\n",
        "\n",
        "    document_centroid = np.mean(embeddings, axis=0)\n",
        "    similarities = cosine_similarity(embeddings, document_centroid.reshape(1, -1)).flatten()\n",
        "\n",
        "    summary_sentences_mmr = []\n",
        "    selected_indices = set() # Correctly initialized here\n",
        "    ranked_initial_indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "    for _ in range(num_sentences_to_extract):\n",
        "        best_sentence_idx = -1\n",
        "        max_mmr_score = -1\n",
        "\n",
        "        for i in ranked_initial_indices:\n",
        "            if i not in selected_indices:\n",
        "                relevance = similarities[i]\n",
        "\n",
        "                if not selected_indices: # This check is now safe\n",
        "                    mmr_score = relevance\n",
        "                else:\n",
        "                    diversity_scores = cosine_similarity(embeddings[i].reshape(1, -1),\n",
        "                                                         embeddings[list(selected_indices)])\n",
        "                    redundancy = np.max(diversity_scores)\n",
        "                    lambda_param = 0.7\n",
        "                    mmr_score = lambda_param * relevance - (1 - lambda_param) * redundancy\n",
        "\n",
        "                if mmr_score > max_mmr_score:\n",
        "                    max_mmr_score = mmr_score\n",
        "                    best_sentence_idx = i\n",
        "\n",
        "        if best_sentence_idx != -1:\n",
        "            summary_sentences_mmr.append((sentences[best_sentence_idx], best_sentence_idx))\n",
        "            selected_indices.add(best_sentence_idx)\n",
        "            ranked_initial_indices = ranked_initial_indices[ranked_initial_indices != best_sentence_idx]\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    summary_sentences_mmr.sort(key=lambda x: x[1])\n",
        "    final_summary_sents = [s[0] for s in summary_sentences_mmr]\n",
        "    final_summary_indices = [s[1] for s in summary_sentences_mmr]\n",
        "\n",
        "    return final_summary_sents, final_summary_indices\n",
        "\n",
        "# --- K-Means Based Summarization Function (Optimized to accept pre-calculated embeddings) ---\n",
        "def kmeans_summarization_optimized(sentences, embeddings, num_clusters=5, num_sentences_per_cluster=1):\n",
        "    \"\"\"\n",
        "    Generates an extractive summary using K-Means clustering.\n",
        "    Accepts pre-calculated sentences and embeddings.\n",
        "    \"\"\"\n",
        "    if not sentences or embeddings.shape[0] == 0:\n",
        "        print(\"  No sentences or embeddings provided. Cannot summarize.\")\n",
        "        return [], []\n",
        "\n",
        "    if num_clusters <= 0 or num_sentences_per_cluster <= 0:\n",
        "        print(\"  Number of clusters and sentences per cluster must be positive.\")\n",
        "        return [], []\n",
        "\n",
        "    effective_num_clusters = min(num_clusters, len(sentences))\n",
        "\n",
        "    if effective_num_clusters == 0:\n",
        "        print(\"  Not enough sentences to form clusters.\")\n",
        "        return [], []\n",
        "\n",
        "    kmeans = KMeans(n_clusters=effective_num_clusters, random_state=42, n_init='auto')\n",
        "    kmeans.fit(embeddings)\n",
        "    clusters = kmeans.labels_\n",
        "    centroids = kmeans.cluster_centers_\n",
        "\n",
        "    summary_sentences_with_idx = []\n",
        "    selected_indices = set()\n",
        "\n",
        "    for i in range(effective_num_clusters):\n",
        "        cluster_sentence_indices = np.where(clusters == i)[0]\n",
        "\n",
        "        if len(cluster_sentence_indices) == 0:\n",
        "            continue\n",
        "\n",
        "        distances = cdist(embeddings[cluster_sentence_indices], centroids[i].reshape(1, -1), 'cosine').flatten()\n",
        "        sorted_cluster_indices = cluster_sentence_indices[np.argsort(distances)]\n",
        "\n",
        "        count_selected_from_cluster = 0\n",
        "        for original_idx in sorted_cluster_indices:\n",
        "            if original_idx not in selected_indices:\n",
        "                summary_sentences_with_idx.append((sentences[original_idx], original_idx))\n",
        "                selected_indices.add(original_idx)\n",
        "                count_selected_from_cluster += 1\n",
        "                if count_selected_from_cluster >= num_sentences_per_cluster:\n",
        "                    break\n",
        "\n",
        "    summary_sentences_with_idx.sort(key=lambda x: x[1])\n",
        "    final_summary_sents = [s[0] for s in summary_sentences_with_idx]\n",
        "    final_summary_indices = [s[1] for s in summary_sentences_with_idx]\n",
        "\n",
        "    return final_summary_sents, final_summary_indices\n",
        "\n",
        "# --- Combined Extractive Summarization Function (Optimized) ---\n",
        "def combined_extractive_summary_optimized(sentences, embeddings, total_summary_sentences=7,\n",
        "                                centroid_sentences_to_propose=5,\n",
        "                                kmeans_clusters_to_propose=4,\n",
        "                                kmeans_sentences_per_cluster_to_propose=1,\n",
        "                                lambda_param_mmr=0.7):\n",
        "    \"\"\"\n",
        "    Generates a single extractive summary by combining candidates from\n",
        "    both centroid-based and K-Means approaches, then using MMR for final selection.\n",
        "    Accepts pre-calculated sentences and embeddings.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Combined Extractive Summarization Candidate Generation ---\")\n",
        "    if not sentences or embeddings.shape[0] == 0:\n",
        "        print(\"  No sentences or embeddings provided. Cannot summarize combined.\")\n",
        "        return []\n",
        "\n",
        "    centroid_candidates_sents, centroid_candidates_indices = centroid_summarization_optimized(\n",
        "        sentences, embeddings, num_sentences=centroid_sentences_to_propose\n",
        "    )\n",
        "    print(f\"  Centroid proposed {len(centroid_candidates_sents)} candidates.\")\n",
        "\n",
        "    kmeans_candidates_sents, kmeans_candidates_indices = kmeans_summarization_optimized(\n",
        "        sentences, embeddings, num_clusters=kmeans_clusters_to_propose, num_sentences_per_cluster=kmeans_sentences_per_cluster_to_propose\n",
        "    )\n",
        "    print(f\"  K-Means proposed {len(kmeans_candidates_sents)} candidates.\")\n",
        "\n",
        "    # Combine candidates and their original indices, removing duplicates\n",
        "    combined_candidates_map = {}\n",
        "    for idx, sent in zip(centroid_candidates_indices, centroid_candidates_sents):\n",
        "        combined_candidates_map[idx] = sent\n",
        "    for idx, sent in zip(kmeans_candidates_indices, kmeans_candidates_sents):\n",
        "        combined_candidates_map[idx] = sent\n",
        "\n",
        "    all_candidate_indices_sorted = sorted(combined_candidates_map.keys())\n",
        "    all_candidate_sentences = [combined_candidates_map[idx] for idx in all_candidate_indices_sorted]\n",
        "    all_candidate_embeddings = np.array([embeddings[idx] for idx in all_candidate_indices_sorted])\n",
        "\n",
        "    if not all_candidate_sentences or all_candidate_embeddings.shape[0] == 0:\n",
        "        print(\"  No unique candidates found after combining. Cannot generate combined summary.\")\n",
        "        return []\n",
        "\n",
        "    num_sentences_to_extract = min(total_summary_sentences, len(all_candidate_sentences))\n",
        "    print(f\"  Total unique candidates: {len(all_candidate_sentences)}. Extracting {num_sentences_to_extract} for combined summary.\")\n",
        "\n",
        "    document_centroid = np.mean(embeddings, axis=0)\n",
        "    candidate_similarities = cosine_similarity(all_candidate_embeddings, document_centroid.reshape(1, -1)).flatten()\n",
        "\n",
        "    final_summary_sentences = []\n",
        "    selected_candidate_indices = set() # <-- FIXED: Initialized here\n",
        "\n",
        "    ranked_initial_candidate_indices = np.argsort(candidate_similarities)[::-1]\n",
        "\n",
        "    for _ in range(num_sentences_to_extract):\n",
        "        best_idx_in_candidates = -1\n",
        "        max_mmr_score = -1\n",
        "\n",
        "        for i_candidate in ranked_initial_candidate_indices:\n",
        "            if i_candidate not in selected_candidate_indices:\n",
        "                relevance = candidate_similarities[i_candidate]\n",
        "\n",
        "                if not selected_candidate_indices:\n",
        "                    mmr_score = relevance\n",
        "                else:\n",
        "                    diversity_scores = cosine_similarity(all_candidate_embeddings[i_candidate].reshape(1, -1),\n",
        "                                                         all_candidate_embeddings[list(selected_candidate_indices)])\n",
        "                    redundancy = np.max(diversity_scores)\n",
        "\n",
        "                    mmr_score = lambda_param_mmr * relevance - (1 - lambda_param_mmr) * redundancy\n",
        "\n",
        "                if mmr_score > max_mmr_score:\n",
        "                    max_mmr_score = mmr_score\n",
        "                    best_idx_in_candidates = i_candidate\n",
        "\n",
        "        if best_idx_in_candidates != -1:\n",
        "            final_summary_sentences.append((all_candidate_sentences[best_idx_in_candidates],\n",
        "                                             all_candidate_indices_sorted[best_idx_in_candidates]))\n",
        "            selected_candidate_indices.add(best_idx_in_candidates)\n",
        "            ranked_initial_candidate_indices = ranked_initial_candidate_indices[ranked_initial_candidate_indices != best_idx_in_candidates]\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    final_summary_sentences.sort(key=lambda x: x[1])\n",
        "    final_summary = [s[0] for s in final_summary_sentences]\n",
        "\n",
        "    print(\"--- Combined Extractive Summarization Selection Complete ---\")\n",
        "    return final_summary\n",
        "\n",
        "# --- Abstractive Summarization Function (using BART) ---\n",
        "def bart_abstractive_summary(text_to_summarize, max_length=150, min_length=50, num_beams=4, early_stopping=True):\n",
        "    \"\"\"\n",
        "    Generates an abstractive summary using the pre-loaded BART model.\n",
        "    Assumes bart_tokenizer and bart_model are loaded globally.\n",
        "\n",
        "    Args:\n",
        "        text_to_summarize (str or list of str): The input text (or list of sentences) to summarize.\n",
        "                                                  If a list, it will be joined into a single string.\n",
        "        max_length (int): Maximum length of the generated summary.\n",
        "        min_length (int): Minimum length of the generated summary.\n",
        "        num_beams (int): Number of beams for beam search. Higher values lead to better quality but slower generation.\n",
        "        early_stopping (bool): Whether to stop beam search when all beams have finished their generation.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated abstractive summary.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting BART Abstractive Summarization ---\")\n",
        "\n",
        "    if isinstance(text_to_summarize, list):\n",
        "        text_to_summarize = \" \".join(text_to_summarize)\n",
        "\n",
        "    if not text_to_summarize.strip():\n",
        "        print(\"  Input text for abstractive summary is empty. Cannot summarize.\")\n",
        "        return \"\"\n",
        "\n",
        "    inputs = bart_tokenizer(\n",
        "        [text_to_summarize],\n",
        "        max_length=1024, # BART's typical max input length\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    ).to(device)\n",
        "\n",
        "    summary_ids = bart_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        num_beams=num_beams,\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        early_stopping=early_stopping\n",
        "    )\n",
        "\n",
        "    summary_text = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    print(\"--- BART Abstractive Summarization Complete ---\")\n",
        "    return summary_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEhKmnvf7L58",
        "outputId": "f3888790-a57b-468b-ddf7-29a2ab2ddaad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.16.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "spaCy 'en_core_web_sm' model already loaded.\n",
            "Loading Longformer model and tokenizer...\n",
            "Loading BART model and tokenizer for abstractive summarization...\n",
            "Longformer using device: cpu\n",
            "BART using device: cpu\n",
            "All models loaded and moved to device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Your Document Text\n",
        "\n",
        "long_document = \"\"\"\n",
        "Artificial intelligence (AI) has rapidly transformed various sectors, revolutionizing industries from healthcare to finance. In healthcare, AI assists in diagnosing diseases earlier and more accurately, personalizing treatment plans, and accelerating drug discovery. Machine learning algorithms, a subset of AI, analyze vast amounts of patient data to identify patterns that human doctors might miss, leading to more effective interventions. For instance, AI-powered tools can detect subtle signs of retinopathy from eye scans, potentially preventing blindness. The integration of AI into electronic health records is also streamlining administrative tasks, freeing up medical professionals to focus more on patient care. This technological leap promises to enhance diagnostic capabilities and optimize treatment protocols significantly.\n",
        "\n",
        "The financial industry also heavily leverages AI for fraud detection, algorithmic trading, and personalized financial advice. AI systems can monitor transactions in real-time, identifying unusual patterns indicative of fraudulent activity with high precision. Furthermore, robo-advisors powered by AI provide automated, data-driven investment advice tailored to individual risk tolerance and financial goals, making financial planning more accessible to a wider demographic. The use of AI in predicting market trends and managing portfolios is becoming increasingly sophisticated, offering new avenues for investors.\n",
        "\n",
        "Beyond these, AI is deeply embedded in everyday life through virtual assistants like Siri and Alexa, recommendation engines on streaming platforms, and autonomous vehicles. AI's role in natural language processing (NLP) has led to advancements in language translation and sentiment analysis, impacting global communication and customer service. The ethical implications of AI, however, are a growing concern among researchers and policymakers. Issues such as algorithmic bias, job displacement due to automation, and privacy breaches require careful consideration and robust regulation. Ensuring transparency, fairness, and accountability in AI development is paramount to harnessing its benefits responsibly.\n",
        "\n",
        "Research in AI continues to advance at an astonishing pace, focusing on areas like explainable AI (XAI) to make AI decisions more understandable, and robust AI to improve performance in real-world, unpredictable environments. Novel architectures like generative adversarial networks (GANs) and reinforcement learning are pushing the boundaries of what AI can achieve, from creating realistic imagery to mastering complex games. The future of AI promises even more integration into society, with potential breakthroughs in areas like general artificial intelligence (AGI) and enhanced human-computer interaction, leading to smarter cities and more efficient resource management. However, achieving these advancements responsibly will necessitate ongoing collaboration between technologists, policymakers, and ethicists to address the complex challenges that arise. The rapid pace of development means that continuous public discourse and legislative adaptation are critical to navigate the challenges and maximize the societal benefits of AI, ensuring it serves humanity's best interests.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DF25_IWL_8_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 3: The Summarization Pipeline\n",
        "\n",
        "print(\"--- Starting Hybrid Summarization Pipeline ---\")\n",
        "print(\"Original Document Length (sentences):\", sum(1 for _ in nlp(long_document).sents))\n",
        "\n",
        "\n",
        "# Step 1: Generate Sentence Embeddings using Longformer\n",
        "print(\"\\n[Pipeline Step 1/3] Calculating document embeddings with Longformer...\")\n",
        "sentences_list, embeddings_array = get_sentence_embeddings(long_document, batch_size=8)\n",
        "print(\"  Embeddings calculation complete.\")\n",
        "\n",
        "\n",
        "# Step 2: Generate Combined Extractive Summary\n",
        "print(\"\\n[Pipeline Step 2/3] Generating combined extractive summary...\")\n",
        "combined_extractive_summary_sentences = combined_extractive_summary_optimized(\n",
        "    sentences_list,\n",
        "    embeddings_array,\n",
        "    total_summary_sentences=6, # Desired length for the extractive part\n",
        "    centroid_sentences_to_propose=7,\n",
        "    kmeans_clusters_to_propose=5,\n",
        "    kmeans_sentences_per_cluster_to_propose=1\n",
        ")\n",
        "print(f\"  Extracted {len(combined_extractive_summary_sentences)} sentences.\")\n",
        "print(\"\\nExtractive Summary:\")\n",
        "for i, sent in enumerate(combined_extractive_summary_sentences):\n",
        "    print(f\"{i+1}. {sent}\")\n",
        "\n",
        "\n",
        "# Step 3: Generate Abstractive Summary from Extractive Output using BART\n",
        "print(\"\\n[Pipeline Step 3/3] Generating abstractive summary with BART...\")\n",
        "extractive_text_for_abstractive = \" \".join(combined_extractive_summary_sentences)\n",
        "final_abstractive_summary = bart_abstractive_summary(\n",
        "    extractive_text_for_abstractive,\n",
        "    max_length=150, # Max length of the final abstractive summary\n",
        "    min_length=50,  # Min length of the final abstractive summary\n",
        "    num_beams=4     # Beam search parameter for quality\n",
        ")\n",
        "print(\"\\nAbstractive Summary:\")\n",
        "print(final_abstractive_summary)\n",
        "\n",
        "print(\"\\n--- Hybrid Summarization Pipeline Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kzc4YCE7ADPZ",
        "outputId": "6d45b1a0-2b16-4607-a33c-b7a87f851c59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Hybrid Summarization Pipeline ---\n",
            "Original Document Length (sentences): 20\n",
            "\n",
            "[Pipeline Step 1/3] Calculating document embeddings with Longformer...\n",
            "Total sentences to process: 20\n",
            "  Embeddings calculation complete.\n",
            "\n",
            "[Pipeline Step 2/3] Generating combined extractive summary...\n",
            "\n",
            "--- Starting Combined Extractive Summarization Candidate Generation ---\n",
            "  Centroid proposed 7 candidates.\n",
            "  K-Means proposed 5 candidates.\n",
            "  Total unique candidates: 11. Extracting 6 for combined summary.\n",
            "--- Combined Extractive Summarization Selection Complete ---\n",
            "  Extracted 6 sentences.\n",
            "\n",
            "Extractive Summary:\n",
            "1. Artificial intelligence (AI) has rapidly transformed various sectors, revolutionizing industries from healthcare to finance.\n",
            "2. The integration of AI into electronic health records is also streamlining administrative tasks, freeing up medical professionals to focus more on patient care.\n",
            "3. Furthermore, robo-advisors powered by AI provide automated, data-driven investment advice tailored to individual risk tolerance and financial goals, making financial planning more accessible to a wider demographic.\n",
            "4. Issues such as algorithmic bias, job displacement due to automation, and privacy breaches require careful consideration and robust regulation.\n",
            "5. The future of AI promises even more integration into society, with potential breakthroughs in areas like general artificial intelligence (AGI) and enhanced human-computer interaction, leading to smarter cities and more efficient resource management.\n",
            "6. The rapid pace of development means that continuous public discourse and legislative adaptation are critical to navigate the challenges and maximize the societal benefits of AI, ensuring it serves humanity's best interests.\n",
            "\n",
            "[Pipeline Step 3/3] Generating abstractive summary with BART...\n",
            "\n",
            "--- Starting BART Abstractive Summarization ---\n",
            "--- BART Abstractive Summarization Complete ---\n",
            "\n",
            "Abstractive Summary:\n",
            "Artificial intelligence (AI) has rapidly transformed various sectors. Issues such as algorithmic bias, job displacement due to automation, and privacy breaches require careful consideration and robust regulation. Future of AI promises even more integration into society, with potential breakthroughs in areas like general artificial intelligence.\n",
            "\n",
            "--- Hybrid Summarization Pipeline Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DKbYwP_ilxC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rough\n"
      ],
      "metadata": {
        "id": "d445q_fulxvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "# --- Load BART Model and Tokenizer ---\n",
        "print(\"Loading BART model and tokenizer for abstractive summarization...\")\n",
        "bart_model_name = 'facebook/bart-large-cnn'\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(bart_model_name)\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(bart_model_name)\n",
        "\n",
        "# --- Set Device (GPU if available, else CPU) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "bart_model.eval()\n",
        "bart_model.to(device)\n",
        "print(f\"BART using device: {device}\")\n",
        "print(\"BART model loaded.\")\n",
        "\n",
        "# Load spaCy for fact extraction\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy model loaded.\")\n",
        "except:\n",
        "    print(\"Installing spaCy model...\")\n",
        "    import os\n",
        "    os.system(\"python -m spacy download en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy model loaded.\")\n",
        "\n",
        "# --- Helper Functions for Constrained Beam Search ---\n",
        "\n",
        "def extract_key_facts(extractive_output, importance_threshold=0.7):\n",
        "    \"\"\"Extract key facts from the extractive summarization output\"\"\"\n",
        "    if isinstance(extractive_output, str):\n",
        "        # If input is a string, treat it as a single sentence with high importance\n",
        "        key_sentences = [extractive_output]\n",
        "    elif isinstance(extractive_output, list) and all(isinstance(item, str) for item in extractive_output):\n",
        "        # If input is a list of strings, use all sentences\n",
        "        key_sentences = extractive_output\n",
        "    else:\n",
        "        # If input is a list of (sentence, score) tuples\n",
        "        key_sentences = [sent for sent, score in extractive_output if score > importance_threshold]\n",
        "\n",
        "    # Process sentences to extract atomic facts\n",
        "    facts = []\n",
        "    for sentence in key_sentences:\n",
        "        # Simple approach: use key noun phrases and entities\n",
        "        doc = nlp(sentence)\n",
        "        for chunk in doc.noun_chunks:\n",
        "            if len(chunk.text.split()) > 1:  # Filter out very short phrases\n",
        "                facts.append(chunk.text)\n",
        "\n",
        "        # Add named entities\n",
        "        for ent in doc.ents:\n",
        "            facts.append(ent.text)\n",
        "\n",
        "    # Deduplicate facts\n",
        "    return list(set(facts))\n",
        "\n",
        "def prepare_constraints(facts, tokenizer):\n",
        "    \"\"\"Convert textual facts to token IDs for constraint checking\"\"\"\n",
        "    constraints = []\n",
        "    for fact in facts:\n",
        "        # Tokenize the fact\n",
        "        fact_tokens = tokenizer.encode(fact, add_special_tokens=False)\n",
        "\n",
        "        # Only use facts that aren't too long or too short\n",
        "        if 2 <= len(fact_tokens) <= 10:\n",
        "            constraints.append(fact_tokens)\n",
        "\n",
        "    return constraints\n",
        "\n",
        "def is_subsequence(smaller, larger):\n",
        "    \"\"\"Check if smaller list appears as a subsequence in larger list\"\"\"\n",
        "    i = j = 0\n",
        "    while i < len(smaller) and j < len(larger):\n",
        "        if smaller[i] == larger[j]:\n",
        "            i += 1\n",
        "        j += 1\n",
        "    return i == len(smaller)\n",
        "\n",
        "def check_constraints(sequence, constraints):\n",
        "    \"\"\"Check which constraints are satisfied by the current sequence\"\"\"\n",
        "    satisfied = []\n",
        "\n",
        "    for i, constraint in enumerate(constraints):\n",
        "        # Check if constraint tokens appear in sequence in the correct order\n",
        "        if is_subsequence(constraint, sequence):\n",
        "            satisfied.append(i)\n",
        "\n",
        "    return satisfied\n",
        "\n",
        "# --- Constrained Beam Search Implementation ---\n",
        "\n",
        "def constrained_beam_search(model, input_ids, attention_mask, constraints,\n",
        "                           num_beams=4, max_length=150, min_length=50,\n",
        "                           constraint_weight=2.0):\n",
        "    \"\"\"\n",
        "    Implements constrained beam search for BART summarization.\n",
        "\n",
        "    Args:\n",
        "        model: The BART model\n",
        "        input_ids: Tokenized input text\n",
        "        attention_mask: Attention mask for input\n",
        "        constraints: List of token sequences that should appear in the output\n",
        "        num_beams: Number of beams for beam search\n",
        "        max_length: Maximum length of the generated summary\n",
        "        min_length: Minimum length of the generated summary\n",
        "        constraint_weight: Weight given to satisfying constraints\n",
        "\n",
        "    Returns:\n",
        "        The generated summary that satisfies the most constraints\n",
        "    \"\"\"\n",
        "    # Get encoder output once\n",
        "    encoder_outputs = model.get_encoder()(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "\n",
        "    # Initialize beams: (tokens, log_prob, satisfied_constraints)\n",
        "    batch_size = input_ids.shape[0]\n",
        "    device = input_ids.device\n",
        "\n",
        "    # Start with the decoder start token\n",
        "    decoder_start_token_id = model.config.decoder_start_token_id\n",
        "    beams = [([decoder_start_token_id], 0.0, set()) for _ in range(num_beams)]\n",
        "\n",
        "    # Track completed sequences\n",
        "    done_beams = []\n",
        "\n",
        "    # Main beam search loop\n",
        "    for step in range(max_length):\n",
        "        all_candidates = []\n",
        "\n",
        "        # Check if all beams are done\n",
        "        if len(done_beams) == num_beams:\n",
        "            break\n",
        "\n",
        "        # Prepare current tokens for all beams\n",
        "        active_beams = [b for b in beams if b[0][-1] != model.config.eos_token_id]\n",
        "        if not active_beams:\n",
        "            break\n",
        "\n",
        "        current_tokens = [beam[0] for beam in active_beams]\n",
        "        max_len = max(len(tokens) for tokens in current_tokens)\n",
        "\n",
        "        # Pad and create tensor\n",
        "        padded_tokens = [tokens + [model.config.pad_token_id] * (max_len - len(tokens)) for tokens in current_tokens]\n",
        "        decoder_input = torch.tensor(padded_tokens, device=device)\n",
        "\n",
        "        # Get next token predictions\n",
        "        with torch.no_grad():\n",
        "            outputs = model.decoder(\n",
        "                input_ids=decoder_input,\n",
        "                encoder_hidden_states=encoder_outputs.last_hidden_state,\n",
        "                encoder_attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            logits = outputs[0]  # Get logits\n",
        "\n",
        "            # Process each beam\n",
        "            for beam_idx, (tokens, score, satisfied) in enumerate(active_beams):\n",
        "                # Get logits for the last token\n",
        "                curr_logits = logits[beam_idx, len(tokens)-1, :]\n",
        "\n",
        "                # Apply softmax to get probabilities\n",
        "                probs = torch.nn.functional.softmax(curr_logits, dim=-1)\n",
        "                log_probs = torch.log(probs + 1e-10)  # Add small epsilon to avoid log(0)\n",
        "\n",
        "                # Get top tokens\n",
        "                topk_log_probs, topk_indices = torch.topk(log_probs, num_beams * 2)\n",
        "\n",
        "                # Create new candidates\n",
        "                for log_prob, token_id in zip(topk_log_probs.tolist(), topk_indices.tolist()):\n",
        "                    new_tokens = tokens + [token_id]\n",
        "                    new_score = score + log_prob\n",
        "\n",
        "                    # Check which constraints are newly satisfied\n",
        "                    new_satisfied = set(satisfied)\n",
        "                    for i, constraint in enumerate(constraints):\n",
        "                        if i not in new_satisfied and is_subsequence(constraint, new_tokens):\n",
        "                            new_satisfied.add(i)\n",
        "\n",
        "                    # Apply constraint bonus\n",
        "                    constraint_bonus = len(new_satisfied) * constraint_weight\n",
        "                    adjusted_score = new_score + constraint_bonus\n",
        "\n",
        "                    # Add to candidates\n",
        "                    all_candidates.append((new_tokens, adjusted_score, new_satisfied))\n",
        "\n",
        "                    # Check if this is a completed sequence\n",
        "                    if token_id == model.config.eos_token_id and len(new_tokens) >= min_length:\n",
        "                        done_beams.append((new_tokens, adjusted_score, new_satisfied))\n",
        "\n",
        "        # Select top beams for next iteration\n",
        "        beams = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:num_beams]\n",
        "\n",
        "    # If we have completed sequences, return the best one\n",
        "    if done_beams:\n",
        "        # Sort by number of constraints satisfied, then by score\n",
        "        best_beam = max(done_beams, key=lambda x: (len(x[2]), x[1]))\n",
        "        return best_beam[0]\n",
        "\n",
        "    # If no sequence completed, return the best current beam\n",
        "    best_beam = max(beams, key=lambda x: (len(x[2]), x[1]))\n",
        "    return best_beam[0]\n",
        "\n",
        "# --- Abstractive Summarization with Constrained Beam Search ---\n",
        "\n",
        "def constrained_bart_summary(text_to_summarize, constraint_sentences=None,\n",
        "                            max_length=150, min_length=50, num_beams=4):\n",
        "    \"\"\"\n",
        "    Generates an abstractive summary using BART with constrained beam search.\n",
        "\n",
        "    Args:\n",
        "        text_to_summarize (str): The input text to summarize\n",
        "        constraint_sentences (list): List of sentences containing facts that must be included\n",
        "        max_length (int): Maximum length of the generated summary\n",
        "        min_length (int): Minimum length of the generated summary\n",
        "        num_beams (int): Number of beams for beam search\n",
        "\n",
        "    Returns:\n",
        "        str: The generated abstractive summary that includes the key facts\n",
        "    \"\"\"\n",
        "    print(\"Starting BART Abstractive Summarization with Constrained Beam Search ---\")\n",
        "\n",
        "    if isinstance(text_to_summarize, list):\n",
        "        text_to_summarize = \" \".join(text_to_summarize)\n",
        "\n",
        "    if not text_to_summarize.strip():\n",
        "        print(\"  Input text for abstractive summary is empty. Cannot summarize.\")\n",
        "        return \"\"\n",
        "\n",
        "    # Process input text\n",
        "    inputs = bart_tokenizer(\n",
        "        [text_to_summarize],\n",
        "        max_length=1024,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    ).to(device)\n",
        "\n",
        "    # Extract and prepare constraints if provided\n",
        "    constraints = []\n",
        "    if constraint_sentences:\n",
        "        print(f\"Extracting key facts from {len(constraint_sentences)} constraint sentences...\")\n",
        "        facts = extract_key_facts(constraint_sentences)\n",
        "        print(f\"Extracted {len(facts)} key facts: {facts[:5]}...\")\n",
        "        constraints = prepare_constraints(facts, bart_tokenizer)\n",
        "        print(f\"Prepared {len(constraints)} constraints for beam search\")\n",
        "\n",
        "    # If no constraints or constraint extraction failed, fall back to standard beam search\n",
        "    if not constraints:\n",
        "        print(\"No constraints provided or extracted. Using standard beam search.\")\n",
        "        summary_ids = bart_model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            num_beams=num_beams,\n",
        "            max_length=max_length,\n",
        "            min_length=min_length,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        summary_text = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    else:\n",
        "        # Use constrained beam search\n",
        "        print(\"Using constrained beam search with extracted facts...\")\n",
        "        output_ids = constrained_beam_search(\n",
        "            model=bart_model,\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            constraints=constraints,\n",
        "            num_beams=num_beams,\n",
        "            max_length=max_length,\n",
        "            min_length=min_length\n",
        "        )\n",
        "        summary_text = bart_tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "\n",
        "    print(\"--- BART Abstractive Summarization with Constrained Beam Search Complete ---\")\n",
        "    return summary_text\n",
        "\n",
        "# --- Example Usage ---\n",
        "# This is where you would input your text and constraint sentences\n",
        "\n",
        "# Example:\n",
        "\n",
        "input_text = \"\"\"Artificial intelligence (AI) has rapidly become a transformative force across various industries. In healthcare, AI systems assist doctors by analyzing medical images, predicting patient risks, and streamlining administrative tasks through automated electronic health records. Hospitals are increasingly relying on AI tools to optimize patient scheduling and improve diagnostic accuracy. In finance, AI-driven algorithms power fraud detection systems, assess credit risk, and support robo-advisors that provide tailored investment advice based on individual financial goals and risk tolerance.\n",
        "\n",
        "AI is also playing a crucial role in transportation. Self-driving cars and traffic optimization systems use vast amounts of data to reduce accidents and improve traffic flow in urban areas. Meanwhile, the education sector is leveraging AI-powered personalized learning platforms that adapt to students’ strengths and weaknesses, enhancing engagement and learning outcomes.\n",
        "\n",
        "However, the rise of AI comes with challenges. Concerns about algorithmic bias, data privacy, and job displacement are prompting calls for stronger regulations and ethical guidelines. Privacy breaches can occur when sensitive personal data is mishandled by AI systems, while automation threatens certain repetitive or low-skilled jobs.\n",
        "\n",
        "The future of AI looks promising, with ongoing research into general artificial intelligence (AGI) and advanced human-computer interaction. Smarter cities, more efficient energy management, and breakthroughs in medicine are all on the horizon. To ensure AI serves humanity’s best interests, governments, companies, and researchers must engage in continuous public discourse, adapt regulations, and focus on ethical deployment of this powerful technology.\"\"\"\n",
        "\n",
        "# These are the factual and relationship constraint sentences\n",
        "constraint_sentences = [\n",
        "    \"Education uses personalized learning platforms powered by AI.\"\n",
        "]\n",
        "\n",
        "summary = constrained_bart_summary(\n",
        "    input_text,\n",
        "    constraint_sentences=constraint_sentences,\n",
        "    max_length=150,\n",
        "    min_length=50,\n",
        "    num_beams=4\n",
        ")\n",
        "\n",
        "print(\"\" + \"=\"*80)\n",
        "print(\"Constrained Abstractive Summary:\")\n",
        "print(summary)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzGfC1EWl1Kz",
        "outputId": "689a7c00-21f5-47e3-afcd-1d478f5bc5fb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BART model and tokenizer for abstractive summarization...\n",
            "BART using device: cpu\n",
            "BART model loaded.\n",
            "spaCy model loaded.\n",
            "Starting BART Abstractive Summarization with Constrained Beam Search ---\n",
            "Extracting key facts from 1 constraint sentences...\n",
            "Extracted 3 key facts: ['Education', 'personalized learning platforms', 'AI']...\n",
            "Prepared 1 constraints for beam search\n",
            "Using constrained beam search with extracted facts...\n",
            "--- BART Abstractive Summarization with Constrained Beam Search Complete ---\n",
            "================================================================================\n",
            "Constrained Abstractive Summary:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your beam search still outputs only <\\s> because the first predicted token is EOS.\n",
        "Even with early EOS blocking, this can happen if:\n",
        "\n",
        "Probability of EOS dominates initially, which happens with small batch beam search using custom decoding.\n",
        "\n",
        "No token in the top-k log probs survives filtering because of the early EOS skip.\n",
        "\n",
        "Constraint subsequences are hard to satisfy, and beam scoring leads to pruning all sequences.\n",
        "\n",
        "Why this happens\n",
        "You are starting with decoder_start_token_id = 0 (BART <s>).\n",
        "\n",
        "On the first step, the model often outputs EOS with very high probability.\n",
        "\n",
        "In your beam loop, if EOS is skipped and all candidates are empty, it leads to immediate termination.\n",
        "\n",
        "Robust Fix\n",
        "Instead of hand-rolling beam search, leverage Hugging Face’s generate with force_words_ids, which natively handles constraints and prevents EOS problems.\n",
        "\n",
        "Here’s a simpler and working solution:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n"
      ],
      "metadata": {
        "id": "7ZxC0vpX1VYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "bart_model_name = 'facebook/bart-large-cnn'\n",
        "tokenizer = BartTokenizer.from_pretrained(bart_model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(bart_model_name).eval()\n",
        "\n",
        "text = \"\"\"Artificial intelligence (AI) has rapidly become a transformative force across various industries.\n",
        "In healthcare, AI systems assist doctors by analyzing medical images, predicting patient risks, and streamlining administrative tasks\n",
        "through automated electronic health records. Hospitals are increasingly relying on AI tools to optimize patient scheduling and improve\n",
        "diagnostic accuracy. In finance, AI-driven algorithms power fraud detection systems, assess credit risk, and support robo-advisors\n",
        "that provide tailored investment advice based on individual financial goals and risk tolerance.\n",
        "\n",
        "AI is also playing a crucial role in transportation. Self-driving cars and traffic optimization systems use vast amounts of data\n",
        "to reduce accidents and improve traffic flow in urban areas. Meanwhile, the education sector is leveraging AI-powered personalized\n",
        "learning platforms that adapt to students’ strengths and weaknesses, enhancing engagement and learning outcomes.\n",
        "\n",
        "However, the rise of AI comes with challenges. Concerns about algorithmic bias, data privacy, and job displacement are prompting\n",
        "calls for stronger regulations and ethical guidelines. Privacy breaches can occur when sensitive personal data is mishandled by\n",
        "AI systems, while automation threatens certain repetitive or low-skilled jobs.\n",
        "\n",
        "The future of AI looks promising, with ongoing research into general artificial intelligence (AGI) and advanced human-computer\n",
        "interaction. Smarter cities, more efficient energy management, and breakthroughs in medicine are all on the horizon. To ensure\n",
        "AI serves humanity’s best interests, governments, companies, and researchers must engage in continuous public discourse, adapt\n",
        "regulations, and focus on ethical deployment of this powerful technology.\n",
        "\"\"\"\n",
        "\n",
        "# --- Constraint words ---\n",
        "constraint_words = [\"Education\", \"AI\"]\n",
        "force_words_ids = [tokenizer([w], add_special_tokens=False).input_ids[0] for w in constraint_words]\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "\n",
        "summary_ids = model.generate(\n",
        "    **inputs,\n",
        "    max_length=150,\n",
        "    min_length=50,\n",
        "    num_beams=5,\n",
        "    force_words_ids=force_words_ids,\n",
        "    no_repeat_ngram_size=3,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(\"Constrained Abstractive Summary:\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPyeUdKEpFNW",
        "outputId": "ae18b73c-e84a-479d-8a5d-cf4531ddfcfd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Constrained Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. To prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constrained Abstractive Summary:\n",
            "Artificial intelligence (AI) has rapidly become a transformative force across various industries. Concerns about algorithmic bias, data privacy, and job displacement are prompting calls for stronger regulations and ethical guidelines. The future of AI looks promising, with ongoing research into general artificial intelligence (AGI) and advanced human-computer interaction. Smarter cities, more efficient energy management, and breakthroughs in medicine are all on the horizon. To ensure AI serves humanity’s best interests, governments, companies, and researchers must engage in continuous public discourse, adapt  protections, and focus on ethical deployment of this powerful technology. Back to Mail Online home. back to the page you came from.  \"The Future of AI\" is publishedEducation\n"
          ]
        }
      ]
    }
  ]
}