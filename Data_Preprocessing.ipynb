{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3KdfFu6gfrIIJwlYGJaeo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nanda654/HEADS/blob/main/Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset preparation using TFIDF centroid similarity"
      ],
      "metadata": {
        "id": "QGrIuYqs0KuP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhmBzSUyx6v-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "import nltk\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer # Explicitly import PunktSentenceTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from tqdm import tqdm # Import tqdm for progress bar\n",
        "\n",
        "# --- 0. Configuration ---\n",
        "# Path to your existing govreport_raw JSON files\n",
        "GOVREPORT_RAW_DIR = \"./govreport_raw\"\n",
        "# Path to store the newly generated dataset with extractive summaries\n",
        "GENERATED_DATASET_DIR = \"./govreport_tfidf_vscode2\" # New output folder name for TF-IDF approach\n",
        "os.makedirs(GENERATED_DATASET_DIR, exist_ok=True)\n",
        "\n",
        "# Language for NLTK tokenization\n",
        "LANGUAGE = \"english\"\n",
        "# Similarity threshold to determine if an original sentence is \"related\" to the abstractive summary.\n",
        "# Adjust this value (0.0 to 1.0) to control how strict the relation must be.\n",
        "# Higher values mean only very similar sentences are included.\n",
        "SIMILARITY_THRESHOLD = 0.5\n",
        "\n",
        "# --- NLTK Resource Download and Tokenizer Initialization ---\n",
        "# This ensures 'punkt' is downloaded and the tokenizer is explicitly loaded once.\n",
        "punkt_tokenizer_instance = None\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    print(\"NLTK 'punkt' downloaded successfully.\")\n",
        "    # After download, the punkt tokenizer should be discoverable by PunktSentenceTokenizer\n",
        "    punkt_tokenizer_instance = PunktSentenceTokenizer()\n",
        "    print(\"Punkt tokenizer instance created successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading or initializing NLTK 'punkt' tokenizer: {e}\")\n",
        "    print(\"NLTK functions will likely fail. Please check internet connection and NLTK setup.\")\n",
        "    # Exit if NLTK setup fails, as the rest of the script depends on it.\n",
        "    exit()\n",
        "\n",
        "# --- 1. Load the existing govreport_raw dataset ---\n",
        "print(f\"Loading existing JSON files from: {GOVREPORT_RAW_DIR}\")\n",
        "try:\n",
        "    data_files = {\n",
        "        \"train\": os.path.join(GOVREPORT_RAW_DIR, \"train.json\"),\n",
        "        \"validation\": os.path.join(GOVREPORT_RAW_DIR, \"validation.json\"),\n",
        "        \"test\": os.path.join(GOVREPORT_RAW_DIR, \"test.json\"),\n",
        "    }\n",
        "\n",
        "    govreport_raw_data = load_dataset(\"json\", data_files=data_files)\n",
        "\n",
        "    print(\"\\nRaw GovReport data loaded successfully!\")\n",
        "    print(govreport_raw_data)\n",
        "except Exception as e:\n",
        "    print(f\"\\nError loading raw GovReport dataset from {GOVREPORT_RAW_DIR}: {e}\")\n",
        "    print(\"Please ensure the directory exists and contains 'train.json', 'validation.json', 'test.json'.\")\n",
        "    print(\"Exiting as the dataset could not be loaded.\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Define a Processing Function to Generate Reference-Based Extractive Summaries ---\n",
        "# Added 'language' and 'similarity_threshold' as arguments\n",
        "def generate_reference_extractive_and_combine(example, language, similarity_threshold):\n",
        "    \"\"\"\n",
        "    Generates an extractive summary by finding sentences in the original text\n",
        "    that are sufficiently similar to any sentence in the abstractive summary,\n",
        "    based on a defined SIMILARITY_THRESHOLD.\n",
        "    \"\"\"\n",
        "    # Use the globally defined punkt_tokenizer_instance to tokenize sentences\n",
        "    # No need for explicit nltk imports or downloads within this function when num_proc=1\n",
        "    sentences_original = punkt_tokenizer_instance.tokenize(example[\"document\"])\n",
        "    sentences_abstractive = punkt_tokenizer_instance.tokenize(example[\"summary\"])\n",
        "\n",
        "    original_text = example[\"document\"]\n",
        "    abstractive_summary = example[\"summary\"]\n",
        "\n",
        "    # Handle potential empty or non-string documents/summaries\n",
        "    if not isinstance(original_text, str) or not original_text.strip() or \\\n",
        "       not isinstance(abstractive_summary, str) or not abstractive_summary.strip():\n",
        "        return {\n",
        "            \"original_text\": original_text,\n",
        "            \"extractive_summary\": \"\",\n",
        "            \"abstractive_summary\": abstractive_summary\n",
        "        }\n",
        "\n",
        "    # If either list of sentences is empty, return empty extractive summary\n",
        "    if not sentences_original or not sentences_abstractive:\n",
        "        return {\n",
        "            \"original_text\": original_text,\n",
        "            \"extractive_summary\": \"\",\n",
        "            \"abstractive_summary\": abstractive_summary\n",
        "        }\n",
        "\n",
        "    # Combine all sentences for TF-IDF vectorization to ensure a consistent vocabulary\n",
        "    all_sentences = sentences_original + sentences_abstractive\n",
        "\n",
        "    # Handle case where all_sentences might be empty or contain only empty strings\n",
        "    # after tokenization, which can happen if original_text/abstractive_summary\n",
        "    # contained only whitespace or un-tokenizable characters.\n",
        "    if not any(s.strip() for s in all_sentences):\n",
        "        return {\n",
        "            \"original_text\": original_text,\n",
        "            \"extractive_summary\": \"\",\n",
        "            \"abstractive_summary\": abstractive_summary\n",
        "        }\n",
        "\n",
        "    vectorizer = TfidfVectorizer().fit(all_sentences)\n",
        "\n",
        "    # Transform sentences into TF-IDF vectors\n",
        "    original_vectors = vectorizer.transform(sentences_original)\n",
        "    abstractive_vectors = vectorizer.transform(sentences_abstractive)\n",
        "\n",
        "    extractive_sentences_indices = []\n",
        "\n",
        "    # Calculate cosine similarity between all original sentences and all abstractive sentences\n",
        "    # The result `similarity_matrix` will have dimensions (num_original_sentences, num_abstractive_sentences)\n",
        "    similarity_matrix = cosine_similarity(original_vectors, abstractive_vectors)\n",
        "\n",
        "    # For each original sentence, find its maximum similarity to any abstractive sentence\n",
        "    # If this max similarity is above the threshold, include the original sentence\n",
        "    for i, _ in enumerate(sentences_original):\n",
        "        # Get the similarities of the current original sentence to all abstractive sentences\n",
        "        current_original_sent_similarities = similarity_matrix[i]\n",
        "\n",
        "        # Find the maximum similarity for this original sentence\n",
        "        max_similarity_to_abstractive = np.max(current_original_sent_similarities)\n",
        "\n",
        "        if max_similarity_to_abstractive >= similarity_threshold: # Use passed similarity_threshold\n",
        "            extractive_sentences_indices.append(i)\n",
        "\n",
        "    # Reconstruct the extractive summary by selecting sentences based on their original order\n",
        "    # and the identified indices\n",
        "    extractive_summary = \" \".join([sentences_original[i] for i in sorted(extractive_sentences_indices)])\n",
        "\n",
        "    return {\n",
        "        \"original_text\": original_text,\n",
        "        \"extractive_summary\": extractive_summary,\n",
        "        \"abstractive_summary\": abstractive_summary\n",
        "    }\n",
        "\n",
        "# --- 3. Main Dataset Processing ---\n",
        "processed_data = DatasetDict()\n",
        "\n",
        "for split_name in [\"train\", \"validation\", \"test\"]:\n",
        "    print(f\"\\nProcessing '{split_name}' split to generate reference-based extractive summaries (TF-IDF Cosine Similarity)...\")\n",
        "\n",
        "    current_split_data = []\n",
        "    # Manually iterate with tqdm to show progress bar when num_proc=1\n",
        "    for example in tqdm(govreport_raw_data[split_name], desc=f\"Generating {split_name} extractive summaries\"):\n",
        "        # Call the processing function for each example\n",
        "        processed_example = generate_reference_extractive_and_combine(\n",
        "            example,\n",
        "            language=LANGUAGE,\n",
        "            similarity_threshold=SIMILARITY_THRESHOLD\n",
        "        )\n",
        "        current_split_data.append(processed_example)\n",
        "\n",
        "    # Convert the list of dictionaries to a Dataset object\n",
        "    processed_split = Dataset.from_list(current_split_data)\n",
        "    processed_data[split_name] = processed_split\n",
        "\n",
        "    print(f\"'{split_name}' split processed. New features: {processed_split.column_names}\")\n",
        "    print(f\"Sample from '{split_name}' split (first entry):\\n{processed_split[0]}\")\n",
        "\n",
        "    # Save the processed split to disk in JSON format\n",
        "    save_path = os.path.join(GENERATED_DATASET_DIR, f\"{split_name}.json\")\n",
        "    processed_split.to_json(save_path) # Save as JSONL by default for Datasets.to_json()\n",
        "    print(f\"'{split_name}' split saved to: {save_path}\")\n",
        "\n",
        "print(f\"\\nAll splits processed and saved to: {GENERATED_DATASET_DIR}\")\n",
        "print(\"\\nSummary of final dataset structure:\")\n",
        "print(processed_data)\n",
        "\n",
        "print(\"\\nTo load this generated dataset later, you can use:\")\n",
        "print(f\"from datasets import load_dataset\")\n",
        "print(f\"train_data = load_dataset('json', data_files='{GENERATED_DATASET_DIR}/train.json', split='train')\")\n",
        "print(f\"validation_data = load_dataset('json', data_files='{GENERATED_DATASET_DIR}/validation.json', split='validation')\")\n",
        "print(f\"test_data = load_dataset('json', data_files='{GENERATED_DATASET_DIR}/test.json', split='test')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset generation using both TFIDF centroid and greeedy ROUGE approach"
      ],
      "metadata": {
        "id": "quQCb8eayKm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "import nltk\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer # Explicitly import PunktSentenceTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from rouge_score import rouge_scorer # For ROUGE score calculation\n",
        "from tqdm import tqdm # Import tqdm for progress bar\n",
        "\n",
        "# --- 0. Configuration ---\n",
        "# Path to your existing govreport_raw JSON files\n",
        "GOVREPORT_RAW_DIR = \"./govreport_raw\"\n",
        "# Path to store the newly generated dataset with the combined extractive summary\n",
        "GENERATED_DATASET_DIR = \"./govreport_combined_extractive\" # New output folder name for combined approach\n",
        "os.makedirs(GENERATED_DATASET_DIR, exist_ok=True)\n",
        "\n",
        "# Language for NLTK tokenization and ROUGE scorer\n",
        "LANGUAGE = \"english\"\n",
        "\n",
        "# Configuration for Combined Extractive Summarization\n",
        "NUM_COMBINED_SENTENCES = 7 # Number of sentences for the combined hybrid summary\n",
        "WEIGHT_ROUGE = 0.6 # Weight for ROUGE score in combined calculation (0.0 to 1.0)\n",
        "WEIGHT_CENTROID = 0.4 # Weight for Centroid similarity in combined calculation (0.0 to 1.0)\n",
        "# Ensure WEIGHT_ROUGE + WEIGHT_CENTROID = 1.0 for normalized combined score\n",
        "if not np.isclose(WEIGHT_ROUGE + WEIGHT_CENTROID, 1.0):\n",
        "    print(\"Warning: WEIGHT_ROUGE and WEIGHT_CENTROID do not sum to 1.0. Normalizing...\")\n",
        "    total_weight = WEIGHT_ROUGE + WEIGHT_CENTROID\n",
        "    WEIGHT_ROUGE /= total_weight\n",
        "    WEIGHT_CENTROID /= total_weight\n",
        "    print(f\"Normalized weights: WEIGHT_ROUGE={WEIGHT_ROUGE:.2f}, WEIGHT_CENTROID={WEIGHT_CENTROID:.2f}\")\n",
        "\n",
        "\n",
        "# --- NLTK Resource Download and Tokenizer Initialization ---\n",
        "# This ensures 'punkt' is downloaded and the tokenizer is explicitly loaded once.\n",
        "punkt_tokenizer_instance = None\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    print(\"NLTK 'punkt' downloaded successfully.\")\n",
        "    # After download, the punkt tokenizer should be discoverable by PunktSentenceTokenizer\n",
        "    punkt_tokenizer_instance = PunktSentenceTokenizer()\n",
        "    print(\"Punkt tokenizer instance created successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading or initializing NLTK 'punkt' tokenizer: {e}\")\n",
        "    print(\"NLTK functions will likely fail. Please check internet connection and NLTK setup.\")\n",
        "    # Exit if NLTK setup fails, as the rest of the script depends on it.\n",
        "    exit()\n",
        "\n",
        "# --- 1. Load the existing govreport_raw dataset ---\n",
        "print(f\"Loading existing JSON files from: {GOVREPORT_RAW_DIR}\")\n",
        "try:\n",
        "    data_files = {\n",
        "        \"train\": os.path.join(GOVREPORT_RAW_DIR, \"train.json\"),\n",
        "        \"validation\": os.path.join(GOVREPORT_RAW_DIR, \"validation.json\"),\n",
        "        \"test\": os.path.join(GOVREPORT_RAW_DIR, \"test.json\"),\n",
        "    }\n",
        "\n",
        "    govreport_raw_data = load_dataset(\"json\", data_files=data_files)\n",
        "\n",
        "    print(\"\\nRaw GovReport data loaded successfully!\")\n",
        "    print(govreport_raw_data)\n",
        "except Exception as e:\n",
        "    print(f\"\\nError loading raw GovReport dataset from {GOVREPORT_RAW_DIR}: {e}\")\n",
        "    print(\"Please ensure the directory exists and contains 'train.json', 'validation.json', 'test.json'.\")\n",
        "    print(\"Exiting as the dataset could not be loaded.\")\n",
        "    exit()\n",
        "\n",
        "# --- Helper for ROUGE (needed for combined_extractive_summary) ---\n",
        "def postprocess_text_for_rouge(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [label.strip() for label in labels]\n",
        "    # For ROUGE, especially Lsum, it's good to have sentences separated by newlines\n",
        "    preds = [\"\\n\".join(punkt_tokenizer_instance.tokenize(pred)) for pred in preds]\n",
        "    labels = [\"\\n\".join(punkt_tokenizer_instance.tokenize(label)) for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "# --- 2. Combined Extractive Summary Function ---\n",
        "def combined_extractive_summary(document, reference_summary, num_sentences, weight_rouge, weight_centroid):\n",
        "    \"\"\"\n",
        "    Generates a single extractive summary by combining ROUGE-L F1 score with\n",
        "    TF-IDF centroid similarity for each sentence.\n",
        "    \"\"\"\n",
        "    sentences = punkt_tokenizer_instance.tokenize(document)\n",
        "\n",
        "    if not sentences or not reference_summary.strip():\n",
        "        return \"\"\n",
        "\n",
        "    # --- Calculate ROUGE-L F1 scores for each sentence ---\n",
        "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    sentence_rouge_scores = []\n",
        "    for sent_idx, sentence in enumerate(sentences):\n",
        "        processed_sent, processed_ref_summary = postprocess_text_for_rouge(\n",
        "            [sentence], [reference_summary]\n",
        "        )\n",
        "        score = rouge_scorer_instance.score(processed_ref_summary[0], processed_sent[0])['rougeL'].fmeasure\n",
        "        sentence_rouge_scores.append(score)\n",
        "\n",
        "    # --- Calculate TF-IDF Centroid similarities for each sentence ---\n",
        "    vectorizer = TfidfVectorizer().fit(sentences)\n",
        "    original_vectors = vectorizer.transform(sentences)\n",
        "\n",
        "    if original_vectors.shape[0] == 0 or np.all(original_vectors.toarray() == 0):\n",
        "        # If no valid vectors, return empty summary\n",
        "        return \"\"\n",
        "\n",
        "    document_centroid = np.mean(original_vectors.toarray(), axis=0)\n",
        "\n",
        "    if np.all(document_centroid == 0):\n",
        "        # If centroid is zero (e.g., all sentences were empty after tokenization), return empty summary\n",
        "        return \"\"\n",
        "\n",
        "    sentence_centroid_similarities = cosine_similarity(document_centroid.reshape(1, -1), original_vectors)[0]\n",
        "\n",
        "    # --- Combine scores and select top sentences ---\n",
        "    sentence_scores = [] # List of (original_index, combined_score, sentence_text)\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        # ROUGE F1 is already 0-1. Cosine similarity is also 0-1.\n",
        "        combined_score = (weight_rouge * sentence_rouge_scores[i]) + \\\n",
        "                         (weight_centroid * sentence_centroid_similarities[i])\n",
        "        sentence_scores.append((i, combined_score, sentence))\n",
        "\n",
        "    # Sort sentences by combined score in descending order\n",
        "    sentence_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Select the top N sentences\n",
        "    num_to_extract = min(num_sentences, len(sentence_scores))\n",
        "    selected_sentences_with_indices = sentence_scores[:num_to_extract]\n",
        "\n",
        "    # Sort these selected sentences by their original index to maintain document order\n",
        "    selected_sentences_with_indices.sort(key=lambda x: x[0])\n",
        "\n",
        "    combined_summary = \" \".join([s for _, _, s in selected_sentences_with_indices])\n",
        "    return combined_summary\n",
        "\n",
        "# --- 3. Main Processing Function to Generate the Combined Summary ---\n",
        "def generate_combined_extractive_summary_for_example(example):\n",
        "    \"\"\"\n",
        "    Generates a single combined extractive summary for a given example.\n",
        "    \"\"\"\n",
        "    original_text = example[\"document\"]\n",
        "    abstractive_summary = example[\"summary\"]\n",
        "\n",
        "    # Handle potential empty or non-string inputs\n",
        "    if not isinstance(original_text, str) or not original_text.strip() or \\\n",
        "       not isinstance(abstractive_summary, str) or not abstractive_summary.strip():\n",
        "        return {\n",
        "            \"original_text\": original_text,\n",
        "            \"extractive_summary_combined\": \"\",\n",
        "            \"abstractive_summary\": abstractive_summary\n",
        "        }\n",
        "\n",
        "    # Generate the combined extractive summary\n",
        "    extractive_combined = combined_extractive_summary(\n",
        "        original_text,\n",
        "        abstractive_summary,\n",
        "        num_sentences=NUM_COMBINED_SENTENCES,\n",
        "        weight_rouge=WEIGHT_ROUGE,\n",
        "        weight_centroid=WEIGHT_CENTROID\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"original_text\": original_text,\n",
        "        \"extractive_summary_combined\": extractive_combined,\n",
        "        \"abstractive_summary\": abstractive_summary\n",
        "    }\n",
        "\n",
        "# --- 4. Main Dataset Processing Loop ---\n",
        "processed_data = DatasetDict()\n",
        "\n",
        "for split_name in [\"train\", \"validation\", \"test\"]:\n",
        "    print(f\"\\nProcessing '{split_name}' split to generate combined extractive summaries...\")\n",
        "\n",
        "    current_split_data = []\n",
        "    # Manually iterate with tqdm to show progress bar\n",
        "    for example in tqdm(govreport_raw_data[split_name], desc=f\"Generating {split_name} combined summaries\"):\n",
        "        processed_example = generate_combined_extractive_summary_for_example(example)\n",
        "        current_split_data.append(processed_example)\n",
        "\n",
        "    # Convert the list of dictionaries to a Dataset object\n",
        "    processed_split = Dataset.from_list(current_split_data)\n",
        "    processed_data[split_name] = processed_split\n",
        "\n",
        "    print(f\"'{split_name}' split processed. New features: {processed_split.column_names}\")\n",
        "    print(f\"Sample from '{split_name}' split (first entry):\\n{processed_split[0]}\")\n",
        "\n",
        "    # Save the processed split to disk in JSON format\n",
        "    save_path = os.path.join(GENERATED_DATASET_DIR, f\"{split_name}.json\")\n",
        "    processed_split.to_json(save_path) # Save as JSONL by default for Datasets.to_json()\n",
        "    print(f\"'{split_name}' split saved to: {save_path}\")\n",
        "\n",
        "print(f\"\\nAll splits processed and saved to: {GENERATED_DATASET_DIR}\")\n",
        "print(\"\\nSummary of final dataset structure:\")\n",
        "print(processed_data)\n",
        "\n",
        "print(\"\\nTo load this generated dataset later, you can use:\")\n",
        "print(f\"from datasets import load_dataset\")\n",
        "print(f\"train_data = load_dataset('json', data_files='{GENERATED_DATASET_DIR}/train.json', split='train')\")\n",
        "print(f\"validation_data = load_dataset('json', data_files='{GENERATED_DATASET_DIR}/validation.json', split='validation')\")\n",
        "print(f\"test_data = load_dataset('json', data_files='{GENERATED_DATASET_DIR}/test.json', split='test')\")\n"
      ],
      "metadata": {
        "id": "F1ZFRi9byEnV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}