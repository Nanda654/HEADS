import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import (
    BartForConditionalGeneration,
    BartTokenizer,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    BeamSearchScorer
)
import spacy
import numpy as np
from typing import List, Dict, Tuple, Set, Optional


class ConstraintExtractor:
    """Extract factual constraints from key sentences."""

    def __init__(self):
        try:
            self.nlp = spacy.load("en_core_web_lg")
        except OSError:
            print("Downloading spaCy model 'en_core_web_lg'...")
            import os
            os.system("python -m spacy download en_core_web_lg")
            self.nlp = spacy.load("en_core_web_lg")


    def extract_constraints(self, key_sentences: List[str]) -> List[Dict]:
        """
        Extract named entities, dates, numbers, and other factual elements
        from key sentences to use as constraints.
        """
        constraints = []

        for sentence in key_sentences:
            doc = self.nlp(sentence)

            # Extract named entities
            entities = [(ent.text, ent.label_) for ent in doc.ents]

            # Extract numerical values
            numbers = []
            for token in doc:
                if token.like_num and not any(token.text in e[0] for e in entities):
                    numbers.append(token.text)

            # Extract key noun phrases
            noun_chunks = [chunk.text for chunk in doc.noun_chunks
                          if not any(chunk.text in e[0] for e in entities)]

            # Create constraint dictionary
            constraint = {
                "sentence": sentence,
                "entities": entities,
                "numbers": numbers,
                "noun_chunks": noun_chunks
            }

            constraints.append(constraint)

        return constraints

    def format_constraints_for_bart(self, constraints: List[Dict]) -> str:
        """Format constraints as a string to prepend to BART input."""
        formatted = "Important facts to include:\n"

        for i, constraint in enumerate(constraints):
            formatted += f"[Fact {i+1}] "

            # Add entities
            if constraint["entities"]:
                entities_str = ", ".join([f"{e[0]} ({e[1]})" for e in constraint["entities"]])
                formatted += f"Entities: {entities_str}. "

            # Add numbers
            if constraint["numbers"]:
                numbers_str = ", ".join(constraint["numbers"])
                formatted += f"Numbers: {numbers_str}. "

            # Add key phrases
            if constraint["noun_chunks"]:
                phrases_str = ", ".join(constraint["noun_chunks"][:3])  # Limit to top 3
                formatted += f"Key phrases: {phrases_str}."

            formatted += "\n"

        return formatted

    def get_constraint_tokens(self, constraints: List[Dict], tokenizer) -> Set[int]:
        """Extract important tokens from constraints to guide beam search."""
        important_words = set()

        for constraint in constraints:
            # Add entity texts
            for entity, _ in constraint["entities"]:
                important_words.add(entity.lower())
                # Add individual words from multi-word entities
                important_words.update(entity.lower().split())

            # Add numbers
            important_words.update([num.lower() for num in constraint["numbers"]])

            # Add key noun phrases
            for chunk in constraint["noun_chunks"]:
                important_words.add(chunk.lower())
                # Add individual words from chunks
                important_words.update(chunk.lower().split())

        # Convert words to token IDs
        token_ids = set()
        for word in important_words:
            tokens = tokenizer.encode(word, add_special_tokens=False)
            token_ids.update(tokens)

        return token_ids


class ConstrainedLogitsProcessor(nn.Module):
    """Logits processor that boosts the probability of constraint tokens."""

    def __init__(self, constraint_token_ids: Set[int], boost_factor: float = 2.0):
        super().__init__()
        self.constraint_token_ids = constraint_token_ids
        self.boost_factor = boost_factor

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
        """Boost scores for constraint tokens."""
        for token_id in self.constraint_token_ids:
            if token_id < scores.size(1):  # Ensure token ID is within vocabulary
                scores[:, token_id] *= self.boost_factor

        return scores


class ConstrainedBartSummarizer:
    """BART summarizer with constrained beam search."""

    def __init__(self, model_name: str = "facebook/bart-large-cnn"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer = BartTokenizer.from_pretrained(model_name)
        self.model = BartForConditionalGeneration.from_pretrained(model_name).to(self.device)
        self.constraint_extractor = ConstraintExtractor()

    def summarize_with_constraints(
        self,
        document: str,
        key_sentences: List[str],
        use_constraint_prefix: bool = True,
        use_constrained_beam_search: bool = True,
        num_beams: int = 4,
        min_length: int = 150,
        max_length: int = 350,
        boost_factor: float = 2.0,
        no_repeat_ngram_size: int = 3
    ) -> str:
        """Generate a summary using constraints from key sentences."""
        # Extract constraints from key sentences
        constraints = self.constraint_extractor.extract_constraints(key_sentences)

        # Prepare input text
        input_text = document
        if use_constraint_prefix:
            constraint_prefix = self.constraint_extractor.format_constraints_for_bart(constraints)
            input_text = constraint_prefix + "\n\nDocument: " + document

        # Tokenize input
        inputs = self.tokenizer(input_text, return_tensors="pt", max_length=1024, truncation=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        # Set up generation parameters
        generation_kwargs = {
            "num_beams": num_beams,
            "min_length": min_length,
            "max_length": max_length,
            "no_repeat_ngram_size": no_repeat_ngram_size,
            "early_stopping": True,
        }

        # Apply constrained beam search if requested
        if use_constrained_beam_search:
            print("Using constrained beam search in ConstrainedBartSummarizer...")
            constraint_token_ids = self.constraint_extractor.get_constraint_tokens(
                constraints, self.tokenizer
            )

            # Create logits processors
            logits_processor = LogitsProcessorList([
                MinLengthLogitsProcessor(min_length, self.tokenizer.eos_token_id),
                ConstrainedLogitsProcessor(constraint_token_ids, boost_factor)
            ])

            generation_kwargs["logits_processor"] = logits_processor
        else:
             print("Using standard beam search in ConstrainedBartSummarizer...")

        # Generate summary
        with torch.no_grad():
            output_ids = self.model.generate(**inputs, **generation_kwargs)

        # Decode and return summary
        summary = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
        return summary

    def custom_beam_search(
        self,
        input_ids: torch.LongTensor,
        attention_mask: torch.LongTensor,
        constraint_token_ids: Set[int],
        num_beams: int = 4,
        min_length: int = 50,
        max_length: int = 200,
        boost_factor: float = 2.0
    ) -> torch.LongTensor:
        """Custom implementation of constrained beam search."""
        batch_size = input_ids.shape[0]
        vocab_size = self.model.config.vocab_size

        # Initialize beam scorer
        beam_scorer = BeamSearchScorer(
            batch_size=batch_size,
            num_beams=num_beams,
            device=self.device,
        )

        # Create logits processors
        logits_processor = LogitsProcessorList([
            MinLengthLogitsProcessor(min_length, self.tokenizer.eos_token_id),
            ConstrainedLogitsProcessor(constraint_token_ids, boost_factor)
        ])

        # Expand input_ids and attention_mask for beam search
        input_ids = input_ids.repeat_interleave(num_beams, dim=0)
        attention_mask = attention_mask.repeat_interleave(num_beams, dim=0)

        # Initialize sequence scores
        beam_scores = torch.zeros(
            (batch_size, num_beams), dtype=torch.float, device=input_ids.device
        )
        beam_scores[:, 1:] = -1e9  # Initialize only first beam for each batch
        beam_scores = beam_scores.view(-1)  # (batch_size * num_beams)

        # Start token for decoder
        decoder_input_ids = torch.ones(
            (batch_size * num_beams, 1),
            dtype=torch.long,
            device=input_ids.device
        ) * self.model.config.decoder_start_token_id

        # Track generated sequences
        current_length = 1

        # Initialize for beam search
        encoder_outputs = self.model.get_encoder()(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )

        # Beam search loop
        while current_length < max_length:
            model_inputs = self.model.prepare_inputs_for_generation(
                decoder_input_ids,
                encoder_outputs=encoder_outputs,
                attention_mask=attention_mask,
            )

            outputs = self.model(**model_inputs, return_dict=True)
            next_token_logits = outputs.logits[:, -1, :]

            # Process logits
            next_token_scores = logits_processor(decoder_input_ids, next_token_logits)
            next_token_scores = F.log_softmax(next_token_scores, dim=-1)
            next_token_scores = next_token_scores + beam_scores[:, None]

            # Reshape scores for beam search
            vocab_size = next_token_scores.shape[-1]
            next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)

            # Get next tokens and scores
            next_token_scores, next_tokens = torch.topk(
                next_token_scores, 2 * num_beams, dim=1, largest=True, sorted=True
            )

            # Convert token indices
            next_indices = next_tokens // vocab_size
            next_tokens = next_tokens % vocab_size

            # Prepare next beam content
            beam_outputs = beam_scorer.process(
                decoder_input_ids,
                next_token_scores,
                next_tokens,
                next_indices,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

            beam_scores = beam_outputs["next_beam_scores"]
            beam_next_tokens = beam_outputs["next_beam_tokens"]
            beam_idx = beam_outputs["next_beam_indices"]

            # Update input_ids
            decoder_input_ids = torch.cat(
                [decoder_input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1
            )

            current_length += 1

            # Check if all beams are finished
            if beam_scorer.is_done:
                break

        # Finalize beam search
        sequence_outputs = beam_scorer.finalize(
            decoder_input_ids,
            beam_scores,
            next_tokens,
            next_indices,
            pad_token_id=self.tokenizer.pad_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
        )

        return sequence_outputs["sequences"]


# Example usage function
def generate_summary(document, key_sentences):
    """
    Generate a summary using the constrained BART model.

    Args:
        document: The full document text
        key_sentences: List of key sentences extracted by Longformer

    Returns:
        Generated summary
    """
    # Initialize the summarizer
    summarizer = ConstrainedBartSummarizer()

    # Generate summary with constraints
    summary = summarizer.summarize_with_constraints(
        document=document,
        key_sentences=key_sentences,
        use_constraint_prefix=True,
        use_constrained_beam_search=True
    )

    return summary


# Alternative implementation with direct constraint integration
class ConstraintGuidedBART(nn.Module):
    """BART model with direct constraint integration."""

    def __init__(self, model_name="facebook/bart-large-cnn"):
        super().__init__()
        self.model = BartForConditionalGeneration.from_pretrained(model_name)
        self.tokenizer = BartTokenizer.from_pretrained(model_name)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)

    def forward(self, input_ids, attention_mask, decoder_input_ids=None, labels=None):
        """Forward pass with standard BART."""
        return self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            decoder_input_ids=decoder_input_ids,
            labels=labels
        )

    def generate_with_constraints(
        self,
        input_ids,
        attention_mask,
        constraint_token_ids,
        num_beams=4,
        min_length=150,
        max_length=350,
        boost_factor=2.0
    ):
        """Generate text with constraint-guided beam search."""
        # Create logits processor with constraints
        logits_processor = LogitsProcessorList([
            MinLengthLogitsProcessor(min_length, self.tokenizer.eos_token_id),
            ConstrainedLogitsProcessor(constraint_token_ids, boost_factor)
        ])

        # Generate with constraints
        outputs = self.model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            num_beams=num_beams,
            min_length=min_length,
            max_length=max_length,
            logits_processor=logits_processor,
            early_stopping=True,
            no_repeat_ngram_size=3
        )

        return outputs

    def summarize(self, document, key_sentences):
        """Generate summary with constraints from key sentences."""
        # Extract constraints
        constraint_extractor = ConstraintExtractor()
        constraints = constraint_extractor.extract_constraints(key_sentences)
        constraint_token_ids = constraint_extractor.get_constraint_tokens(constraints, self.tokenizer)

        # Tokenize input
        inputs = self.tokenizer(document, return_tensors="pt", max_length=1024, truncation=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        # Generate summary
        print("Using constraint-guided beam search in ConstraintGuidedBART...")
        output_ids = self.generate_with_constraints(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            constraint_token_ids=constraint_token_ids
        )

        # Decode summary
        summary = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
        return summary


# Function to run the complete pipeline
def run_summarization_pipeline(document, extractive_key_sentences):
    """
    Run the complete summarization pipeline.

    Args:
        document: The full document text
        extractive_key_sentences: Key sentences extracted by Longformer

    Returns:
        Generated summary
    """
    # Method 1: Using ConstrainedBartSummarizer
    print("\n--- Running ConstrainedBartSummarizer ---")
    summarizer1 = ConstrainedBartSummarizer()
    summary1 = summarizer1.summarize_with_constraints(
        document=document,
        key_sentences=extractive_key_sentences,
        use_constraint_prefix=True,
        use_constrained_beam_search=True # Set to False to see standard beam search path
    )
    print("--- Finished ConstrainedBartSummarizer ---\n")

    # Method 2: Using ConstraintGuidedBART
    print("\n--- Running ConstraintGuidedBART ---")
    summarizer2 = ConstraintGuidedBART()
    summary2 = summarizer2.summarize(document, extractive_key_sentences)
    print("--- Finished ConstraintGuidedBART ---\n")


    return {
        "constrained_bart_summary": summary1,
        "constraint_guided_bart_summary": summary2
    }
