{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMv0wv6xXBbf2CoS9Yj9GXV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nanda654/HEADS/blob/main/beam_search_constrained_Bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hW0pDWgwm4Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Data\n",
        "\n",
        "input_text_for_bart_1 = \"\"\"FDA and USDA have responsibility for overseeing the safety of the food supply. General information about the process of making cell-cultured meat is available, but specific information about the technology being used and the eventual commercial production methods as well as the final products is not yet known. However, the technology and methods to commercially produce cell- cultured meat are still in development, and producers, regulators, and consumers do not yet have clarity on what these will entail. The composition of the final product is also not yet known. The general process for making cell-cultured meat contains five phases: biopsy, cell banking, growth, harvest, and food processing. The technology to produce cell-cultured meat at a commercial scale is still in development, and information about the methods to be used for commercial production and the composition of the final product are not yet known. Consequently, they have not finalized aspects of the technology and eventual commercial production methods to be used or the composition of the final product. As a result, certain information is not yet available to stakeholders\\u2014including cell-cultured meat firms themselves, regulators, and the public\\u2014about specific aspects of the technology and commercial production methods that will be used, such as the composition of the growth medium and of the final products. This lack of information results in unanswered questions about cell- cultured meat as it relates to the eventual technology and commercial production methods to be used and the composition of the final products. Some firms have developed prototypes of cell-cultured meat products as part of their research and development. In June 2019, FDA and USDA created three working groups to carry out the terms of the interagency agreement. FDA and USDA could more fully incorporate leading practices for collaboration in their interagency agreement and working groups. Developing and updating written guidance and agreements. However, the agreement does not describe how the agencies will track and monitor progress toward outcomes. Developing and updating written guidance and agreements. Developing and updating written guidance and agreements. Developing and updating written guidance and agreements. By more fully incorporating all seven leading practices for interagency collaboration early in the development of the three working groups, FDA and USDA could proactively minimize potential fragmentation and overlap in their oversight of cell-cultured meat, ensure consistency and efficient use of resources, and provide clarity to key stakeholders. While FDA and USDA officials told us they have decided who will oversee cell-cultured seafood, they have not formally announced or documented this decision, and some stakeholders have reported confusion or ambiguity about which agency will oversee cell-cultured seafood other than catfish. While FDA and USDA officials told us they had agreed that FDA would oversee cell-cultured seafood other than catfish, as of December 2019, the agencies had not formally announced or documented this agreement. Developing and updating written guidance and agreements is a leading practice for collaboration, as we have previously reported. Compounding this challenge is that specific information about key aspects of cell-cultured meat, such as the technology and production methods to be used as well as the composition of the products, is not yet known. FDA and USDA officials told us they have decided FDA will oversee most cell-cultured seafood, but the agencies have not formally documented this decision. We agree that the technology to produce cell-cultured meat is still in development and that information about the commercial production methods and composition of the final product are not yet known, as we state in our report. FDA concurred with two recommendations and partially concurred with one. USDA also concurred with two recommendations and partially concurred with one. FDA and USDA partially concurred with our recommendation, directed to each agency, to more fully incorporate the seven leading practices for effective collaboration into the agencies\\u2019 interagency agreement for the joint oversight of cell-cultured meat. We continue to believe that FDA and USDA should more fully incorporate the seven leading practices for effective collaboration into their interagency agreement for the joint oversight of cell-cultured meat. GAO staff who made key contributions to this report are listed in appendix V. Our report (1) describes what is known about methods for commercially producing cell-cultured meat and (2) examines the extent to which the Food and Drug Administration (FDA) and U.S. Department of Agriculture (USDA) are collaborating to provide regulatory oversight of cell-cultured meat. Developing and updating written guidance and agreements How will the collaborative mechanism be funded?\"\"\"\n",
        "constraints_list_1 = [\n",
        "\"FDA and USDA have responsibility for overseeing the safety of the food supply.\",\n",
        "    \"The Federal Food, Drug, and Cosmetic Act prohibits the misbranding of food, which includes food labeling that is false or misleading.\",\n",
        "    \"The technology and methods to commercially produce cell-cultured meat are still in development, and producers, regulators, and consumers do not yet have clarity on what these will entail.\",\n",
        "    \"The general process for making cell-cultured meat contains five phases: biopsy, cell banking, growth, harvest, and food processing.\",\n",
        "    \"FDA and USDA have established multiple mechanisms to collaborate on regulatory oversight of cell-cultured meat.\",\n",
        "    \"In June 2019, FDA and USDA created three working groups to carry out the terms of the interagency agreement.\",\n",
        "    \"FDA and USDA could more fully incorporate leading practices for collaboration in their interagency agreement and working groups.\",\n",
        "    \"FDA and USDA officials told us that FDA will have sole oversight responsibility for cell-cultured seafood other than catfish.\",\n",
        "    \"We are making a total of six recommendations, three to FDA and three to USDA.\",\n",
        "    \"The Commissioner of the Food and Drug Administration, in coordination with the Secretary of Agriculture, should more fully incorporate the seven leading practices for effective collaboration in the agencies' interagency agreement for the joint oversight of cell-cultured meat.\"\n",
        "]\n",
        "\n",
        "\n",
        "input_text_for_bart_2 = \"\"\"\n",
        "A new clinical trial conducted by the Global Health Institute tested a vaccine candidate for a rare tropical disease.\n",
        "The preliminary results showed that the vaccine was somewhat effective, but researchers have not yet released\n",
        "the exact efficacy rate. The trial involved 120 participants across three countries, but follow-up data is still pending.\n",
        "Some participants reported mild side effects, such as fever and fatigue.\n",
        "\n",
        "Experts caution that more research is needed before confirming the vaccineâ€™s effectiveness.\n",
        "So far, there is no published evidence on long-term immunity or whether the vaccine prevents severe cases.\n",
        "Despite this, early media reports have claimed that the vaccine is 'highly effective' and 'ready for use,'\n",
        "but researchers have not made any such statement.\n",
        "\n",
        "The World Health Organization (WHO) has acknowledged the trial but emphasized that\n",
        "official approval requires larger studies and peer-reviewed results.\n",
        "\"\"\"\n",
        "constraints_list_2 = [\"Global Health Institute\",   # factual source\n",
        "    \"120 participants\",          # exact trial size\n",
        "    \"mild side effects\",         # true effect\n",
        "    \"WHO\",                       # key organization mentioned\n",
        "    \"requires larger studies\"    # ensures cautious tone\n",
        "     ]\n",
        "\n",
        "abstractive_summary = \"\"\"Multiple firms have produced cell-cultured meat as part of their research and development. These products appear likely to become available to consumers in coming years. FDA and USDA are the primary agencies responsible for overseeing the safety of the nation's food supply. However, some stakeholders have expressed concern about the agencies' oversight of cell-cultured meat amidst a fragmented federal food safety oversight system. GAO was asked to review federal oversight of cell-cultured meat. This report (1) describes what is known about methods for commercially producing cell-cultured meat, and (2) examines the extent to which FDA and USDA are collaborating to provide regulatory oversight of cell-cultured meat. GAO conducted a literature review; reviewed documentation from FDA, USDA, and stakeholder groups; analyzed public comments submitted to the agencies; compared agency efforts with leading practices for interagency collaboration; and conducted site visits to selected cell-cultured meat firms. General information about the process of making cell-cultured meat\\u2014food products grown from the cells of livestock, poultry, and seafood\\u2014is available. However, no company is commercially producing cell-cultured meat. Specific information about the technology being used, eventual commercial production methods, and composition of the final products is not yet known. The general process contains five phases: biopsy, cell banking, growth, harvest, and food processing (see figure). The technology and methods to be used for commercial production are still in development, and producers, regulators, and consumers do not have clarity about many specifics about the process and final product. For example, it is unclear whether production methods and products will use or contain genetically-engineered cells or medications such as antibiotics. The Food and Drug Administration (FDA) and U.S. Department of Agriculture (USDA) have begun collaborating on regulatory oversight of cell-cultured meat. For example, in 2019, the agencies signed an interagency agreement and created three working groups to carry out the terms of the agreement. However, the agreement and working groups could more fully incorporate practices to enhance and sustain collaboration, such as defining outcomes. For example, the agreement identifies the development of labeling principles as an outcome, but does not describe how the agencies will track and monitor progress toward this outcome, and the working groups identify a lead agency but not members' roles. Also, agency officials said they decided FDA would oversee cell-cultured seafood other than catfish, but they have not formally announced or documented this decision. Developing and updating written guidance and agreements is also a leading practice for interagency collaboration. By fully incorporating leading practices into their efforts to collaborate, the agencies could minimize potential overlap and fragmentation, use resources in a more efficient manner, and better ensure the public and other key stakeholders have clarity about the agencies' oversight responsibilities. GAO recommends that FDA and USDA more fully incorporate leading practices for effective collaboration in the agencies' interagency agreement. FDA and USDA partially concurred and indicated a willingness to incorporate these practices in a more detailed agreement, which would also meet the intent of the recommendations. The agencies concurred with the four other recommendations.\"\"\""
      ],
      "metadata": {
        "id": "rI7hH0KDMEcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "PC6e-IPL-F8O",
        "outputId": "ab58a98b-7098-46a8-bf7e-ea7914808e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BART model and tokenizer for abstractive summarization...\n",
            "BART using device: cpu\n",
            "BART model loaded.\n",
            "spaCy model loaded.\n",
            "Starting BART Abstractive Summarization with Constrained Beam Search ---\n",
            "Extracting key facts from 2 constraint sentences...\n",
            "Extracted 1 key facts: ['AI']...\n",
            "Prepared 0 constraints for beam search\n",
            "No constraints provided or extracted. Using standard beam search.\n",
            "--- BART Abstractive Summarization with Constrained Beam Search Complete ---\n",
            "================================================================================\n",
            "Constrained Abstractive Summary:\n",
            "Artificial intelligence (AI) has rapidly become a transformative force across various industries. Concerns about algorithmic bias, data privacy, and job displacement are prompting calls for stronger regulations and ethical guidelines. The future of AI looks promising, with ongoing research into general artificial intelligence (AGI) and advanced human-computer interaction.\n"
          ]
        }
      ],
      "source": [
        "# @title 1\n",
        "import torch\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "# --- Load BART Model and Tokenizer ---\n",
        "print(\"Loading BART model and tokenizer for abstractive summarization...\")\n",
        "bart_model_name = 'facebook/bart-large-cnn'\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(bart_model_name)\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(bart_model_name)\n",
        "\n",
        "# --- Set Device (GPU if available, else CPU) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "bart_model.eval()\n",
        "bart_model.to(device)\n",
        "print(f\"BART using device: {device}\")\n",
        "print(\"BART model loaded.\")\n",
        "\n",
        "# Load spaCy for fact extraction\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy model loaded.\")\n",
        "except:\n",
        "    print(\"Installing spaCy model...\")\n",
        "    import os\n",
        "    os.system(\"python -m spacy download en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy model loaded.\")\n",
        "\n",
        "# --- Helper Functions for Constrained Beam Search ---\n",
        "\n",
        "def extract_key_facts(extractive_output, importance_threshold=0.7):\n",
        "    \"\"\"Extract key facts from the extractive summarization output\"\"\"\n",
        "    if isinstance(extractive_output, str):\n",
        "        # If input is a string, treat it as a single sentence with high importance\n",
        "        key_sentences = [extractive_output]\n",
        "    elif isinstance(extractive_output, list) and all(isinstance(item, str) for item in extractive_output):\n",
        "        # If input is a list of strings, use all sentences\n",
        "        key_sentences = extractive_output\n",
        "    else:\n",
        "        # If input is a list of (sentence, score) tuples\n",
        "        key_sentences = [sent for sent, score in extractive_output if score > importance_threshold]\n",
        "\n",
        "    # Process sentences to extract atomic facts\n",
        "    facts = []\n",
        "    for sentence in key_sentences:\n",
        "        # Simple approach: use key noun phrases and entities\n",
        "        doc = nlp(sentence)\n",
        "        for chunk in doc.noun_chunks:\n",
        "            if len(chunk.text.split()) > 1:  # Filter out very short phrases\n",
        "                facts.append(chunk.text)\n",
        "\n",
        "        # Add named entities\n",
        "        for ent in doc.ents:\n",
        "            facts.append(ent.text)\n",
        "\n",
        "    # Deduplicate facts\n",
        "    return list(set(facts))\n",
        "\n",
        "def prepare_constraints(facts, tokenizer):\n",
        "    \"\"\"Convert textual facts to token IDs for constraint checking\"\"\"\n",
        "    constraints = []\n",
        "    for fact in facts:\n",
        "        # Tokenize the fact\n",
        "        fact_tokens = tokenizer.encode(fact, add_special_tokens=False)\n",
        "\n",
        "        # Only use facts that aren't too long or too short\n",
        "        if 2 <= len(fact_tokens) <= 10:\n",
        "            constraints.append(fact_tokens)\n",
        "\n",
        "    return constraints\n",
        "\n",
        "def is_subsequence(smaller, larger):\n",
        "    \"\"\"Check if smaller list appears as a subsequence in larger list\"\"\"\n",
        "    i = j = 0\n",
        "    while i < len(smaller) and j < len(larger):\n",
        "        if smaller[i] == larger[j]:\n",
        "            i += 1\n",
        "        j += 1\n",
        "    return i == len(smaller)\n",
        "\n",
        "def check_constraints(sequence, constraints):\n",
        "    \"\"\"Check which constraints are satisfied by the current sequence\"\"\"\n",
        "    satisfied = []\n",
        "\n",
        "    for i, constraint in enumerate(constraints):\n",
        "        # Check if constraint tokens appear in sequence in the correct order\n",
        "        if is_subsequence(constraint, sequence):\n",
        "            satisfied.append(i)\n",
        "\n",
        "    return satisfied\n",
        "\n",
        "# --- Constrained Beam Search Implementation ---\n",
        "\n",
        "def constrained_beam_search(model, input_ids, attention_mask, constraints,\n",
        "                           num_beams=4, max_length=150, min_length=50,\n",
        "                           constraint_weight=2.0):\n",
        "    \"\"\"\n",
        "    Implements constrained beam search for BART summarization.\n",
        "\n",
        "    Args:\n",
        "        model: The BART model\n",
        "        input_ids: Tokenized input text\n",
        "        attention_mask: Attention mask for input\n",
        "        constraints: List of token sequences that should appear in the output\n",
        "        num_beams: Number of beams for beam search\n",
        "        max_length: Maximum length of the generated summary\n",
        "        min_length: Minimum length of the generated summary\n",
        "        constraint_weight: Weight given to satisfying constraints\n",
        "\n",
        "    Returns:\n",
        "        The generated summary that satisfies the most constraints\n",
        "    \"\"\"\n",
        "    # Get encoder output once\n",
        "    encoder_outputs = model.get_encoder()(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "\n",
        "    # Initialize beams: (tokens, log_prob, satisfied_constraints)\n",
        "    batch_size = input_ids.shape[0]\n",
        "    device = input_ids.device\n",
        "\n",
        "    # Start with the decoder start token\n",
        "    decoder_start_token_id = model.config.decoder_start_token_id\n",
        "    beams = [([decoder_start_token_id], 0.0, set()) for _ in range(num_beams)]\n",
        "\n",
        "    # Track completed sequences\n",
        "    done_beams = []\n",
        "\n",
        "    # Main beam search loop\n",
        "    for step in range(max_length):\n",
        "        all_candidates = []\n",
        "\n",
        "        # Check if all beams are done\n",
        "        if len(done_beams) == num_beams:\n",
        "            break\n",
        "\n",
        "        # Prepare current tokens for all beams\n",
        "        active_beams = [b for b in beams if b[0][-1] != model.config.eos_token_id]\n",
        "        if not active_beams:\n",
        "            break\n",
        "\n",
        "        current_tokens = [beam[0] for beam in active_beams]\n",
        "        max_len = max(len(tokens) for tokens in current_tokens)\n",
        "\n",
        "        # Pad and create tensor\n",
        "        padded_tokens = [tokens + [model.config.pad_token_id] * (max_len - len(tokens)) for tokens in current_tokens]\n",
        "        decoder_input = torch.tensor(padded_tokens, device=device)\n",
        "\n",
        "        # Get next token predictions\n",
        "        with torch.no_grad():\n",
        "            outputs = model.decoder(\n",
        "                input_ids=decoder_input,\n",
        "                encoder_hidden_states=encoder_outputs.last_hidden_state,\n",
        "                encoder_attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            logits = outputs[0]  # Get logits\n",
        "\n",
        "            # Process each beam\n",
        "            for beam_idx, (tokens, score, satisfied) in enumerate(active_beams):\n",
        "                # Get logits for the last token\n",
        "                curr_logits = logits[beam_idx, len(tokens)-1, :]\n",
        "\n",
        "                # Apply softmax to get probabilities\n",
        "                probs = torch.nn.functional.softmax(curr_logits, dim=-1)\n",
        "                log_probs = torch.log(probs + 1e-10)  # Add small epsilon to avoid log(0)\n",
        "\n",
        "                # Get top tokens\n",
        "                topk_log_probs, topk_indices = torch.topk(log_probs, num_beams * 2)\n",
        "\n",
        "                # Create new candidates\n",
        "                for log_prob, token_id in zip(topk_log_probs.tolist(), topk_indices.tolist()):\n",
        "                    new_tokens = tokens + [token_id]\n",
        "                    new_score = score + log_prob\n",
        "\n",
        "                    # Check which constraints are newly satisfied\n",
        "                    new_satisfied = set(satisfied)\n",
        "                    for i, constraint in enumerate(constraints):\n",
        "                        if i not in new_satisfied and is_subsequence(constraint, new_tokens):\n",
        "                            new_satisfied.add(i)\n",
        "\n",
        "                    # Apply constraint bonus\n",
        "                    constraint_bonus = len(new_satisfied) * constraint_weight\n",
        "                    adjusted_score = new_score + constraint_bonus\n",
        "\n",
        "                    # Add to candidates\n",
        "                    all_candidates.append((new_tokens, adjusted_score, new_satisfied))\n",
        "\n",
        "                    # Check if this is a completed sequence\n",
        "                    if token_id == model.config.eos_token_id and len(new_tokens) >= min_length:\n",
        "                        done_beams.append((new_tokens, adjusted_score, new_satisfied))\n",
        "\n",
        "        # Select top beams for next iteration\n",
        "        beams = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:num_beams]\n",
        "\n",
        "    # If we have completed sequences, return the best one\n",
        "    if done_beams:\n",
        "        # Sort by number of constraints satisfied, then by score\n",
        "        best_beam = max(done_beams, key=lambda x: (len(x[2]), x[1]))\n",
        "        return best_beam[0]\n",
        "\n",
        "    # If no sequence completed, return the best current beam\n",
        "    best_beam = max(beams, key=lambda x: (len(x[2]), x[1]))\n",
        "    return best_beam[0]\n",
        "\n",
        "# --- Abstractive Summarization with Constrained Beam Search ---\n",
        "\n",
        "def constrained_bart_summary(text_to_summarize, constraint_sentences=None,\n",
        "                            max_length=150, min_length=50, num_beams=4):\n",
        "    \"\"\"\n",
        "    Generates an abstractive summary using BART with constrained beam search.\n",
        "\n",
        "    Args:\n",
        "        text_to_summarize (str): The input text to summarize\n",
        "        constraint_sentences (list): List of sentences containing facts that must be included\n",
        "        max_length (int): Maximum length of the generated summary\n",
        "        min_length (int): Minimum length of the generated summary\n",
        "        num_beams (int): Number of beams for beam search\n",
        "\n",
        "    Returns:\n",
        "        str: The generated abstractive summary that includes the key facts\n",
        "    \"\"\"\n",
        "    print(\"Starting BART Abstractive Summarization with Constrained Beam Search ---\")\n",
        "\n",
        "    if isinstance(text_to_summarize, list):\n",
        "        text_to_summarize = \" \".join(text_to_summarize)\n",
        "\n",
        "    if not text_to_summarize.strip():\n",
        "        print(\"  Input text for abstractive summary is empty. Cannot summarize.\")\n",
        "        return \"\"\n",
        "\n",
        "    # Process input text\n",
        "    inputs = bart_tokenizer(\n",
        "        [text_to_summarize],\n",
        "        max_length=1024,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    ).to(device)\n",
        "\n",
        "    # Extract and prepare constraints if provided\n",
        "    constraints = []\n",
        "    if constraint_sentences:\n",
        "        print(f\"Extracting key facts from {len(constraint_sentences)} constraint sentences...\")\n",
        "        facts = extract_key_facts(constraint_sentences)\n",
        "        print(f\"Extracted {len(facts)} key facts: {facts[:5]}...\")\n",
        "        constraints = prepare_constraints(facts, bart_tokenizer)\n",
        "        print(f\"Prepared {len(constraints)} constraints for beam search\")\n",
        "\n",
        "    # If no constraints or constraint extraction failed, fall back to standard beam search\n",
        "    if not constraints:\n",
        "        print(\"No constraints provided or extracted. Using standard beam search.\")\n",
        "        summary_ids = bart_model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            num_beams=num_beams,\n",
        "            max_length=max_length,\n",
        "            min_length=min_length,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        summary_text = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    else:\n",
        "        # Use constrained beam search\n",
        "        print(\"Using constrained beam search with extracted facts...\")\n",
        "        output_ids = constrained_beam_search(\n",
        "            model=bart_model,\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            constraints=constraints,\n",
        "            num_beams=num_beams,\n",
        "            max_length=max_length,\n",
        "            min_length=min_length\n",
        "        )\n",
        "        summary_text = bart_tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "\n",
        "    print(\"--- BART Abstractive Summarization with Constrained Beam Search Complete ---\")\n",
        "    return summary_text\n",
        "\n",
        "# --- Example Usage ---\n",
        "# This is where you would input your text and constraint sentences\n",
        "\n",
        "# Example:\n",
        "\n",
        "input_text = \"\"\"Artificial intelligence (AI) has rapidly become a transformative force across various industries. In healthcare, AI systems assist doctors by analyzing medical images, predicting patient risks, and streamlining administrative tasks through automated electronic health records. Hospitals are increasingly relying on AI tools to optimize patient scheduling and improve diagnostic accuracy. In finance, AI-driven algorithms power fraud detection systems, assess credit risk, and support robo-advisors that provide tailored investment advice based on individual financial goals and risk tolerance.\n",
        "\n",
        "AI is also playing a crucial role in transportation. Self-driving cars and traffic optimization systems use vast amounts of data to reduce accidents and improve traffic flow in urban areas. Meanwhile, the education sector is leveraging AI-powered personalized learning platforms that adapt to studentsâ€™ strengths and weaknesses, enhancing engagement and learning outcomes.\n",
        "\n",
        "However, the rise of AI comes with challenges. Concerns about algorithmic bias, data privacy, and job displacement are prompting calls for stronger regulations and ethical guidelines. Privacy breaches can occur when sensitive personal data is mishandled by AI systems, while automation threatens certain repetitive or low-skilled jobs.\n",
        "\n",
        "The future of AI looks promising, with ongoing research into general artificial intelligence (AGI) and advanced human-computer interaction. Smarter cities, more efficient energy management, and breakthroughs in medicine are all on the horizon. To ensure AI serves humanityâ€™s best interests, governments, companies, and researchers must engage in continuous public discourse, adapt regulations, and focus on ethical deployment of this powerful technology.\"\"\"\n",
        "\n",
        "# These are the factual and relationship constraint sentences\n",
        "constraint_sentences = [ \"Strength.\",\"AI\"]\n",
        "\n",
        "summary = constrained_bart_summary(\n",
        "    input_text,\n",
        "    constraint_sentences=constraint_sentences,\n",
        "    max_length=150,\n",
        "    min_length=50,\n",
        "    num_beams=4\n",
        ")\n",
        "\n",
        "print(\"\" + \"=\"*80)\n",
        "print(\"Constrained Abstractive Summary:\")\n",
        "print(summary)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "bart_model_name = 'facebook/bart-large-cnn'\n",
        "tokenizer = BartTokenizer.from_pretrained(bart_model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(bart_model_name).eval()\n",
        "\n",
        "text = \"\"\"Artificial intelligence (AI) has rapidly become a transformative force across various industries.\n",
        "In healthcare, AI systems assist doctors by analyzing medical images, predicting patient risks, and streamlining administrative tasks\n",
        "through automated electronic health records. Hospitals are increasingly relying on AI tools to optimize patient scheduling and improve\n",
        "diagnostic accuracy. In finance, AI-driven algorithms power fraud detection systems, assess credit risk, and support robo-advisors\n",
        "that provide tailored investment advice based on individual financial goals and risk tolerance.\n",
        "\n",
        "AI is also playing a crucial role in transportation. Self-driving cars and traffic optimization systems use vast amounts of data\n",
        "to reduce accidents and improve traffic flow in urban areas. Meanwhile, the education sector is leveraging AI-powered personalized\n",
        "learning platforms that adapt to studentsâ€™ strengths and weaknesses, enhancing engagement and learning outcomes.\n",
        "\n",
        "However, the rise of AI comes with challenges. Concerns about algorithmic bias, data privacy, and job displacement are prompting\n",
        "calls for stronger regulations and ethical guidelines. Privacy breaches can occur when sensitive personal data is mishandled by\n",
        "AI systems, while automation threatens certain repetitive or low-skilled jobs.\n",
        "\n",
        "The future of AI looks promising, with ongoing research into general artificial intelligence (AGI) and advanced human-computer\n",
        "interaction. Smarter cities, more efficient energy management, and breakthroughs in medicine are all on the horizon. To ensure\n",
        "AI serves humanityâ€™s best interests, governments, companies, and researchers must engage in continuous public discourse, adapt\n",
        "regulations, and focus on ethical deployment of this powerful technology.\n",
        "\"\"\"\n",
        "\n",
        "# --- Constraint words ---\n",
        "constraint_words = [\"Education\", \"AI\"]\n",
        "force_words_ids = [tokenizer([w], add_special_tokens=False).input_ids[0] for w in constraint_words]\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "\n",
        "summary_ids = model.generate(\n",
        "    **inputs,\n",
        "    max_length=150,\n",
        "    min_length=50,\n",
        "    num_beams=5,\n",
        "    force_words_ids=force_words_ids,\n",
        "    no_repeat_ngram_size=3,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(\"Constrained Abstractive Summary:\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "w_EV1PJS-XkR",
        "outputId": "a246ed01-61c7-4a65-e3ca-b12ae462b860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Constrained Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. To prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constrained Abstractive Summary:\n",
            "Artificial intelligence (AI) has rapidly become a transformative force across various industries. Concerns about algorithmic bias, data privacy, and job displacement are prompting calls for stronger regulations and ethical guidelines. The future of AI looks promising, with ongoing research into general artificial intelligence (AGI) and advanced human-computer interaction. Smarter cities, more efficient energy management, and breakthroughs in medicine are all on the horizon. To ensure humanityâ€™s best interests, governments, companies, and researchers must engage in continuous public discourse, adapt Guidelines, and focus on ethical deployment of this powerful technology. The Future of AI is here. It's time to embrace it. It is time to start using it. To learn more about theEducation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3\n",
        "\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, PhrasalConstraint\n",
        "import torch\n",
        "\n",
        "# Load model and tokenizer\n",
        "bart_model_name = 'facebook/bart-large-cnn'\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(bart_model_name)\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(bart_model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "bart_model.to(device)\n",
        "bart_model.eval()\n",
        "print(f\"BART loaded on {device}\")\n",
        "\n",
        "# --- Constrained Beam Search Function ---\n",
        "def bart_constrained_summary(\n",
        "    text_to_summarize,\n",
        "    constraints_list,\n",
        "    max_length=150,\n",
        "    min_length=50,\n",
        "    num_beams=4,\n",
        "    early_stopping=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates an abstractive summary using BART with Constrained Beam Search.\n",
        "\n",
        "    Args:\n",
        "        text_to_summarize (str or list of str): Input document or list of sentences.\n",
        "        constraints_list (list of str): List of key factual phrases that must appear.\n",
        "        max_length (int): Max length of generated summary.\n",
        "        min_length (int): Min length of generated summary.\n",
        "        num_beams (int): Beam size for beam search.\n",
        "        early_stopping (bool): Stop early when all beams finish.\n",
        "\n",
        "    Returns:\n",
        "        str: Constrained abstractive summary.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting BART Constrained Abstractive Summarization ---\")\n",
        "\n",
        "    if isinstance(text_to_summarize, list):\n",
        "        text_to_summarize = \" \".join(text_to_summarize)\n",
        "\n",
        "    if not text_to_summarize.strip():\n",
        "        print(\"  Input text is empty. Cannot summarize.\")\n",
        "        return \"\"\n",
        "\n",
        "    # Encode input\n",
        "    inputs = bart_tokenizer(\n",
        "        [text_to_summarize],\n",
        "        max_length=1024,  # BART input limit\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True\n",
        "    ).to(device)\n",
        "\n",
        "    # Create Hugging Face constraints\n",
        "    constraints = []\n",
        "    for phrase in constraints_list:\n",
        "        phrase_ids = bart_tokenizer(phrase, add_special_tokens=False).input_ids\n",
        "        constraints.append(PhrasalConstraint(phrase_ids))\n",
        "\n",
        "    # Generate summary with constrained beam search\n",
        "    summary_ids = bart_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        num_beams=num_beams,\n",
        "        min_length=min_length,\n",
        "        max_length=max_length,\n",
        "        early_stopping=early_stopping,\n",
        "        constraints=constraints  # <-- Key step for constrained beam search\n",
        "    )\n",
        "\n",
        "    summary_text = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    print(\"--- BART Constrained Summarization Complete ---\")\n",
        "    return summary_text\n",
        "\n",
        "# Example factual constraints from Bridging Mechanism\n",
        "\n",
        "\n",
        "\n",
        "constrained_summary = bart_constrained_summary(\n",
        "    input_text_for_bart_2,\n",
        "    constraints_list_2,\n",
        "    max_length=150,\n",
        "    min_length=50,\n",
        "    num_beams=5\n",
        ")\n",
        "print(\"\\nConstrained Beam Search Summary:\\n\", constrained_summary)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHlYIkgWDVKh",
        "outputId": "deea6ecf-d981-4df1-f9a7-e279e2c38b22",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BART loaded on cpu\n",
            "\n",
            "--- Starting BART Constrained Abstractive Summarization ---\n",
            "--- BART Constrained Summarization Complete ---\n",
            "\n",
            "Constrained Beam Search Summary:\n",
            " Global Health InstituteWHOrequires larger studies before approval120 participants across three countries. No published evidence on long-term immunity or whether the vaccine prevents severe cases. Some participants reported mild side effects, such as fever and fatigue. The vaccine is not yet ready for use. The trial is still ongoing. The World Health Organization has acknowledged the trial but emphasized that official approval requires larger studies and peer-reviewed results. The study is still in its early stages. The results are still pending. The clinical trial is being funded by the Global Health Institute. The full results of the trial are expected in 2015. The WHO has acknowledged that the trial is underway but emphasized the need for larger studies. The research is being financed by themild side effects\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# bart1"
      ],
      "metadata": {
        "id": "LTfN9xhF0P0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    BartForConditionalGeneration,\n",
        "    BartTokenizer,\n",
        "    LogitsProcessorList,\n",
        "    MinLengthLogitsProcessor,\n",
        "    BeamSearchScorer,\n",
        ")\n",
        "import spacy\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Set, Optional\n"
      ],
      "metadata": {
        "id": "phyiGRzo0YG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConstraintExtractor:\n",
        "    \"\"\"Extract factual constraints from key sentences.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "    def extract_constraints(self, key_sentences: List[str]) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Extract named entities, dates, numbers, and other factual elements\n",
        "        from key sentences to use as constraints.\n",
        "\n",
        "        Args:\n",
        "            key_sentences: List of sentences selected by the extractive component\n",
        "\n",
        "        Returns:\n",
        "            List of constraint dictionaries\n",
        "        \"\"\"\n",
        "        constraints = []\n",
        "\n",
        "        for sentence in key_sentences:\n",
        "            doc = self.nlp(sentence)\n",
        "\n",
        "            # Extract named entities\n",
        "            entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "            # Extract numerical values\n",
        "            numbers = []\n",
        "            for token in doc:\n",
        "                if token.like_num and not any(token.text in e[0] for e in entities):\n",
        "                    numbers.append(token.text)\n",
        "\n",
        "            # Extract key noun phrases\n",
        "            noun_chunks = [chunk.text for chunk in doc.noun_chunks\n",
        "                          if not any(chunk.text in e[0] for e in entities)]\n",
        "\n",
        "            # Create constraint dictionary\n",
        "            constraint = {\n",
        "                \"sentence\": sentence,\n",
        "                \"entities\": entities,\n",
        "                \"numbers\": numbers,\n",
        "                \"noun_chunks\": noun_chunks\n",
        "            }\n",
        "\n",
        "            constraints.append(constraint)\n",
        "\n",
        "        return constraints\n",
        "\n",
        "    def format_constraints_for_bart(self, constraints: List[Dict]) -> str:\n",
        "        \"\"\"\n",
        "        Format constraints as a string to prepend to BART input.\n",
        "\n",
        "        Args:\n",
        "            constraints: List of constraint dictionaries\n",
        "\n",
        "        Returns:\n",
        "            Formatted constraint string\n",
        "        \"\"\"\n",
        "        formatted = \"Important facts to include:\\n\"\n",
        "\n",
        "        for i, constraint in enumerate(constraints):\n",
        "            formatted += f\"[Fact {i+1}] \"\n",
        "\n",
        "            # Add entities\n",
        "            if constraint[\"entities\"]:\n",
        "                entities_str = \", \".join([f\"{e[0]} ({e[1]})\" for e in constraint[\"entities\"]])\n",
        "                formatted += f\"Entities: {entities_str}. \"\n",
        "\n",
        "            # Add numbers\n",
        "            if constraint[\"numbers\"]:\n",
        "                numbers_str = \", \".join(constraint[\"numbers\"])\n",
        "                formatted += f\"Numbers: {numbers_str}. \"\n",
        "\n",
        "            # Add key phrases\n",
        "            if constraint[\"noun_chunks\"]:\n",
        "                phrases_str = \", \".join(constraint[\"noun_chunks\"][:3])  # Limit to top 3\n",
        "                formatted += f\"Key phrases: {phrases_str}.\"\n",
        "\n",
        "            formatted += \"\\n\"\n",
        "\n",
        "        return formatted\n",
        "\n",
        "    def get_constraint_tokens(self, constraints: List[Dict], tokenizer) -> Set[int]:\n",
        "        \"\"\"\n",
        "        Extract important tokens from constraints to guide beam search.\n",
        "\n",
        "        Args:\n",
        "            constraints: List of constraint dictionaries\n",
        "            tokenizer: BART tokenizer\n",
        "\n",
        "        Returns:\n",
        "            Set of token IDs that should be preferred during generation\n",
        "        \"\"\"\n",
        "        important_words = set()\n",
        "\n",
        "        for constraint in constraints:\n",
        "            # Add entity texts\n",
        "            for entity, _ in constraint[\"entities\"]:\n",
        "                important_words.add(entity.lower())\n",
        "                # Add individual words from multi-word entities\n",
        "                important_words.update(entity.lower().split())\n",
        "\n",
        "            # Add numbers\n",
        "            important_words.update([num.lower() for num in constraint[\"numbers\"]])\n",
        "\n",
        "            # Add key noun phrases\n",
        "            for chunk in constraint[\"noun_chunks\"]:\n",
        "                important_words.add(chunk.lower())\n",
        "                # Add individual words from chunks\n",
        "                important_words.update(chunk.lower().split())\n",
        "\n",
        "        # Convert words to token IDs\n",
        "        token_ids = set()\n",
        "        for word in important_words:\n",
        "            tokens = tokenizer.encode(word, add_special_tokens=False)\n",
        "            token_ids.update(tokens)\n",
        "\n",
        "        return token_ids"
      ],
      "metadata": {
        "id": "rZzvJ77D0YJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConstrainedLogitsProcessor(nn.Module):\n",
        "    \"\"\"\n",
        "    Logits processor that boosts the probability of constraint tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, constraint_token_ids: Set[int], boost_factor: float = 2.0):\n",
        "        super().__init__()\n",
        "        self.constraint_token_ids = constraint_token_ids\n",
        "        self.boost_factor = boost_factor\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        \"\"\"\n",
        "        Boost scores for constraint tokens.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Current sequence of tokens\n",
        "            scores: Scores for next token prediction\n",
        "\n",
        "        Returns:\n",
        "            Modified scores with boosted probabilities for constraint tokens\n",
        "        \"\"\"\n",
        "        for token_id in self.constraint_token_ids:\n",
        "            if token_id < scores.size(1):  # Ensure token ID is within vocabulary\n",
        "                scores[:, token_id] *= self.boost_factor\n",
        "\n",
        "        return scores\n",
        "\n",
        "\n",
        "class ConstrainedBartSummarizer:\n",
        "    \"\"\"\n",
        "    BART summarizer with constrained beam search.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"facebook/bart-large-cnn\"):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "        self.model = BartForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
        "        self.constraint_extractor = ConstraintExtractor()\n",
        "\n",
        "    def summarize_with_constraints(\n",
        "        self,\n",
        "        document: str,\n",
        "        key_sentences: List[str],\n",
        "        use_constraint_prefix: bool = True,\n",
        "        use_constrained_beam_search: bool = True,\n",
        "        num_beams: int = 4,\n",
        "        min_length: int = 50,\n",
        "        max_length: int = 200,\n",
        "        boost_factor: float = 2.0,\n",
        "        no_repeat_ngram_size: int = 3\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Generate a summary using constraints from key sentences.\n",
        "\n",
        "        Args:\n",
        "            document: Full document text\n",
        "            key_sentences: List of key sentences from extractive component\n",
        "            use_constraint_prefix: Whether to prepend constraints to input\n",
        "            use_constrained_beam_search: Whether to use constrained beam search\n",
        "            num_beams: Number of beams for beam search\n",
        "            min_length: Minimum length of generated summary\n",
        "            max_length: Maximum length of generated summary\n",
        "            boost_factor: Factor to boost constraint token probabilities\n",
        "            no_repeat_ngram_size: Size of n-grams to avoid repeating\n",
        "\n",
        "        Returns:\n",
        "            Generated summary\n",
        "        \"\"\"\n",
        "        # Extract constraints from key sentences\n",
        "        constraints = self.constraint_extractor.extract_constraints(key_sentences)\n",
        "\n",
        "        # Prepare input text\n",
        "        input_text = document\n",
        "        if use_constraint_prefix:\n",
        "            constraint_prefix = self.constraint_extractor.format_constraints_for_bart(constraints)\n",
        "            input_text = constraint_prefix + \"\\n\\nDocument: \" + document\n",
        "\n",
        "        # Tokenize input\n",
        "        inputs = self.tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        # Set up generation parameters\n",
        "        generation_kwargs = {\n",
        "            \"num_beams\": num_beams,\n",
        "            \"min_length\": min_length,\n",
        "            \"max_length\": max_length,\n",
        "            \"no_repeat_ngram_size\": no_repeat_ngram_size,\n",
        "            \"early_stopping\": True,\n",
        "        }\n",
        "\n",
        "        # Apply constrained beam search if requested\n",
        "        if use_constrained_beam_search:\n",
        "            constraint_token_ids = self.constraint_extractor.get_constraint_tokens(\n",
        "                constraints, self.tokenizer\n",
        "            )\n",
        "\n",
        "            # Create logits processors\n",
        "            logits_processor = LogitsProcessorList([\n",
        "                MinLengthLogitsProcessor(min_length, self.tokenizer.eos_token_id),\n",
        "                ConstrainedLogitsProcessor(constraint_token_ids, boost_factor)\n",
        "            ])\n",
        "\n",
        "            generation_kwargs[\"logits_processor\"] = logits_processor\n",
        "\n",
        "        # Generate summary\n",
        "        with torch.no_grad():\n",
        "            output_ids = self.model.generate(**inputs, **generation_kwargs)\n",
        "\n",
        "        # Decode and return summary\n",
        "        summary = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        return summary\n",
        "\n",
        "    def custom_beam_search(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "        attention_mask: torch.LongTensor,\n",
        "        constraint_token_ids: Set[int],\n",
        "        num_beams: int = 4,\n",
        "        min_length: int = 50,\n",
        "        max_length: int = 200,\n",
        "        boost_factor: float = 2.0\n",
        "    ) -> torch.LongTensor:\n",
        "        \"\"\"\n",
        "        Custom implementation of constrained beam search.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Input token IDs\n",
        "            attention_mask: Attention mask\n",
        "            constraint_token_ids: Set of token IDs to prefer\n",
        "            num_beams: Number of beams\n",
        "            min_length: Minimum length of generated sequence\n",
        "            max_length: Maximum length of generated sequence\n",
        "            boost_factor: Factor to boost constraint token probabilities\n",
        "\n",
        "        Returns:\n",
        "            Generated token IDs\n",
        "        \"\"\"\n",
        "        batch_size = input_ids.shape[0]\n",
        "        vocab_size = self.model.config.vocab_size\n",
        "\n",
        "        # Initialize beam scorer\n",
        "        beam_scorer = BeamSearchScorer(\n",
        "            batch_size=batch_size,\n",
        "            num_beams=num_beams,\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "        # Create logits processors\n",
        "        logits_processor = LogitsProcessorList([\n",
        "            MinLengthLogitsProcessor(min_length, self.tokenizer.eos_token_id),\n",
        "            ConstrainedLogitsProcessor(constraint_token_ids, boost_factor)\n",
        "        ])\n",
        "\n",
        "        # Expand input_ids and attention_mask for beam search\n",
        "        input_ids = input_ids.repeat_interleave(num_beams, dim=0)\n",
        "        attention_mask = attention_mask.repeat_interleave(num_beams, dim=0)\n",
        "\n",
        "        # Initialize sequence scores\n",
        "        beam_scores = torch.zeros(\n",
        "            (batch_size, num_beams), dtype=torch.float, device=input_ids.device\n",
        "        )\n",
        "        beam_scores[:, 1:] = -1e9  # Initialize only first beam for each batch\n",
        "        beam_scores = beam_scores.view(-1)  # (batch_size * num_beams)\n",
        "\n",
        "        # Start token for decoder\n",
        "        decoder_input_ids = torch.ones(\n",
        "            (batch_size * num_beams, 1),\n",
        "            dtype=torch.long,\n",
        "            device=input_ids.device\n",
        "        ) * self.model.config.decoder_start_token_id\n",
        "\n",
        "        # Track generated sequences\n",
        "        current_length = 1\n",
        "\n",
        "        # Initialize for beam search\n",
        "        encoder_outputs = self.model.get_encoder()(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        # Beam search loop\n",
        "        while current_length < max_length:\n",
        "            model_inputs = self.model.prepare_inputs_for_generation(\n",
        "                decoder_input_ids,\n",
        "                encoder_outputs=encoder_outputs,\n",
        "                attention_mask=attention_mask,\n",
        "            )\n",
        "\n",
        "            outputs = self.model(**model_inputs, return_dict=True)\n",
        "            next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            # Process logits\n",
        "            next_token_scores = logits_processor(decoder_input_ids, next_token_logits)\n",
        "            next_token_scores = F.log_softmax(next_token_scores, dim=-1)\n",
        "            next_token_scores = next_token_scores + beam_scores[:, None]\n",
        "\n",
        "            # Reshape scores for beam search\n",
        "            vocab_size = next_token_scores.shape[-1]\n",
        "            next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n",
        "\n",
        "            # Get next tokens and scores\n",
        "            next_token_scores, next_tokens = torch.topk(\n",
        "                next_token_scores, 2 * num_beams, dim=1, largest=True, sorted=True\n",
        "            )\n",
        "\n",
        "            # Convert token indices\n",
        "            next_indices = next_tokens // vocab_size\n",
        "            next_tokens = next_tokens % vocab_size\n",
        "\n",
        "            # Prepare next beam content\n",
        "            beam_outputs = beam_scorer.process(\n",
        "                decoder_input_ids,\n",
        "                next_token_scores,\n",
        "                next_tokens,\n",
        "                next_indices,\n",
        "                pad_token_id=self.tokenizer.pad_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "            beam_scores = beam_outputs[\"next_beam_scores\"]\n",
        "            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
        "            beam_idx = beam_outputs[\"next_beam_indices\"]\n",
        "\n",
        "            # Update input_ids\n",
        "            decoder_input_ids = torch.cat(\n",
        "                [decoder_input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1\n",
        "            )\n",
        "\n",
        "            current_length += 1\n",
        "\n",
        "            # Check if all beams are finished\n",
        "            if beam_scorer.is_done:\n",
        "                break\n",
        "\n",
        "        # Finalize beam search\n",
        "        sequence_outputs = beam_scorer.finalize(\n",
        "            decoder_input_ids,\n",
        "            beam_scores,\n",
        "            next_tokens,\n",
        "            next_indices,\n",
        "            pad_token_id=self.tokenizer.pad_token_id,\n",
        "            eos_token_id=self.tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "        return sequence_outputs[\"sequences\"]"
      ],
      "metadata": {
        "id": "Ljx-ud020YMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t-0wcmkR0YPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bart\n"
      ],
      "metadata": {
        "id": "jzWv7Mx5Fod_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Bart\n",
        "\n",
        "# Install necessary libraries (only runs if not already installed)\n",
        "#%pip install transformers torch\n",
        "\n",
        "import torch\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "# --- Load BART Model and Tokenizer ---\n",
        "print(\"Loading BART model and tokenizer for abstractive summarization...\")\n",
        "bart_model_name = 'facebook/bart-large-cnn' # This is a good choice for summarization\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(bart_model_name)\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(bart_model_name)\n",
        "\n",
        "# --- Set Device (GPU if available, else CPU) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "bart_model.eval() # Set to evaluation mode\n",
        "bart_model.to(device)\n",
        "print(f\"BART using device: {device}\")\n",
        "print(\"BART model loaded.\")\n",
        "\n",
        "# --- Abstractive Summarization Function (using BART) ---\n",
        "def bart_abstractive_summary(text_to_summarize, max_length, min_length, num_beams, early_stopping=True):\n",
        "    \"\"\"\n",
        "    Generates an abstractive summary using the pre-loaded BART model.\n",
        "    Assumes bart_tokenizer and bart_model are loaded globally.\n",
        "\n",
        "    Args:\n",
        "        text_to_summarize (str or list of str): The input text (or list of sentences) to summarize.\n",
        "                                                  If a list, it will be joined into a single string.\n",
        "        max_length (int): Maximum length of the generated summary.\n",
        "        min_length (int): Minimum length of the generated summary.\n",
        "        num_beams (int): Number of beams for beam search. Higher values lead to better quality but slower generation.\n",
        "        early_stopping (bool): Whether to stop beam search when all beams have finished their generation.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated abstractive summary.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting BART Abstractive Summarization ---\")\n",
        "\n",
        "    if isinstance(text_to_summarize, list):\n",
        "        text_to_summarize = \" \".join(text_to_summarize)\n",
        "\n",
        "    if not text_to_summarize.strip():\n",
        "        print(\"  Input text for abstractive summary is empty. Cannot summarize.\")\n",
        "        return \"\"\n",
        "\n",
        "    inputs = bart_tokenizer( # bart_tokenizer is now accessible globally\n",
        "        [text_to_summarize],\n",
        "        max_length=1024, # BART's typical max input length\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    ).to(device) # device is also globally accessible\n",
        "\n",
        "    summary_ids = bart_model.generate( # bart_model is now accessible globally\n",
        "        inputs[\"input_ids\"],\n",
        "        num_beams=num_beams,\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        early_stopping=early_stopping\n",
        "    )\n",
        "\n",
        "    summary_text = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    print(\"--- BART Abstractive Summarization Complete ---\")\n",
        "    return summary_text\n",
        "\n",
        "# --- Example Usage for BART only ---\n",
        "#input_text_for_bart = \" \".join(combined_summary)\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Abstractive Summary (using BART directly on the full text):\")\n",
        "bart_only_summary = bart_abstractive_summary(\n",
        "    input_text_for_bart_1,\n",
        "    max_length=500, # Max length of the final abstractive summary\n",
        "    min_length=400,  # Min length of the final abstractive summary\n",
        "    num_beams=5     # Beam search parameter for quality\n",
        ")\n",
        "print(bart_only_summary)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\\nBART only summarization complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d87vdAFv7EEv",
        "outputId": "4c4e4535-b130-45d0-8427-9fb29b3e7e9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BART model and tokenizer for abstractive summarization...\n",
            "BART using device: cpu\n",
            "BART model loaded.\n",
            "\n",
            "================================================================================\n",
            "Abstractive Summary (using BART directly on the full text):\n",
            "\n",
            "--- Starting BART Abstractive Summarization ---\n",
            "--- BART Abstractive Summarization Complete ---\n",
            "FDA and USDA have responsibility for overseeing the safety of the food supply. The technology to produce cell-cultured meat at a commercial scale is still in development. Information about the methods to be used for commercial production and the composition of the final product are not yet known. FDA and USDA could more fully incorporate the seven leading practices for effective collaboration into their interagency agreement for the joint oversight of cell- Cultured meat, the GAO says. The agreement does not describe how the agencies will track and monitor progress toward outcomes, the report says.. Developing and updating written guidance and agreements is a leading practice for collaboration, as we have previously reported, it says. FDA concurred with two recommendations and partially concurring with one. USDA also concurred, but only partially, with a recommendation that the agencies incorporate the leading practices into the agenciesâ€™ interagency. agreement. The agencies have not formally announced or documented this agreement. While FDA and. USDA officials told us they had agreed that FDA would oversee cell- cultured seafood other than catfish, as of December 2019, the agencies had not formally. announced or. documented this decision. Some stakeholders have reported confusion or ambiguity about which agency will oversee Cell Cultured Seafood other than. catfish. We continue to believe that FDA and USDA should more fully. incorporate the. seven leading Practices for Effective Collaboration in their inter Agency agreement and working groups. The GAO report also asks how the U.S. Department of Agriculture will be funded to provide regulatory oversight ofcell-culturing meat. How will the collaborative mechanism be funded? The report says it will be developed and updated to ensure consistency and efficient use of resources, and provide clarity to key stakeholders. The agency has not yet formally announced that it will oversee cell Cultured seafood, but it has said it has decided FDA will oversee most cell-ultured seafood. It also has said that it has not decided who will overseeCell Cultured Meat.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "BART only summarization complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#bart 4--\n"
      ],
      "metadata": {
        "id": "cACnCxvz4ere"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4--\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    BartForConditionalGeneration,\n",
        "    BartTokenizer,\n",
        "    LogitsProcessorList,\n",
        "    MinLengthLogitsProcessor,\n",
        "    BeamSearchScorer\n",
        ")\n",
        "import spacy\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Set, Optional\n",
        "\n",
        "\n",
        "class ConstraintExtractor:\n",
        "    \"\"\"Extract factual constraints from key sentences.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        try:\n",
        "            self.nlp = spacy.load(\"en_core_web_lg\")\n",
        "        except OSError:\n",
        "            print(\"Downloading spaCy model 'en_core_web_lg'...\")\n",
        "            import os\n",
        "            os.system(\"python -m spacy download en_core_web_lg\")\n",
        "            self.nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "\n",
        "    def extract_constraints(self, key_sentences: List[str]) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Extract named entities, dates, numbers, and other factual elements\n",
        "        from key sentences to use as constraints.\n",
        "        \"\"\"\n",
        "        constraints = []\n",
        "\n",
        "        for sentence in key_sentences:\n",
        "            doc = self.nlp(sentence)\n",
        "\n",
        "            # Extract named entities\n",
        "            entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "            # Extract numerical values\n",
        "            numbers = []\n",
        "            for token in doc:\n",
        "                if token.like_num and not any(token.text in e[0] for e in entities):\n",
        "                    numbers.append(token.text)\n",
        "\n",
        "            # Extract key noun phrases\n",
        "            noun_chunks = [chunk.text for chunk in doc.noun_chunks\n",
        "                          if not any(chunk.text in e[0] for e in entities)]\n",
        "\n",
        "            # Create constraint dictionary\n",
        "            constraint = {\n",
        "                \"sentence\": sentence,\n",
        "                \"entities\": entities,\n",
        "                \"numbers\": numbers,\n",
        "                \"noun_chunks\": noun_chunks\n",
        "            }\n",
        "\n",
        "            constraints.append(constraint)\n",
        "\n",
        "        return constraints\n",
        "\n",
        "    def format_constraints_for_bart(self, constraints: List[Dict]) -> str:\n",
        "        \"\"\"Format constraints as a string to prepend to BART input.\"\"\"\n",
        "        formatted = \"Important facts to include:\\n\"\n",
        "\n",
        "        for i, constraint in enumerate(constraints):\n",
        "            formatted += f\"[Fact {i+1}] \"\n",
        "\n",
        "            # Add entities\n",
        "            if constraint[\"entities\"]:\n",
        "                entities_str = \", \".join([f\"{e[0]} ({e[1]})\" for e in constraint[\"entities\"]])\n",
        "                formatted += f\"Entities: {entities_str}. \"\n",
        "\n",
        "            # Add numbers\n",
        "            if constraint[\"numbers\"]:\n",
        "                numbers_str = \", \".join(constraint[\"numbers\"])\n",
        "                formatted += f\"Numbers: {numbers_str}. \"\n",
        "\n",
        "            # Add key phrases\n",
        "            if constraint[\"noun_chunks\"]:\n",
        "                phrases_str = \", \".join(constraint[\"noun_chunks\"][:3])  # Limit to top 3\n",
        "                formatted += f\"Key phrases: {phrases_str}.\"\n",
        "\n",
        "            formatted += \"\\n\"\n",
        "\n",
        "        return formatted\n",
        "\n",
        "    def get_constraint_tokens(self, constraints: List[Dict], tokenizer) -> Set[int]:\n",
        "        \"\"\"Extract important tokens from constraints to guide beam search.\"\"\"\n",
        "        important_words = set()\n",
        "\n",
        "        for constraint in constraints:\n",
        "            # Add entity texts\n",
        "            for entity, _ in constraint[\"entities\"]:\n",
        "                important_words.add(entity.lower())\n",
        "                # Add individual words from multi-word entities\n",
        "                important_words.update(entity.lower().split())\n",
        "\n",
        "            # Add numbers\n",
        "            important_words.update([num.lower() for num in constraint[\"numbers\"]])\n",
        "\n",
        "            # Add key noun phrases\n",
        "            for chunk in constraint[\"noun_chunks\"]:\n",
        "                important_words.add(chunk.lower())\n",
        "                # Add individual words from chunks\n",
        "                important_words.update(chunk.lower().split())\n",
        "\n",
        "        # Convert words to token IDs\n",
        "        token_ids = set()\n",
        "        for word in important_words:\n",
        "            tokens = tokenizer.encode(word, add_special_tokens=False)\n",
        "            token_ids.update(tokens)\n",
        "\n",
        "        return token_ids\n",
        "\n",
        "\n",
        "class ConstrainedLogitsProcessor(nn.Module):\n",
        "    \"\"\"Logits processor that boosts the probability of constraint tokens.\"\"\"\n",
        "\n",
        "    def __init__(self, constraint_token_ids: Set[int], boost_factor: float = 2.0):\n",
        "        super().__init__()\n",
        "        self.constraint_token_ids = constraint_token_ids\n",
        "        self.boost_factor = boost_factor\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        \"\"\"Boost scores for constraint tokens.\"\"\"\n",
        "        for token_id in self.constraint_token_ids:\n",
        "            if token_id < scores.size(1):  # Ensure token ID is within vocabulary\n",
        "                scores[:, token_id] *= self.boost_factor\n",
        "\n",
        "        return scores\n",
        "\n",
        "\n",
        "class ConstrainedBartSummarizer:\n",
        "    \"\"\"BART summarizer with constrained beam search.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"facebook/bart-large-cnn\"):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "        self.model = BartForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
        "        self.constraint_extractor = ConstraintExtractor()\n",
        "\n",
        "    def summarize_with_constraints(\n",
        "        self,\n",
        "        document: str,\n",
        "        key_sentences: List[str],\n",
        "        use_constraint_prefix: bool = True,\n",
        "        use_constrained_beam_search: bool = True,\n",
        "        num_beams: int = 4,\n",
        "        min_length: int = 400,\n",
        "        max_length: int = 500,\n",
        "        boost_factor: float = 2.0,\n",
        "        no_repeat_ngram_size: int = 3\n",
        "    ) -> str:\n",
        "        \"\"\"Generate a summary using constraints from key sentences.\"\"\"\n",
        "        # Extract constraints from key sentences\n",
        "        constraints = self.constraint_extractor.extract_constraints(key_sentences)\n",
        "\n",
        "        # Prepare input text\n",
        "        input_text = document\n",
        "        if use_constraint_prefix:\n",
        "            constraint_prefix = self.constraint_extractor.format_constraints_for_bart(constraints)\n",
        "            input_text = constraint_prefix + \"\\n\\nDocument: \" + document\n",
        "\n",
        "        # Tokenize input\n",
        "        inputs = self.tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        # Set up generation parameters\n",
        "        generation_kwargs = {\n",
        "            \"num_beams\": num_beams,\n",
        "            \"min_length\": min_length,\n",
        "            \"max_length\": max_length,\n",
        "            \"no_repeat_ngram_size\": no_repeat_ngram_size,\n",
        "            \"early_stopping\": True,\n",
        "        }\n",
        "\n",
        "        # Apply constrained beam search if requested\n",
        "        if use_constrained_beam_search:\n",
        "            print(\"Using constrained beam search in ConstrainedBartSummarizer...\")\n",
        "            constraint_token_ids = self.constraint_extractor.get_constraint_tokens(\n",
        "                constraints, self.tokenizer\n",
        "            )\n",
        "\n",
        "            # Create logits processors\n",
        "            logits_processor = LogitsProcessorList([\n",
        "                MinLengthLogitsProcessor(min_length, self.tokenizer.eos_token_id),\n",
        "                ConstrainedLogitsProcessor(constraint_token_ids, boost_factor)\n",
        "            ])\n",
        "\n",
        "            generation_kwargs[\"logits_processor\"] = logits_processor\n",
        "        else:\n",
        "             print(\"Using standard beam search in ConstrainedBartSummarizer...\")\n",
        "\n",
        "        # Generate summary\n",
        "        with torch.no_grad():\n",
        "            output_ids = self.model.generate(**inputs, **generation_kwargs)\n",
        "\n",
        "        # Decode and return summary\n",
        "        summary = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        return summary\n",
        "\n",
        "    def custom_beam_search(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "        attention_mask: torch.LongTensor,\n",
        "        constraint_token_ids: Set[int],\n",
        "        num_beams: int = 4,\n",
        "        min_length: int = 50,\n",
        "        max_length: int = 200,\n",
        "        boost_factor: float = 2.0\n",
        "    ) -> torch.LongTensor:\n",
        "        \"\"\"Custom implementation of constrained beam search.\"\"\"\n",
        "        batch_size = input_ids.shape[0]\n",
        "        vocab_size = self.model.config.vocab_size\n",
        "\n",
        "        # Initialize beam scorer\n",
        "        beam_scorer = BeamSearchScorer(\n",
        "            batch_size=batch_size,\n",
        "            num_beams=num_beams,\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "        # Create logits processors\n",
        "        logits_processor = LogitsProcessorList([\n",
        "            MinLengthLogitsProcessor(min_length, self.tokenizer.eos_token_id),\n",
        "            ConstrainedLogitsProcessor(constraint_token_ids, boost_factor)\n",
        "        ])\n",
        "\n",
        "        # Expand input_ids and attention_mask for beam search\n",
        "        input_ids = input_ids.repeat_interleave(num_beams, dim=0)\n",
        "        attention_mask = attention_mask.repeat_interleave(num_beams, dim=0)\n",
        "\n",
        "        # Initialize sequence scores\n",
        "        beam_scores = torch.zeros(\n",
        "            (batch_size, num_beams), dtype=torch.float, device=input_ids.device\n",
        "        )\n",
        "        beam_scores[:, 1:] = -1e9  # Initialize only first beam for each batch\n",
        "        beam_scores = beam_scores.view(-1)  # (batch_size * num_beams)\n",
        "\n",
        "        # Start token for decoder\n",
        "        decoder_input_ids = torch.ones(\n",
        "            (batch_size * num_beams, 1),\n",
        "            dtype=torch.long,\n",
        "            device=input_ids.device\n",
        "        ) * self.model.config.decoder_start_token_id\n",
        "\n",
        "        # Track generated sequences\n",
        "        current_length = 1\n",
        "\n",
        "        # Initialize for beam search\n",
        "        encoder_outputs = self.model.get_encoder()(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        # Beam search loop\n",
        "        while current_length < max_length:\n",
        "            model_inputs = self.model.prepare_inputs_for_generation(\n",
        "                decoder_input_ids,\n",
        "                encoder_outputs=encoder_outputs,\n",
        "                attention_mask=attention_mask,\n",
        "            )\n",
        "\n",
        "            outputs = self.model(**model_inputs, return_dict=True)\n",
        "            next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            # Process logits\n",
        "            next_token_scores = logits_processor(decoder_input_ids, next_token_logits)\n",
        "            next_token_scores = F.log_softmax(next_token_scores, dim=-1)\n",
        "            next_token_scores = next_token_scores + beam_scores[:, None]\n",
        "\n",
        "            # Reshape scores for beam search\n",
        "            vocab_size = next_token_scores.shape[-1]\n",
        "            next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n",
        "\n",
        "            # Get next tokens and scores\n",
        "            next_token_scores, next_tokens = torch.topk(\n",
        "                next_token_scores, 2 * num_beams, dim=1, largest=True, sorted=True\n",
        "            )\n",
        "\n",
        "            # Convert token indices\n",
        "            next_indices = next_tokens // vocab_size\n",
        "            next_tokens = next_tokens % vocab_size\n",
        "\n",
        "            # Prepare next beam content\n",
        "            beam_outputs = beam_scorer.process(\n",
        "                decoder_input_ids,\n",
        "                next_token_scores,\n",
        "                next_tokens,\n",
        "                next_indices,\n",
        "                pad_token_id=self.tokenizer.pad_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "            beam_scores = beam_outputs[\"next_beam_scores\"]\n",
        "            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
        "            beam_idx = beam_outputs[\"next_beam_indices\"]\n",
        "\n",
        "            # Update input_ids\n",
        "            decoder_input_ids = torch.cat(\n",
        "                [decoder_input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1\n",
        "            )\n",
        "\n",
        "            current_length += 1\n",
        "\n",
        "            # Check if all beams are finished\n",
        "            if beam_scorer.is_done:\n",
        "                break\n",
        "\n",
        "        # Finalize beam search\n",
        "        sequence_outputs = beam_scorer.finalize(\n",
        "            decoder_input_ids,\n",
        "            beam_scores,\n",
        "            next_tokens,\n",
        "            next_indices,\n",
        "            pad_token_id=self.tokenizer.pad_token_id,\n",
        "            eos_token_id=self.tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "        return sequence_outputs[\"sequences\"]\n",
        "\n",
        "\n",
        "# Example usage function\n",
        "def generate_summary(document, key_sentences):\n",
        "    \"\"\"\n",
        "    Generate a summary using the constrained BART model.\n",
        "\n",
        "    Args:\n",
        "        document: The full document text\n",
        "        key_sentences: List of key sentences extracted by Longformer\n",
        "\n",
        "    Returns:\n",
        "        Generated summary\n",
        "    \"\"\"\n",
        "    # Initialize the summarizer\n",
        "    summarizer = ConstrainedBartSummarizer()\n",
        "\n",
        "    # Generate summary with constraints\n",
        "    summary = summarizer.summarize_with_constraints(\n",
        "        document=document,\n",
        "        key_sentences=key_sentences,\n",
        "        use_constraint_prefix=True,\n",
        "        use_constrained_beam_search=True\n",
        "    )\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "# Alternative implementation with direct constraint integration\n",
        "class ConstraintGuidedBART(nn.Module):\n",
        "    \"\"\"BART model with direct constraint integration.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"facebook/bart-large-cnn\"):\n",
        "        super().__init__()\n",
        "        self.model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "        self.tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_input_ids=None, labels=None):\n",
        "        \"\"\"Forward pass with standard BART.\"\"\"\n",
        "        return self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "    def generate_with_constraints(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask,\n",
        "        constraint_token_ids,\n",
        "        num_beams=4,\n",
        "        min_length=400,\n",
        "        max_length=500,\n",
        "        boost_factor=2.0\n",
        "    ):\n",
        "        \"\"\"Generate text with constraint-guided beam search.\"\"\"\n",
        "        # Create logits processor with constraints\n",
        "        logits_processor = LogitsProcessorList([\n",
        "            MinLengthLogitsProcessor(min_length, self.tokenizer.eos_token_id),\n",
        "            ConstrainedLogitsProcessor(constraint_token_ids, boost_factor)\n",
        "        ])\n",
        "\n",
        "        # Generate with constraints\n",
        "        outputs = self.model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            num_beams=num_beams,\n",
        "            min_length=min_length,\n",
        "            max_length=max_length,\n",
        "            logits_processor=logits_processor,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=3\n",
        "        )\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def summarize(self, document, key_sentences):\n",
        "        \"\"\"Generate summary with constraints from key sentences.\"\"\"\n",
        "        # Extract constraints\n",
        "        constraint_extractor = ConstraintExtractor()\n",
        "        constraints = constraint_extractor.extract_constraints(key_sentences)\n",
        "        constraint_token_ids = constraint_extractor.get_constraint_tokens(constraints, self.tokenizer)\n",
        "\n",
        "        # Tokenize input\n",
        "        inputs = self.tokenizer(document, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        # Generate summary\n",
        "        print(\"Using constraint-guided beam search in ConstraintGuidedBART...\")\n",
        "        output_ids = self.generate_with_constraints(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            constraint_token_ids=constraint_token_ids\n",
        "        )\n",
        "\n",
        "        # Decode summary\n",
        "        summary = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        return summary\n",
        "\n",
        "\n",
        "# Function to run the complete pipeline\n",
        "def run_summarization_pipeline(document, extractive_key_sentences):\n",
        "    \"\"\"\n",
        "    Run the complete summarization pipeline.\n",
        "\n",
        "    Args:\n",
        "        document: The full document text\n",
        "        extractive_key_sentences: Key sentences extracted by Longformer\n",
        "\n",
        "    Returns:\n",
        "        Generated summary\n",
        "    \"\"\"\n",
        "    # Method 1: Using ConstrainedBartSummarizer\n",
        "    print(\"\\n--- Running ConstrainedBartSummarizer ---\")\n",
        "    summarizer1 = ConstrainedBartSummarizer()\n",
        "    summary1 = summarizer1.summarize_with_constraints(\n",
        "        document=document,\n",
        "        key_sentences=extractive_key_sentences,\n",
        "        use_constraint_prefix=True,\n",
        "        use_constrained_beam_search=True # Set to False to see standard beam search path\n",
        "    )\n",
        "    print(\"--- Finished ConstrainedBartSummarizer ---\\n\")\n",
        "\n",
        "    # Method 2: Using ConstraintGuidedBART\n",
        "    print(\"\\n--- Running ConstraintGuidedBART ---\")\n",
        "    summarizer2 = ConstraintGuidedBART()\n",
        "    summary2 = summarizer2.summarize(document, extractive_key_sentences)\n",
        "    print(\"--- Finished ConstraintGuidedBART ---\\n\")\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"constrained_bart_summary\": summary1,\n",
        "        \"constraint_guided_bart_summary\": summary2\n",
        "    }"
      ],
      "metadata": {
        "id": "7fjW3UIdauy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-kJ56aZsE53l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_summarization_pipeline(input_text_for_bart_1, constraints_list_1)"
      ],
      "metadata": {
        "id": "og_asDFWs3mq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ba952f0-ef86-424c-982c-421332036d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running ConstrainedBartSummarizer ---\n",
            "Downloading spaCy model 'en_core_web_lg'...\n",
            "Using constrained beam search in ConstrainedBartSummarizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A custom logits processor of type <class 'transformers.generation.logits_process.MinLengthLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.MinLengthLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.MinLengthLogitsProcessor'> to see related `.generate()` flags.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Finished ConstrainedBartSummarizer ---\n",
            "\n",
            "\n",
            "--- Running ConstraintGuidedBART ---\n",
            "Using constraint-guided beam search in ConstraintGuidedBART...\n",
            "--- Finished ConstraintGuidedBART ---\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'constrained_bart_summary': 'FDA and USDA have responsibility for overseeing the safety of the food supply. The technology to produce cell-cultured meat at a commercial scale is still in development. The composition of the final product is also not yet known. FDA and USDA could more fully incorporate leading practices for collaboration in their interagency agreement and working groups. Developing and updating written guidance and agreements is a leading practice for collaboration as we have previously reported. We agree that cell- Cultured seafood should be produced on a large scale. We also agree that the technology and production methods to be used as well as the composition of. the final products should be developed in a manner that is safe for humans and animals. We have made a total of six recommendations for the development of cell- cultured seafood. For more information, visit the Food and Drug Administrationâ€™s Cell Cultured Seafood webpage. For confidential support call the National Suicide Prevention Lifeline at 1-800-273-8255 or visit http://www.suicidepreventionlifeline.org/. For support on suicide matters call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch or see www.samaritans.org. For support in the U.S., call the national suicide prevention Lifeline on 1-877-856-8457 or visit\\xa0http:// www.suicidesprevention.org\\xa0or\\xa0the\\xa0National Suicide Prevention Helpline on 1\\xa0800\\xa0988\\xa09090. For help in the United States, call the Salvation Army on 1 (800) 273-7255 or\\xa0 visit http\\xa0http\\xa0www.samarsitans. org/. For help with suicide matters in the UK, call\\xa0the Samaritans\\xa0on 08457 909090 or\\xa0 http\\xa0www\\xa0http :\\xa0http:\\xa0http:\\\\/www. Samaritans.co.uk/. For information on suicide prevention in the Middle East, visit http:\\\\/http://www\\xa0samaritan.org/Middle East/.',\n",
              " 'constraint_guided_bart_summary': \"FDA and USDA have responsibility for overseeing the safety of the food supply. The technology to produce cell-cultured meat at a commercial scale is still in development. The composition of the final product is also not yet known. GAO: FDA and USDA could more fully incorporate leading practices for collaboration in their interagency agreement and working groups.. Developing and updating written guidance and agreements is a leading practice for collaboration, as we have previously reported. The agreement does not describe how the agencies will track and monitor progress toward outcomes.. FDA concurred with two recommendations and partially concurring with one. USDA also concurred. with two Recommendations and partially Concurred with one;. The agencies have not formally announced or documented this agreement;. Some stakeholders have reported confusion or ambiguity about which agency will oversee cell- Cultured seafood other than catfish;. FDA has decided that FDA will oversee most cell- cultured seafood, but the agencies havenâ€™t formally documented this decision;. USDA has agreed to provide regulatory oversight of cell-ultured meat but hasn't formally announced this decision or documented it;. It is not clear how the U.S. Department of Agriculture will fund the collaborative mechanism;. There are questions about how much funding will be available for cell- cultured meat;. And there are concerns about the quality of the finished products;. They have not yet finalized aspects of the technology and eventual commercial production methods to be used or the composition of a final product. information is not yet available to stakeholders, such as cell-cultural meat firms themselves, regulators, and the public;. This lack of information results in unanswered questions about cell-culture meat as it relates to the eventual technology and production methods;. Compounding this challenge is that specific information about key aspects of Cell Cultured meat is not currently known. The GAO report (1) describes what is known about methods for commercially producing cell Cultured Meat. The report (2) describes how the FDA and the Food and Drug Administration (FDA) are collaborating to provide guidance and updates.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from rouge_score import rouge_scorer\n",
        "import bert_score\n",
        "\n",
        "# Load models\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Sentence splitter\n",
        "def split_sentences(text):\n",
        "    return [sent.text.strip() for sent in nlp(text).sents]\n",
        "\n",
        "# Coverage score\n",
        "def get_sentence_coverage(para1, para2, threshold=0.5):\n",
        "    sents1 = split_sentences(para1)\n",
        "    sents2 = split_sentences(para2)\n",
        "\n",
        "    emb1 = model.encode(sents1, convert_to_tensor=True)\n",
        "    emb2 = model.encode(sents2, convert_to_tensor=True)\n",
        "\n",
        "    matched = 0\n",
        "    for i in range(len(sents2)):\n",
        "        sims = util.cos_sim(emb2[i], emb1)[0]\n",
        "        if sims.max().item() >= threshold:\n",
        "            matched += 1\n",
        "\n",
        "    coverage = matched / len(sents2) if sents2 else 0.0\n",
        "    return round(coverage, 4)\n",
        "\n",
        "# ROUGE score\n",
        "def get_rouge_score(reference, candidate):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(reference, candidate)\n",
        "    return {\n",
        "        \"rouge1\": round(scores[\"rouge1\"].fmeasure, 4),\n",
        "        \"rouge2\": round(scores[\"rouge2\"].fmeasure, 4),\n",
        "        \"rougeL\": round(scores[\"rougeL\"].fmeasure, 4)\n",
        "    }\n",
        "\n",
        "# BERTScore\n",
        "def get_bert_score(reference, candidate, lang=\"en\"):\n",
        "    P, R, F1 = bert_score.score([candidate], [reference], lang=lang, verbose=False)\n",
        "    return {\n",
        "        \"bertscore_precision\": round(P[0].item(), 4),\n",
        "        \"bertscore_recall\": round(R[0].item(), 4),\n",
        "        \"bertscore_f1\": round(F1[0].item(), 4)\n",
        "    }\n",
        "\n",
        "# Combined comparison\n",
        "def compare_paragraphs(para1, para2):\n",
        "    scores = {\n",
        "        \"coverage_score\": get_sentence_coverage(para1, para2),\n",
        "        **get_rouge_score(para1, para2),\n",
        "        **get_bert_score(para1, para2)\n",
        "    }\n",
        "    return scores"
      ],
      "metadata": {
        "id": "wYfTkam45a0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f075e00b-a489-40d8-8764-0aba69585eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: bert_score in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.55.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.34.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.8.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert_score) (1.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "constraint_guided_bart_summary = \"\"\"FDA and USDA have responsibility for overseeing the safety of the food supply. The technology to produce cell-cultured meat at a commercial scale is still in development. The composition of the final product is also not yet known. GAO: FDA and USDA could more fully incorporate leading practices for collaboration in their interagency agreement and working groups.. Developing and updating written guidance and agreements is a leading practice for collaboration, as we have previously reported. The agreement does not describe how the agencies will track and monitor progress toward outcomes.. FDA concurred with two recommendations and partially concurring with one. USDA also concurred. with two Recommendations and partially Concurred with one;. The agencies have not formally announced or documented this agreement;. Some stakeholders have reported confusion or ambiguity about which agency will oversee cell- Cultured seafood other than catfish;. FDA has decided that FDA will oversee most cell- cultured seafood, but the agencies havenâ€™t formally documented this decision;. USDA has agreed to provide regulatory oversight of cell-ultured meat but hasn't formally announced this decision or documented it;. It is not clear how the U.S. Department of Agriculture will fund the collaborative mechanism;. There are questions about how much funding will be available for cell- cultured meat;. And there are concerns about the quality of the finished products;. They have not yet finalized aspects of the technology and eventual commercial production methods to be used or the composition of a final product. information is not yet available to stakeholders, such as cell-cultural meat firms themselves, regulators, and the public;. This lack of information results in unanswered questions about cell-culture meat as it relates to the eventual technology and production methods;. Compounding this challenge is that specific information about key aspects of Cell Cultured meat is not currently known. The GAO report (1) describes what is known about methods for commercially producing cell Cultured Meat. The report (2) describes how the FDA and the Food and Drug Administration (FDA) are collaborating to provide guidance and updates.\"\"\"\n",
        "bart_summary = \"\"\"FDA and USDA have responsibility for overseeing the safety of the food supply. The technology to produce cell-cultured meat at a commercial scale is still in development. Information about the methods to be used for commercial production and the composition of the final product are not yet known. FDA and USDA could more fully incorporate the seven leading practices for effective collaboration into their interagency agreement for the joint oversight of cell- Cultured meat, the GAO says. The agreement does not describe how the agencies will track and monitor progress toward outcomes, the report says.. Developing and updating written guidance and agreements is a leading practice for collaboration, as we have previously reported, it says. FDA concurred with two recommendations and partially concurring with one. USDA also concurred, but only partially, with a recommendation that the agencies incorporate the leading practices into the agenciesâ€™ interagency. agreement. The agencies have not formally announced or documented this agreement. While FDA and. USDA officials told us they had agreed that FDA would oversee cell- cultured seafood other than catfish, as of December 2019, the agencies had not formally. announced or. documented this decision. Some stakeholders have reported confusion or ambiguity about which agency will oversee Cell Cultured Seafood other than. catfish. We continue to believe that FDA and USDA should more fully. incorporate the. seven leading Practices for Effective Collaboration in their inter Agency agreement and working groups. The GAO report also asks how the U.S. Department of Agriculture will be funded to provide regulatory oversight ofcell-culturing meat. How will the collaborative mechanism be funded? The report says it will be developed and updated to ensure consistency and efficient use of resources, and provide clarity to key stakeholders. The agency has not yet formally announced that it will oversee cell Cultured seafood, but it has said it has decided FDA will oversee most cell-ultured seafood. It also has said that it has not decided who will overseeCell Cultured Meat.\"\"\"\n",
        "input_text_for_bart_1\n",
        "abstractive_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "dicH7czU-Pgl",
        "outputId": "c955b52f-ad9a-49ba-b0cb-70864834a4b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'FDA and USDA have responsibility for overseeing the safety of the food supply. General information about the process of making cell-cultured meat is available, but specific information about the technology being used and the eventual commercial production methods as well as the final products is not yet known. However, the technology and methods to commercially produce cell- cultured meat are still in development, and producers, regulators, and consumers do not yet have clarity on what these will entail. The composition of the final product is also not yet known. The general process for making cell-cultured meat contains five phases: biopsy, cell banking, growth, harvest, and food processing. The technology to produce cell-cultured meat at a commercial scale is still in development, and information about the methods to be used for commercial production and the composition of the final product are not yet known. Consequently, they have not finalized aspects of the technology and eventual commercial production methods to be used or the composition of the final product. As a result, certain information is not yet available to stakeholdersâ€”including cell-cultured meat firms themselves, regulators, and the publicâ€”about specific aspects of the technology and commercial production methods that will be used, such as the composition of the growth medium and of the final products. This lack of information results in unanswered questions about cell- cultured meat as it relates to the eventual technology and commercial production methods to be used and the composition of the final products. Some firms have developed prototypes of cell-cultured meat products as part of their research and development. In June 2019, FDA and USDA created three working groups to carry out the terms of the interagency agreement. FDA and USDA could more fully incorporate leading practices for collaboration in their interagency agreement and working groups. Developing and updating written guidance and agreements. However, the agreement does not describe how the agencies will track and monitor progress toward outcomes. Developing and updating written guidance and agreements. Developing and updating written guidance and agreements. Developing and updating written guidance and agreements. By more fully incorporating all seven leading practices for interagency collaboration early in the development of the three working groups, FDA and USDA could proactively minimize potential fragmentation and overlap in their oversight of cell-cultured meat, ensure consistency and efficient use of resources, and provide clarity to key stakeholders. While FDA and USDA officials told us they have decided who will oversee cell-cultured seafood, they have not formally announced or documented this decision, and some stakeholders have reported confusion or ambiguity about which agency will oversee cell-cultured seafood other than catfish. While FDA and USDA officials told us they had agreed that FDA would oversee cell-cultured seafood other than catfish, as of December 2019, the agencies had not formally announced or documented this agreement. Developing and updating written guidance and agreements is a leading practice for collaboration, as we have previously reported. Compounding this challenge is that specific information about key aspects of cell-cultured meat, such as the technology and production methods to be used as well as the composition of the products, is not yet known. FDA and USDA officials told us they have decided FDA will oversee most cell-cultured seafood, but the agencies have not formally documented this decision. We agree that the technology to produce cell-cultured meat is still in development and that information about the commercial production methods and composition of the final product are not yet known, as we state in our report. FDA concurred with two recommendations and partially concurred with one. USDA also concurred with two recommendations and partially concurred with one. FDA and USDA partially concurred with our recommendation, directed to each agency, to more fully incorporate the seven leading practices for effective collaboration into the agenciesâ€™ interagency agreement for the joint oversight of cell-cultured meat. We continue to believe that FDA and USDA should more fully incorporate the seven leading practices for effective collaboration into their interagency agreement for the joint oversight of cell-cultured meat. GAO staff who made key contributions to this report are listed in appendix V. Our report (1) describes what is known about methods for commercially producing cell-cultured meat and (2) examines the extent to which the Food and Drug Administration (FDA) and U.S. Department of Agriculture (USDA) are collaborating to provide regulatory oversight of cell-cultured meat. Developing and updating written guidance and agreements How will the collaborative mechanism be funded?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = compare_paragraphs(abstractive_summary, constraint_guided_bart_summary)\n",
        "for k, v in results.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score(abstractive_summary, constraint_guided_bart_summary)\n",
        "print(\"ROUGE Recall:\", scores['rouge1'].recall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jixjpl8V_P5W",
        "outputId": "1b376995-4ced-4926-96ac-d53c1a577164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coverage_score: 0.8636\n",
            "rouge1: 0.6051\n",
            "rouge2: 0.3513\n",
            "rougeL: 0.222\n",
            "bertscore_precision: 0.8762\n",
            "bertscore_recall: 0.8739\n",
            "bertscore_f1: 0.875\n",
            "ROUGE Recall: 0.5029126213592233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = compare_paragraphs(bart_summary, constraint_guided_bart_summary)\n",
        "for k, v in results.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score(bart_summary, constraint_guided_bart_summary)\n",
        "print(\"ROUGE Recall:\", scores['rouge1'].recall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEiDQNmK_RRL",
        "outputId": "4c5f09a2-d885-4b6f-af8b-3ef98d68cb80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coverage_score: 0.9091\n",
            "rouge1: 0.7273\n",
            "rouge2: 0.5052\n",
            "rougeL: 0.4382\n",
            "bertscore_precision: 0.9011\n",
            "bertscore_recall: 0.9094\n",
            "bertscore_f1: 0.9052\n",
            "ROUGE Recall: 0.7393939393939394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = compare_paragraphs(abstractive_summary, constraint_guided_bart_summary)\n",
        "for k, v in results.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score(abstractive_summary, constraint_guided_bart_summary)\n",
        "print(\"ROUGE Recall:\", scores['rouge1'].recall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVMWR4gIC3bA",
        "outputId": "5e9de722-ac59-49c9-b3b4-e45703c84b0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coverage_score: 0.8636\n",
            "rouge1: 0.6051\n",
            "rouge2: 0.3513\n",
            "rougeL: 0.222\n",
            "bertscore_precision: 0.8762\n",
            "bertscore_recall: 0.8739\n",
            "bertscore_f1: 0.875\n",
            "ROUGE Recall: 0.5029126213592233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = compare_paragraphs(abstractive_summary, bart_summary)\n",
        "for k, v in results.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score(abstractive_summary, bart_summary)\n",
        "print(\"ROUGE Recall:\", scores['rouge1'].recall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSfkMaBBC3kY",
        "outputId": "3d8bcac3-47f9-4a65-ef27-037f52073894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coverage_score: 0.75\n",
            "rouge1: 0.5728\n",
            "rouge2: 0.3013\n",
            "rougeL: 0.2627\n",
            "bertscore_precision: 0.8729\n",
            "bertscore_recall: 0.8629\n",
            "bertscore_f1: 0.8678\n",
            "ROUGE Recall: 0.46990291262135925\n"
          ]
        }
      ]
    }
  ]
}